{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQpq8SZ0o98ai87+kpoQFj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hesther/teaching/blob/main/med_inf/Lecture_demonstration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hesther/teaching/blob/main/med_inf/Lecture_demonstration.ipynb)"
      ],
      "metadata": {
        "id": "VxtY1r0mn5re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LV 18 of Methods for Data Generation and Analytics in Medicine and Life Sciences"
      ],
      "metadata": {
        "id": "nl9raM0TYo2d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jaGTT0zlXnZn"
      },
      "outputs": [],
      "source": [
        "!pip install -q rdkit numpy scikit-learn chemprop torch==2.0.1\n",
        "!pip install -q torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-2.0.1+cpu.html\n",
        "!pip install -q torch_geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning in Chemistry\n",
        "\n",
        "### 1. Solubility data\n",
        "One great Python package for molecules is RDKit. We can input a molecule from an SMILES string, which is a in-line notation for molecules that is also readable by humans. For example:\n",
        "\n",
        "- Methane: C\n",
        "- Ethane: CC\n",
        "- Ethene: C=C\n",
        "- Propane: CCC\n",
        "- Butane: CCCC\n",
        "- Isobutane: CC(C)C\n",
        "- Cyclobutane: C1CCC1\n",
        "\n",
        "You get the picture."
      ],
      "metadata": {
        "id": "XOP4GWCgXw13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rdkit\n",
        "from rdkit.Chem import Descriptors\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CjNFK8__ZB4v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"https://github.com/hesther/rxn_workshop/raw/main/data/esol/train_full.csv\")\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "EYT48m0fY0aK",
        "outputId": "37d7fad6-57b1-49d2-d894-b00acd76cefb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                smiles  logSolubility\n",
              "0                                             CC/C=C\\C         -2.540\n",
              "1                       O=C1NC(=O)NC(=O)C1(CC)CC=C(C)C         -2.253\n",
              "2                     Cc1[nH]c(=O)n(c(=O)c1Cl)C(C)(C)C         -2.484\n",
              "3                                             CC/C=C/C         -2.540\n",
              "4                   ClC(Cl)C(c1ccc(Cl)cc1)c2ccc(Cl)cc2         -7.200\n",
              "..                                                 ...            ...\n",
              "897                 O2c1ccc(N)cc1N(C)C(=O)c3cc(C)ccc23         -3.928\n",
              "898                                         CCCCCCCC#C         -4.240\n",
              "899                                        CCCC(=O)OCC         -1.360\n",
              "900  CC(=O)OCC(=O)C1(O)CCC2C3CCC4=CC(=O)CCC4(C)C3C(...         -4.880\n",
              "901                                      Clc1ccc(I)cc1         -4.030\n",
              "\n",
              "[902 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec3707a0-896e-4aae-bd48-8abe8d9be456\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>smiles</th>\n",
              "      <th>logSolubility</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CC/C=C\\C</td>\n",
              "      <td>-2.540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>O=C1NC(=O)NC(=O)C1(CC)CC=C(C)C</td>\n",
              "      <td>-2.253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Cc1[nH]c(=O)n(c(=O)c1Cl)C(C)(C)C</td>\n",
              "      <td>-2.484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CC/C=C/C</td>\n",
              "      <td>-2.540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ClC(Cl)C(c1ccc(Cl)cc1)c2ccc(Cl)cc2</td>\n",
              "      <td>-7.200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>897</th>\n",
              "      <td>O2c1ccc(N)cc1N(C)C(=O)c3cc(C)ccc23</td>\n",
              "      <td>-3.928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>898</th>\n",
              "      <td>CCCCCCCC#C</td>\n",
              "      <td>-4.240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>899</th>\n",
              "      <td>CCCC(=O)OCC</td>\n",
              "      <td>-1.360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>900</th>\n",
              "      <td>CC(=O)OCC(=O)C1(O)CCC2C3CCC4=CC(=O)CCC4(C)C3C(...</td>\n",
              "      <td>-4.880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>901</th>\n",
              "      <td>Clc1ccc(I)cc1</td>\n",
              "      <td>-4.030</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>902 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec3707a0-896e-4aae-bd48-8abe8d9be456')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ec3707a0-896e-4aae-bd48-8abe8d9be456 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ec3707a0-896e-4aae-bd48-8abe8d9be456');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-51cb5345-b789-4420-a993-00cfe664965f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-51cb5345-b789-4420-a993-00cfe664965f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-51cb5345-b789-4420-a993-00cfe664965f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f8401a00-8c3c-4179-8183-664ed37c58a0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f8401a00-8c3c-4179-8183-664ed37c58a0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 902,\n  \"fields\": [\n    {\n      \"column\": \"smiles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 898,\n        \"samples\": [\n          \"ClC(Cl)(Cl)Cl\",\n          \"CCOP(=S)(OCC)SCSCC\",\n          \"Oc1cc(Cl)c(Cl)c(Cl)c1Cl\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"logSolubility\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.1019626263291658,\n        \"min\": -11.6,\n        \"max\": 1.58,\n        \"num_unique_values\": 618,\n        \"samples\": [\n          -7.92,\n          -4.23,\n          -1.38\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mol1 = rdkit.Chem.MolFromSmiles(data['smiles'][0])\n",
        "mol1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "2ZeThUjFYBQY",
        "outputId": "8b39cdc4-dc1a-4b47-9819-0e3e071be1b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<rdkit.Chem.rdchem.Mol at 0x7e17ac78c350>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fit a neural network, we have to transform the SMILES strings into a list of features. Luckily, RDKit has a functionality to do that:"
      ],
      "metadata": {
        "id": "aIkaK3WNZgZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Descriptors.CalcMolDescriptors(mol1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST7DVN-wZPxj",
        "outputId": "97fd2c9b-c8cd-4c0b-c4e4-01964f588b8c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MaxAbsEStateIndex': 2.125,\n",
              " 'MaxEStateIndex': 2.125,\n",
              " 'MinAbsEStateIndex': 1.1631944444444444,\n",
              " 'MinEStateIndex': 1.1631944444444444,\n",
              " 'qed': 0.4136396804400309,\n",
              " 'SPS': 10.0,\n",
              " 'MolWt': 70.135,\n",
              " 'HeavyAtomMolWt': 60.05499999999999,\n",
              " 'ExactMolWt': 70.07825032,\n",
              " 'NumValenceElectrons': 30,\n",
              " 'NumRadicalElectrons': 0,\n",
              " 'MaxPartialCharge': -0.03794287307367419,\n",
              " 'MinPartialCharge': -0.0916744470464646,\n",
              " 'MaxAbsPartialCharge': 0.0916744470464646,\n",
              " 'MinAbsPartialCharge': 0.03794287307367419,\n",
              " 'FpDensityMorgan1': 1.6,\n",
              " 'FpDensityMorgan2': 2.2,\n",
              " 'FpDensityMorgan3': 2.2,\n",
              " 'BCUT2D_MWHI': 13.594079354378078,\n",
              " 'BCUT2D_MWLOW': 10.430607628427651,\n",
              " 'BCUT2D_CHGHI': 1.5174139912845042,\n",
              " 'BCUT2D_CHGLO': -1.6468102360406212,\n",
              " 'BCUT2D_LOGPHI': 1.7323374263754383,\n",
              " 'BCUT2D_LOGPLOW': -1.4311843309765475,\n",
              " 'BCUT2D_MRHI': 4.730012035564743,\n",
              " 'BCUT2D_MRLOW': 1.2174768301743983,\n",
              " 'AvgIpc': 1.4056390622295665,\n",
              " 'BalabanJ': 2.622415807990217,\n",
              " 'BertzCT': 27.019550008653876,\n",
              " 'Chi0': 4.121320343559642,\n",
              " 'Chi0n': 3.861807319565799,\n",
              " 'Chi0v': 3.861807319565799,\n",
              " 'Chi1': 2.414213562373095,\n",
              " 'Chi1n': 2.02603867417337,\n",
              " 'Chi1v': 2.02603867417337,\n",
              " 'Chi2n': 0.9772838841927123,\n",
              " 'Chi2v': 0.9772838841927123,\n",
              " 'Chi3n': 0.47140452079103173,\n",
              " 'Chi3v': 0.47140452079103173,\n",
              " 'Chi4n': 0.23570226039551587,\n",
              " 'Chi4v': 0.23570226039551587,\n",
              " 'HallKierAlpha': -0.26,\n",
              " 'Ipc': 11.245112497836532,\n",
              " 'Kappa1': 4.74,\n",
              " 'Kappa2': 3.7400000000000007,\n",
              " 'Kappa3': 3.740000000000001,\n",
              " 'LabuteASA': 33.50941645323172,\n",
              " 'PEOE_VSA1': 0.0,\n",
              " 'PEOE_VSA10': 0.0,\n",
              " 'PEOE_VSA11': 0.0,\n",
              " 'PEOE_VSA12': 0.0,\n",
              " 'PEOE_VSA13': 0.0,\n",
              " 'PEOE_VSA14': 0.0,\n",
              " 'PEOE_VSA2': 0.0,\n",
              " 'PEOE_VSA3': 0.0,\n",
              " 'PEOE_VSA4': 0.0,\n",
              " 'PEOE_VSA5': 0.0,\n",
              " 'PEOE_VSA6': 19.075777413358384,\n",
              " 'PEOE_VSA7': 13.344558822616634,\n",
              " 'PEOE_VSA8': 0.0,\n",
              " 'PEOE_VSA9': 0.0,\n",
              " 'SMR_VSA1': 0.0,\n",
              " 'SMR_VSA10': 0.0,\n",
              " 'SMR_VSA2': 0.0,\n",
              " 'SMR_VSA3': 0.0,\n",
              " 'SMR_VSA4': 0.0,\n",
              " 'SMR_VSA5': 20.268296022307258,\n",
              " 'SMR_VSA6': 0.0,\n",
              " 'SMR_VSA7': 12.152040213667762,\n",
              " 'SMR_VSA8': 0.0,\n",
              " 'SMR_VSA9': 0.0,\n",
              " 'SlogP_VSA1': 0.0,\n",
              " 'SlogP_VSA10': 0.0,\n",
              " 'SlogP_VSA11': 0.0,\n",
              " 'SlogP_VSA12': 0.0,\n",
              " 'SlogP_VSA2': 0.0,\n",
              " 'SlogP_VSA3': 0.0,\n",
              " 'SlogP_VSA4': 0.0,\n",
              " 'SlogP_VSA5': 20.268296022307258,\n",
              " 'SlogP_VSA6': 12.152040213667762,\n",
              " 'SlogP_VSA7': 0.0,\n",
              " 'SlogP_VSA8': 0.0,\n",
              " 'SlogP_VSA9': 0.0,\n",
              " 'TPSA': 0.0,\n",
              " 'EState_VSA1': 0.0,\n",
              " 'EState_VSA10': 0.0,\n",
              " 'EState_VSA11': 0.0,\n",
              " 'EState_VSA2': 0.0,\n",
              " 'EState_VSA3': 0.0,\n",
              " 'EState_VSA4': 6.4208216229260096,\n",
              " 'EState_VSA5': 0.0,\n",
              " 'EState_VSA6': 0.0,\n",
              " 'EState_VSA7': 6.923737199690624,\n",
              " 'EState_VSA8': 19.075777413358384,\n",
              " 'EState_VSA9': 0.0,\n",
              " 'VSA_EState1': 0.0,\n",
              " 'VSA_EState10': 0.0,\n",
              " 'VSA_EState2': 0.0,\n",
              " 'VSA_EState3': 0.0,\n",
              " 'VSA_EState4': 0.0,\n",
              " 'VSA_EState5': 0.0,\n",
              " 'VSA_EState6': 0.0,\n",
              " 'VSA_EState7': 5.34375,\n",
              " 'VSA_EState8': 4.15625,\n",
              " 'VSA_EState9': 0.0,\n",
              " 'FractionCSP3': 0.6,\n",
              " 'HeavyAtomCount': 5,\n",
              " 'NHOHCount': 0,\n",
              " 'NOCount': 0,\n",
              " 'NumAliphaticCarbocycles': 0,\n",
              " 'NumAliphaticHeterocycles': 0,\n",
              " 'NumAliphaticRings': 0,\n",
              " 'NumAromaticCarbocycles': 0,\n",
              " 'NumAromaticHeterocycles': 0,\n",
              " 'NumAromaticRings': 0,\n",
              " 'NumHAcceptors': 0,\n",
              " 'NumHDonors': 0,\n",
              " 'NumHeteroatoms': 0,\n",
              " 'NumRotatableBonds': 1,\n",
              " 'NumSaturatedCarbocycles': 0,\n",
              " 'NumSaturatedHeterocycles': 0,\n",
              " 'NumSaturatedRings': 0,\n",
              " 'RingCount': 0,\n",
              " 'MolLogP': 1.9725,\n",
              " 'MolMR': 25.10499999999999,\n",
              " 'fr_Al_COO': 0,\n",
              " 'fr_Al_OH': 0,\n",
              " 'fr_Al_OH_noTert': 0,\n",
              " 'fr_ArN': 0,\n",
              " 'fr_Ar_COO': 0,\n",
              " 'fr_Ar_N': 0,\n",
              " 'fr_Ar_NH': 0,\n",
              " 'fr_Ar_OH': 0,\n",
              " 'fr_COO': 0,\n",
              " 'fr_COO2': 0,\n",
              " 'fr_C_O': 0,\n",
              " 'fr_C_O_noCOO': 0,\n",
              " 'fr_C_S': 0,\n",
              " 'fr_HOCCN': 0,\n",
              " 'fr_Imine': 0,\n",
              " 'fr_NH0': 0,\n",
              " 'fr_NH1': 0,\n",
              " 'fr_NH2': 0,\n",
              " 'fr_N_O': 0,\n",
              " 'fr_Ndealkylation1': 0,\n",
              " 'fr_Ndealkylation2': 0,\n",
              " 'fr_Nhpyrrole': 0,\n",
              " 'fr_SH': 0,\n",
              " 'fr_aldehyde': 0,\n",
              " 'fr_alkyl_carbamate': 0,\n",
              " 'fr_alkyl_halide': 0,\n",
              " 'fr_allylic_oxid': 2,\n",
              " 'fr_amide': 0,\n",
              " 'fr_amidine': 0,\n",
              " 'fr_aniline': 0,\n",
              " 'fr_aryl_methyl': 0,\n",
              " 'fr_azide': 0,\n",
              " 'fr_azo': 0,\n",
              " 'fr_barbitur': 0,\n",
              " 'fr_benzene': 0,\n",
              " 'fr_benzodiazepine': 0,\n",
              " 'fr_bicyclic': 0,\n",
              " 'fr_diazo': 0,\n",
              " 'fr_dihydropyridine': 0,\n",
              " 'fr_epoxide': 0,\n",
              " 'fr_ester': 0,\n",
              " 'fr_ether': 0,\n",
              " 'fr_furan': 0,\n",
              " 'fr_guanido': 0,\n",
              " 'fr_halogen': 0,\n",
              " 'fr_hdrzine': 0,\n",
              " 'fr_hdrzone': 0,\n",
              " 'fr_imidazole': 0,\n",
              " 'fr_imide': 0,\n",
              " 'fr_isocyan': 0,\n",
              " 'fr_isothiocyan': 0,\n",
              " 'fr_ketone': 0,\n",
              " 'fr_ketone_Topliss': 0,\n",
              " 'fr_lactam': 0,\n",
              " 'fr_lactone': 0,\n",
              " 'fr_methoxy': 0,\n",
              " 'fr_morpholine': 0,\n",
              " 'fr_nitrile': 0,\n",
              " 'fr_nitro': 0,\n",
              " 'fr_nitro_arom': 0,\n",
              " 'fr_nitro_arom_nonortho': 0,\n",
              " 'fr_nitroso': 0,\n",
              " 'fr_oxazole': 0,\n",
              " 'fr_oxime': 0,\n",
              " 'fr_para_hydroxylation': 0,\n",
              " 'fr_phenol': 0,\n",
              " 'fr_phenol_noOrthoHbond': 0,\n",
              " 'fr_phos_acid': 0,\n",
              " 'fr_phos_ester': 0,\n",
              " 'fr_piperdine': 0,\n",
              " 'fr_piperzine': 0,\n",
              " 'fr_priamide': 0,\n",
              " 'fr_prisulfonamd': 0,\n",
              " 'fr_pyridine': 0,\n",
              " 'fr_quatN': 0,\n",
              " 'fr_sulfide': 0,\n",
              " 'fr_sulfonamd': 0,\n",
              " 'fr_sulfone': 0,\n",
              " 'fr_term_acetylene': 0,\n",
              " 'fr_tetrazole': 0,\n",
              " 'fr_thiazole': 0,\n",
              " 'fr_thiocyan': 0,\n",
              " 'fr_thiophene': 0,\n",
              " 'fr_unbrch_alkane': 0,\n",
              " 'fr_urea': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def features(smi):\n",
        "    mol = rdkit.Chem.MolFromSmiles(smi)\n",
        "    return np.array(list(Descriptors.CalcMolDescriptors(mol).values()))\n",
        "\n",
        "features(\"CCC\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8yrT3lMZlhN",
        "outputId": "dfe16112-60d8-4ab1-fcb1-9466c561a49d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.125     ,  2.125     ,  1.25      ,  1.25      ,  0.38547066,\n",
              "        6.        , 44.097     , 36.033     , 44.06260026, 20.        ,\n",
              "        0.        , -0.05903836, -0.06564544,  0.06564544,  0.05903836,\n",
              "        1.33333333,  1.33333333,  1.33333333, 13.42571365, 10.59728635,\n",
              "        1.35237444, -1.47605824,  1.55881365, -1.26961365,  3.91771365,\n",
              "        1.08928635,  0.91829583,  1.63299316,  0.        ,  2.70710678,\n",
              "        2.70710678,  2.70710678,  1.41421356,  1.41421356,  1.41421356,\n",
              "        0.70710678,  0.70710678,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  2.7548875 ,  3.        ,  2.        ,\n",
              "        0.        , 21.46913526,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        , 20.26829602,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        , 20.26829602,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        , 20.26829602,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        6.42082162,  0.        ,  0.        , 13.8474744 ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  1.25      ,  4.25      ,  0.        ,\n",
              "        1.        ,  3.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  1.4163    , 15.965     ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, let's now make one giant 2D array of x values, and try to predict y (the solubility):"
      ],
      "metadata": {
        "id": "WKajH2nEZysa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_raw = np.array([features(smi) for smi in data['smiles']])\n",
        "standardize_by = np.max(np.abs(x_raw),axis=0)+0.1\n",
        "x_train = x_raw / standardize_by\n",
        "y_train = np.array(data['logSolubility'])"
      ],
      "metadata": {
        "id": "VPQgeULBZpPi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also load a test set:"
      ],
      "metadata": {
        "id": "dEER2G3gaBjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"https://github.com/hesther/rxn_workshop/raw/main/data/esol/test_full.csv\")\n",
        "x_raw = np.array([features(smi) for smi in data['smiles']])\n",
        "x_test = x_raw / standardize_by\n",
        "y_test = np.array(data['logSolubility'])"
      ],
      "metadata": {
        "id": "L1mRfIP0aAjk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Linear regression\n",
        "Let's try a simple regression first:"
      ],
      "metadata": {
        "id": "uk7V7A_zaSqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "10KCFgvuaRzg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
        "\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel(\"true solubility\")\n",
        "plt.ylabel(\"predicted solubility\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "vtfwvOsFaNY7",
        "outputId": "70d03f13-399c-4099-ca26-08fc2074054c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.5379380941475387\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'predicted solubility')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAG0CAYAAADKEdZ4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABIlklEQVR4nO3deXhU9dn/8c+EJQFMRpaEBEQISCkxFQgI5idVilCCitC6UhHwQVp4FKu4QUUj0Ba3p7gWFRVBlHopVsQlFcQNCtIaQWNYJEChmCBLmSBIgMz8/qATs83MOZMzyzl5v64r10UmZ2bujC3n5vu9v/ft8vl8PgEAANhcQqwDAAAAsAJJDQAAcASSGgAA4AgkNQAAwBFIagAAgCOQ1AAAAEcgqQEAAI5AUgMAAByBpAYAADgCSQ0AAHAE2yQ1c+bM0bnnnqvk5GSlpaVp1KhR2rJlS6zDAgAAccJll9lPeXl5uuaaa3Tuuefq5MmT+t3vfqeioiIVFxerVatWhl7D6/Xqm2++UXJyslwuV4QjBgAAVvD5fDp8+LA6dOighITA6zG2SWpq27dvn9LS0vTRRx/pggsuMPScf//73+rUqVOEIwMAAJGwe/dunXHGGQF/3jSKsVjK4/FIktq0aRPwmoqKClVUVFR978/fdu/erZSUlMgGCAAALFFeXq5OnTopOTk56HW2TGq8Xq9uueUWnX/++crOzg543Zw5czRz5sw6j6ekpJDUAABgM6FKR2y5/TR58mS9++67Wr16ddBlqNorNf5Mz+PxkNQAAGAT5eXlcrvdIe/ftlupuemmm/TWW2/p448/DprQSFJiYqISExOjFBkAAIgl2yQ1Pp9PU6ZM0V//+ld9+OGHyszMjHVIAAAgjtgmqbnxxhv18ssva9myZUpOTlZZWZkkye12q0WLFjGODgAAxJptamoCFQctWLBA48ePN/QaRvfkAABA/HBcTY1Nci8AABAjthmTAAAAEAxJDQAAcASSGgAA4Ai2qakBAADxqdLr0/odB/Xt4WNKS05S/8w2apIQ/cHRJDUAACBsBUWlmrm8WKWeY1WPZbiTlD8iS3nZGVGNhe0nAAAQloKiUk1eXFgjoZGkMs8xTV5cqIKi0qjGQ1IDAABMq/T6NHN5sepruOJ/bObyYlV6o9eShaQGAACYtn7HwTorNNX5JJV6jmn9joNRi4mkBgAAmPbt4cAJTTjXWYGkBgAAmJaWnGTpdVYgqQEAAKb1z2yjDHeSAh3cdunUKaj+mW2iFhNJDQAAMK1Jgkv5I7IkqU5i4/8+f0RWVPvVkNQAAIA6Kr0+rS05oGUb9mhtyYF6TzHlZWdo3pgcpbtrbjGlu5M0b0xO1PvU0HwPAADUYKahXl52hoZmpcdFR2GXz+eL3gHyGCsvL5fb7ZbH41FKSkqswwEAIO74G+rVTg78KUosVmCM3r/ZfgIAAJLis6GeGSQ1AABAUnw21DODpAYAAEiKz4Z6ZpDUAAAASfHZUM8MkhoAACApPhvqmUFSAwAAJMVnQz0zSGoAAECVeGuoZwbN9wAAQA3x1FDPDJIaAABQR5MEl3K7tY11GKaw/QQAAByBpAYAADgCSQ0AAHAEkhoAAOAIFAoDAOAAlV6f7U4rWY2kBgAAmysoKtXM5cU1hlFmuJOUPyIrrvvKWI3tJwAAbKygqFSTFxfWma5d5jmmyYsLVVBUGqPIoo+kBgAAm6r0+jRzebF89fzM/9jM5cWq9NZ3hfOQ1AAAYFPrdxyss0JTnU9SqeeY1u84GL2gYoikBgAAm/r2cOCEJpzr7I6kBgAAm0pLTgp9kYnr7I6kBgAAm+qf2UYZ7iQFOrjt0qlTUP0z20QzrJghqQEAwKaaJLiUPyJLkuokNv7v80dkNZp+NSQ1AADYRKXXp7UlB7Rswx6tLTmgSq9PedkZmjcmR+numltM6e4kzRuT06j61NB8DwAAGwjVYG9oVjodhWMdAAAACM7fYK92t5kyzzFNWlyo/zm/i4ZmpTfKRKY6khoAAOKYkQZ7z6/ZqefX7GyUoxGqo6YGAIA4FqrBXnWNcTRCdSQ1AADEMTON88yORqiv8NjO2H4CACCOmW2cV300Qm63tgGvc+Jkb1ZqAACIY6Ea7AUSbIXHqZO9SWoAAIhjwRrsBRNohSdU4bFP0rTXv9Sar/fbbjuKpAYAgDgXqMFefUKNRjBSeHzo6Ald+9ynGvjAKlut2pDUAABgA3nZGVp912AtmXieJpzfpd5rjIxGMFN4bLftKJIaAABsokmCS7nd2uqeEWfrqTE5yghjNIKZwmOzp6lijdNPAADYULijEfyFx2WeY/XW1dRm9DRVPCCpAQDApvwrN6FUen01kp97LsnSjS8XyiUZSmwkc9tWsUJSAwCAgwXqR/PrCzL15sZSw92KzfbLiQWSGgAAHCrYIMxnPt6hJ3/VR+6WzXXjS4U69P2Jel/DpVO1OoFOU8UTCoUBAHAgI4MwZ7+9Sed1bav7L/+JXKrbB8fIaap4QlIDAIADhepHU70AOFAfHCOnqeIJ208AADiQ0cJe/3XhnqaKJyQ1AAA4kNHC3urXGT1NFa9st/305JNPqkuXLkpKStKAAQO0fv36WIcEAGjkKr0+rS05oGUb9mhtyYG4aFQXahBmqHEKdmSrlZpXXnlFU6dO1VNPPaUBAwbokUce0bBhw7RlyxalpaXFOjwAQCMU6Mh0/oismNai+AdhTl5ctx+N3QqAjXL5fL7Yp5MGDRgwQOeee66eeOIJSZLX61WnTp00ZcoUTZs2LeTzy8vL5Xa75fF4lJKSEulwAQAOF+jItD9NiIci23hNuswwev+2zUrN8ePH9dlnn2n69OlVjyUkJGjIkCFau3Ztvc+pqKhQRUVF1ffl5eURjxMA0DiEOjLt0qmZSUOz0mO6GuKEAmCjbFNTs3//flVWVqp9+/Y1Hm/fvr3Kysrqfc6cOXPkdrurvjp16hSNUAEAjYCZI9Ox5i8AHtm7o3K7tXVkQiPZKKkJx/Tp0+XxeKq+du/eHeuQAAAOYfbItBHxWHBsJ7bZfmrXrp2aNGmivXv31nh87969Sk9Pr/c5iYmJSkxMjEZ4AIBGJpwj08E4ofYl1myzUtO8eXP17dtX77//ftVjXq9X77//vnJzc2MYGQCgMbLyyLS/4Lj2dlaZ55gmLy5UQVFpwwNuBGyT1EjS1KlTNX/+fC1cuFCbNm3S5MmTdeTIEV1//fWxDg0A0Mj4j0xLDZuZZGRG08zlxWxFGWCrpObqq6/Www8/rHvvvVe9e/fWhg0bVFBQUKd4GACAaLBiZpKdCo7jnW1qavxuuukm3XTTTbEOAwAASQ0/Mh2JguPGynZJDQAA8ab2zCT/KSYjSY7RQuJ2p3HwJRSSGgAALGT2FJO/4LjMc6zeuhq/qa98rpkjszkJFYStamoAAIhn4ZxiClZwXN3ew8c1iZNQQZHUAABggYacYvIXHLdPCb0VNe31LzkJFQBJDQAAFmjoKaa87Aw9dPk5Id/n0NETWldyINwwHY2kBgAAC1hxiunTncaSlbXb9xu6rrEhqQEAwALWjE0wOmjSmQMpG4qkBgAAC1gxNqH6sfBgjF7X2JDUAABsLx6mW1sxNuG8rm11estmQd+ndctmOq8rSU196FMDALC1+vrCpKckaXT/M9WlXUvTHX4bwn+KqXY8rVs10+8N9JhpkuDS/b/8iSYtLgx4zZxf/iQqv4sdkdQAAGzL3xem9rpMWfkxzV25ter7YM3vrJaXnSGvV5qxrEgHjxyXJB08ckKz396khASXoRhOb9lMh46eqPPY/b/8Cc33gmD7CQBgS8H6wtQWrPmd1QqKSnXjy4VVCY2ZGPxJWu2ERpI89TyGmkhqAAC2FKovTHWhmt9ZpSEN+IwkaZGO3+5IagAAtmR2anWo5ndWaEgDvoY27wNJDQDApoz2hanNbDIUideu7zormvc1dhQKAwBsqW/n1mrTqnmd2pVQaidDlV6f1u84qG8PH2vwSamGNOCzpnlf40ZSAwA2ZuUN2U78x7jNJjSS9J9qz6nvOHhDTkr5G/CVeY7VWxvjkpQeoAFfQ56LU9h+AgCbKigq1cAHVmn0/HX67V82aPT8dRr4wKqonPCJJf8JIaNFwrXNfvtUsW2g12nISamGNOCzonlfY0dSAwA2FIkbsh2YOcYdSKnnmNaVHAj7lFIo/gZ86e6a20Tp7iTNG5MTdAWoIc8F208AYDuhjg27dOqGPDQr3XH/qjdzjDuYtdv3Gz5pFM6cpbzsDA3NSg9ra7Ahz23sSGoAwGbMHP11wuDD6nVDX+89bNGrGksQGnLSqEmCK+zPvyHPbcxIagDAZhrT0d/6Cnkbwl9sm9utrZ74YFvI6zlpZC8kNQBgM43l6G+guU4N4ZM0PDtd8p0aerm3vP6TRtKpadicNLIXCoUBwAYqvT6tLTmgZRv2yOv1KT0lKeAGikunjiXb+YZsRUFwIM+v2alrn/tUx05WBn39/xw9oRXFZRGIAJHCSg0AxLn6tmBOb9msqii4+o3ZKUd/rSoIDqa+oZHVObng2qlYqQGAOBbo6LZ/YrO7ZbMajzvl6G881AMxa8l+WKkBgDhl5Oh2UtMEvXTDAO3/rsJRR3/jqR4oHhIsGENSAwBxysjR7bLyCiW4XBrZu2P0AouCUCMDoimeEiwEx/YTAMSpxnR0u7bqIwPCNTa3s+65pGfYz3dCwXVjQ1IDAHGqsRzdDiQvO0O/viAz7OcPz87Q+PMzleEOfFLMj1lLzkBSAwBxyr8F4+Sj28FUen16c6P5GVbVP5dQQyJdkn5zQSazlhyCmhoAiFP+G/LkxYWOPbodTDjHuuv7XPxDImsfi093Jyl/RJbysjN0Z15PR81aqj5awgm/j1EkNQAQx4zckONduDfYcGqFAn0uoYZEOmnWUn19jTJs9L+XhnD5fL5YF5ZHTXl5udxutzwej1JSUmIdDgAYZtd/eTfkBru25IBGz18X8j3uuaSn2iUn2upziZRAoyX8n4hdt9SM3r9ZqQEAG7DjSkKgG2yZ55gmLy7UvDE5QVdPQh3r9g+nHH9+ZqNOZPyM9DVyeodkkhoAgOWM3GCnvf6l7nuzWGXl9a/iNPaaIrOM9DXyd0i2W4JsFKefAACWM3KDPXT0RI2ERvphFaeg6NSpJ39NkZ1PJ1UfRrq25IAqvZGp+mjMfY38WKkBAFgu3Bun/3ZffZskVJFvPItm0W5j72sksVIDALYWrVUAsxp64yz1HNMTq76u+t5fUzSyd0fldmtrWUITyc8v0DDS2qtRVmnsfY0kVmoAwLbi+eiuFbOb5q78Wj3SkyP2u0Ty84tF0S41SKzUAIAtRXIVwIrVi2CdfM2Yuby4zvtbEV+kV1HMFO1ayQk1SA3BSg0A2EwkVwGsXL0I1Diw9ipCMLVP61gRXzRWUWJZtGvnGqSGYqUGAGwmUqsAkVi9yMvO0Oq7BmvJxPM04fwuVfGZ4b/xWxVfNFZRYl20G6kapHhnOqlZsGCBjh49GolYAAAGRGIVINTqhVT/VpARTRJc6p/ZRu8UlZl+rnTqxl/p9em+N62JLxqrKBTtxobppGbatGlKT0/XhAkT9Pe//z0SMQEAgojEKoDZ1QuzdS3hDqf03/ifWPV1nZ42weILJhqrKKGmg0vOL9qNBdNJzZ49e7Rw4ULt379fgwYN0o9//GM98MADKisLLwMHAJgTiVUAM6sXBUWlGvjAKo2ev06//csGjZ6/TgMfWBV0+8fsqkf1G/+K4jLNXfl10OvNvE+0VlEae9FuLJhOapo2bapf/OIXWrZsmXbv3q2JEyfqpZde0plnnqnLLrtMy5Ytk9frjUSsAABFZhXA6KrEzv1HwqprMbvq4b/xD81K18zlxYafZ+R9ormKUr2m6NFremvJxPO0+q7BJDQR0qBC4fbt22vgwIHKzc1VQkKCvvzyS40bN07dunXThx9+aFGIAGBcvDajs5rVqwBGVy+WrN8VVl1LqNeXpDatmmnu1TVv/Ga2rcysrkRzFaWxFu3GQlhHuvfu3asXX3xRCxYs0Pbt2zVq1Ci99dZbGjJkiI4cOaJZs2Zp3Lhx+te//mV1vAAQUDw3o4sEK4/uGmncds25Z2ruyq0BXyPYwEQjr//HX/ykzn8nM9tWZldXGvPRZ6dy+Xw+U/+MGTFihP72t7/pRz/6kW644QaNHTtWbdrUzIy//fZbpaenx902VHl5udxutzwej1JSUmIdDgAL+Y/71v4LzX97oobBmECJ4TXnnqkDRyq0aG3of6w+ek1vjezd0dTrB0o815Yc0Oj560K+561DfqTfDuke8jrYk9H7t+mVmrS0NH300UfKzc0NeE1qaqp27Nhh9qUBICyxaElvJrZgKwGhfh5ttVcvdu4/oiXrdwVdoaktWF2L2dURI+MW0lMSddPgswzHB+cyndRceOGFysnJqfP48ePH9Ze//EVjx46Vy+VS586dLQkQAEIxcxy59rZIJIValYjX7TJ/DUhBUakeWfm14WZ5Lp2qSQlV1+J/faOxhNq2uu+ys9kygqQwCoWvv/56eTyeOo8fPnxY119/vSVBAYAZsWxJH0io7rdz3imO6gRns4KtftUnkr1XOBoNo0yv1Ph8Prlcdf8H++9//1tut9uSoADAjFi3pK/NSHfe+Z/siMvtMj+zzfLSI7zCRFEvjDCc1PTp00cul0sul0sXXXSRmjb94amVlZXasWOH8vLyIhIkAAQTqu7C6LaIVYwkBMFOmsdqu6w6o6taY3M7a3h2RlQSDDPbVmicDCc1o0aNkiRt2LBBw4YN02mnnVb1s+bNm6tLly66/PLLLQ8QAEIxUncRzZb0Vm1zRXO7rDajq1rDszNINBA3DCc1+fn5kqQuXbro6quvVlJSdJZxAcAIf91F7cLbSG+L1Meqba5obZfVJ95WvwAjTNfUjBs3LhJxBLVz507Nnj1bq1atUllZmTp06KAxY8bo7rvvVvPmzaMeD4D4FC91F0aOISe4JJ9PUU8YjB4hj7fVL8AIQ0lNmzZttHXrVrVr106tW7eut1DY7+DB0BNSzdq8ebO8Xq+efvppnXXWWSoqKtLEiRN15MgRPfzww5a/HwD7ioe6CyMJwcSfZuqZj3dENWEwe4Q8nla/ACMMdRReuHChrrnmGiUmJuqFF14ImtREayXnoYce0rx587R9+3bDz6GjMIBosqJPjVXN+RrScTneGgSi8TF6/zY9JiFezJgxQwUFBfrnP/8Z8JqKigpVVFRUfV9eXq5OnTqR1ACImoZ0FLaqOV+l16eBD6wKeCLLv921+q7BJCuIS5aOSSgvLzf8xtFIFrZt26bHH3885NbTnDlzNHPmzIjHAwCBhNoOC/TzQCsr/uZ8ZprOxWvHZcBqhjoKn3766WrdunXQL/81ZkybNq2q902gr82bN9d4zp49e5SXl6crr7xSEydODPr606dPl8fjqfravXu3qfgAIBZCNe/z6VRzvspgzW6qiceOy0AkGFqp+eCDDyLy5rfddpvGjx8f9JquXbtW/fmbb77Rz372M/2///f/9Mwzz4R8/cTERCUmJjY0TACIKiPN+0o9x/TEqq/12yE/Cvl68dZxGYgUQ0nNhRdeGJE3T01NVWpqqqFr9+zZo5/97Gfq27evFixYoIQE02OrAMAWjK6YzF35tXqkJ4fchqLnDBoLQ0nNF198oezsbCUkJOiLL74Ieu0555xjSWDV7dmzR4MGDVLnzp318MMPa9++fVU/S09Pt/z9ACCWzKyYGJkRRc8ZNBaGkprevXurrKxMaWlp6t27t1wul+o7NOVyuVRZWWl5kCtWrNC2bdu0bds2nXHGGTV+ZtPDWwAQkH9lxchASaMFvvScQWNg6Ej3v/71L5155plyuVz617/+FfTazp07Wxac1ehTA8AuCopKNWlxoaFrH72mt0b27mjoWnrOwI4sPdJdPVGJ56QFAJwiLztDl56Tobe+KA15rZntqnjouAxEiunZT5K0ZcsWPf7449q0aZMkqWfPnpoyZYp69OhhaXAA0FhVen36505jY2f+c6Qi5DWs0KAxMJ3ULF26VNdcc4369eun3NxcSdK6deuUnZ2tv/zlL7r88sstDxIAGpv1Ow6qrDx0siJJs9/epGHZGQGTFKs6EwPxzvSYhG7duunaa6/VrFmzajyen5+vxYsXq6SkxNIArURNDQC7WLZhj377lw2Gr18y8TxTnYmNzHwC4oXR+7fpZi+lpaUaO3ZsncfHjBmj0tLQe78AgNDMNsKrr7dNqM7EkrnOxEC8M53UDBo0SJ988kmdx1evXq2f/vSnlgQFAI1d/8w2Or1lM8PX15cEmZn5BDiBoZqaN998s+rPl112me666y599tlnOu+88ySdqql59dVXGR4JANVEqzg3I0A3YGY+obExVFNjdCRBpJrvWYWaGgDR0tDi3LUlBzR6/jpD7/VUgLoYo68RqB4HiBeW1tR4vV5DX/Gc0ABAtPiLc2tv/ZR5jmny4kIVFIWuPzS6ejLh/C4Bk6S+nVurTavAW1guBV7lAeyIqZAAYCGrinONFgoPyap//l1BUakufOgDHTxyot6fM/MJTmS6T03to9y13XvvvWEHAwB2Z6Y4N9iWT0Mmawc6xl0dM5/gRKaTmr/+9a81vj9x4oR27Nihpk2bqlu3biQ1ABo1q4pzg03W9qtvlSXYSpFf21bN9dEdP1PzpizWw1lMJzWff/55ncfKy8s1fvx4/eIXv7AkKACwk+qnnPYfNtYF2Mj2kn+y9rTXv9ShozW3kdwBjnuHWimSpANHjuuzf/2H4mA4Tlizn2pLSUnRzJkzNWLECF133XVWvCQA2EJ9p5wSXFKgkpnq20ZGj3zXTmj8j01aXKg//ypHF5/zwxYSx7jRmFmS1EiSx+ORx+Ox6uUAIO4Fql0JltBIp7aNVhSXhTzy7d9KCuamJYV6Qn108TkdJBkvMDbbsRiwA9NJzWOPPVbje5/Pp9LSUr344osaPny4ZYEBQDwzUrtSe8XGX5wrqd5kyH/k2z+PychWktcn/e/Ln+vWb4/opsFnNajAGLA700nN3Llza3yfkJCg1NRUjRs3TtOnT7csMACINjMdgI0mHPdc0lPtkhOrXk+SBj6wKuCRb5dOHfkempVuaoto7sqtevnTnZo5MjtggTHHuOF0ppOaHTt2RCIOAAgpkmMHzHYANppwtEtO1MjeHau+X1tywPCRb7NbRHsPH9ekxYV6akyO5o3JqfP7cIwbTmdZTQ0ARFJDxw6Eem0j20HVhVu7YqaQ99JzOijDnRRyRai2aa9/qc9mDNXQrPSIJYHRmmsFmGEoqfnlL39p+AVff/31sIMBgPqEk3T4Bbr5+h8v83yv2W9vMrQdVP2mbbZ2xf9+X+/9ztDvnJacVNWrZtLiQkPP8Tt09ITWlRzQ+d3bReTYdiQTTKAhDCU1brc70nEAQL1CjR0IlHRIgW++l/XK0JsbSw2tgPi3g9aVHFBCgqtGcmS0dqW+OAKpnQzlZWfoz7/K0U1LCgOeqqrP2u37dX73dsafYFBDEkwg0gwlNQsWLIh0HABQr3DHDgS6+ZZ6junpj83XBt74cqEOff9Dvxj/ykSo2hUjIwv8AhXyXnxOhp5QH/3vy3Wbn4Z+Nes0JMEEoiHsmpp9+/Zpy5YtkqQePXooNTXVsqAAwC+cZnJGjlubVT2hkWquTKy+a3DALS4zcQQr5L34nA56KsGl6a9/qf/U04yvtgEROLJt1VwrIFJMJzVHjhzRlClTtGjRInm9XklSkyZNNHbsWD3++ONq2bKl5UECaLzCKcg1cty6oWqvTNR3Ezcax00/66bzz0oNWWybl52hwT9ur5zZK/Rdxcmgr3nHa1/ovsusrXGhWzHinelpZlOnTtVHH32k5cuX69ChQzp06JCWLVumjz76SLfddlskYgTQiPkLcgPd6l06tRVUvZlctG6q1Vcm6mM0ju7tk5Xbra2hLZvmTRP08JXnhLxub/mplaSColJDMRhBt2LEO9NJzdKlS/Xcc89p+PDhSklJUUpKii6++GLNnz9fr732WiRiBBBDlV6f1pYc0LINe7S25IAqzVSrWsB/AkiqWyUSqAbFqpvq6S3qHxpZW6DkJVJJQF52hp4ak6P2yYkBr/H/V5q5vNiy/2bhJJhANJnefjp69Kjat29f5/G0tDQdPXrUkqAAxIf6Tu2c3qKZrj8/UzcNPitqxaD+adVGm8mFOm4dTJtWzXTPpWcrPSVJXp9P1z77acjnBEpKIjmyIC87Q8lJzYLGZ3WNiz/BpFsx4pXplZrc3Fzl5+fr2LEf/mL5/vvvNXPmTOXm5loaHIDY8Z/aqV0Tcuj7E5q7cqv6/n6FpVsboeRlZ2j1XYO1ZOJ5evSa3loy8TytvmtwvTUj1Vd3zBqXm6lf9Omo3G5tdV7Xtg1amQhnlcmM/d9VGLrOyu04f4KZ7q6ZyKW7kzjOjZhz+Xw+U/+QKSoq0rBhw1RRUaFevXpJkjZu3KikpCT97W9/09lnnx2RQK1QXl4ut9stj8ejlJSUWIcDxK1Kr08DH1hlqMj1qTi+kRUUlep3f/1SB4+EPi3kNza3s4ZnZ1QV7fqTO6n+lQkjN/JINatbW3JAo+evC3ndkonnWX4aiY7CiCaj92/TSY10agvqpZde0ubNmyVJPXv21LXXXqsWLVqEH3EUkNQAxhi9WUqnbs6r7xoctze0v36+R7e+ssH08zJq9ZppaFISiSTAn3yG2t6K5/8+gBFG799h9alp2bKlJk6cGHZwAOKbme0KIzUbsfxXfXpKeEXDZZ5jmrS4ULcO6a4u7Vrp4St7ST5p/5GKsH6HJgkuy1dLqHEBajKd1CxcuFDt2rXTJZdcIkm688479cwzzygrK0tLlixR586dLQ8SQHSZPY0TLAmK9ZygcIuG/dfOXfl11WP+uOOpsZzZImrAyUxvP/Xo0UPz5s3T4MGDtXbtWl100UV65JFH9NZbb6lp06ZxPdCS7SfAGDM1NVLgmo1AIwLM1KNYIVBdjFnRjtsMalzgZEbv36ZPP+3evVtnnXWWJOmNN97QFVdcoV//+teaM2eOPvnkk/AjBhA3/NsaoW6JwU7/hJoTJFnbQyWYQCd2WjZvYup1oh23Gf7trZG9Oxpu5Ac4jemk5rTTTtOBAwckSe+9956GDh0qSUpKStL3339vbXQAYsafCJzesv4GdKFqNszMCYqG+o6Ez7+un+nXiXbcAIwzXVMzdOhQ3XDDDerTp4+2bt2qiy++WJL01VdfqUuXLlbHByCG8rIzNDQrXU+s+loL1uysMdQxVM1GPM4Jql2sW+n1hd2kzw7zjdiSQmNjOql58sknNWPGDO3evVtLly5V27an/oL47LPPNHr0aMsDBBBbTRJc+u2QH+mmwd1N3SDtMCco2OmhUOJ9vlGsC7SBWAirT41dUSgMRE+oHiqSlOCSnhjdRxef0yGqsdVWXwIQiB16v8RLgTZglYg237MrkhogugLdXKtzKT5ustW3anbuP6q5K7cGvLZ6F+VQWzzR3gIKdXLNDkkZUFtEm+8BaFzCvTHnZWfoyV/l6KYlhQp2WGjm8mINzUqP6U22er2N0ZlWobZ4YrEFZKZAO5767QBWIKkBEFRDb8ytWzUPmtDE203WfxQ9EJdOJWFer083vvx5nVWoMs8xTV5cqF9fkKlnPt4R8OeRWp2KxwJtIFpMH+kG0HgEmtTtvzEbWdGw203W6ErHjGVFAXvw+CTN/6RuQuP/uRS5XjdmC7QrvT6tLTmgZRv2aG3JgbjrvwOYwUoNgHqFap7nX7EItm1U6fVp/+EKQ+8XL6eJjCZXoSZ/x2p1KtRYCH9NTf/MNpyQguMYSmr69Okjl8vYXndhYWGDAgIQHxpam2H0RFH1m2w8iGZyFYnVKaNDLlcUl9VbxB3p7TEgkgxtP40aNUojR47UyJEjNWzYMJWUlCgxMVGDBg3SoEGDlJSUpJKSEg0bNizS8QKIkoZsGwXatqot0pOkw9la8a90BIrGJaltq+aWxBepBCrQWIh0d5LmjcnR0Kz0uBlhAVjJ0EpNfn5+1Z9vuOEG3XzzzZo9e3ada3bv3m1tdABiJtzmecG2rWqL5CTpcLdWjKx0zB6ZrdlvF4fVidj/OpFenfJ3g67v1NrakgOckIIjmS4UfvXVVzV27Ng6j48ZM0ZLly61JCgAsWdkxaK+YZahtq387rmkp1bfNThiCU1DCpxDrXRcfE6G8kdkSVLIoZ+1RXp1qrpAQy7tVrwNGGW6ULhFixZas2aNunfvXuPxNWvWKCkpPgr9ADSc0dqM2jdmozfCdsmJEdtyamiBsxR8pcP/83ljcgx3IvaL5OqUUXYYYQGEw3RSc8stt2jy5MkqLCxU//79JUmffvqpnn/+ed1zzz2WBwggdgLduIPdmGN9w7Sy+VztAZi1+ROfF9bs0Oy3N4WM7Z5Lemr8+Zkx7+Rr5oQUYCemk5pp06apa9euevTRR7V48WJJUs+ePbVgwQJdddVVlgcIILZCrVjUFusbZrS3VpokuNQuOdHQtZFanTIr3FU4IN6F1afmqquuIoEBGpFQKxa1r43lDTMWK0XReE+rZ0iFswoHxLuwkppDhw7ptdde0/bt23X77berTZs2KiwsVPv27dWxY0erYwRgM7G8YcZipSjS7xmpJnlmV+GAeGd6SvcXX3yhIUOGyO12a+fOndqyZYu6du2qGTNmaNeuXVq0aFGkYm0wpnQD0RXtCdV+/tNPUv0rRZFoLBep9ww06TySvwsQb4zev00f6Z46darGjx+vr7/+usZpp4svvlgff/xxeNECcKRAR4ojLdSR7HCTgGDN/CLxnqFOckk0yQOqM7399I9//ENPP/10ncc7duyosrIyS4ICgIayemvFyBaQ1e9p5UkuoDEwndQkJiaqvLy8zuNbt25VamqqJUEBgBXMFDgHE2gLqL45SVa9p0STPMAs09tPl112mWbNmqUTJ05NqHW5XNq1a5fuuusuXX755ZYHCCBywpmN1NjEcgso1j1/ALsxvVLzf//3f7riiiuUlpam77//XhdeeKHKysqUm5urP/zhD5GIsYaKigoNGDBAGzdu1Oeff67evXtH/D2BeNWQQtxInaixMsZ4EMstoFj3/AHsxnRS43a7tWLFCq1Zs0YbN27Ud999p5ycHA0ZMiQS8dVx5513qkOHDtq4cWNU3g+IVw1JSsxsp8QqxngRyy2gWPf8AezG9PbTokWLVFFRofPPP1//+7//qzvvvFNDhgzR8ePHI36c+91339V7772nhx9+OKLvA8S7hgxsjNZ2SkOHSsaLWG8BReokF+BEpvvUNGnSRKWlpUpLS6vx+IEDB5SWlqbKykpLA/Tbu3ev+vbtqzfeeEPt2rVTZmZmyO2niooKVVRUVH1fXl6uTp060acGtlbp9WngA6sCbon4tyRW3zW43n/Bry05oNHz14V8nyUTzwt7O6WhMcYT/+8Sagso0r+L3bfxgIaIWJ8an88nl6vu/5H+/e9/y+12m305w+85fvx4TZo0Sf369TP8vDlz5sjtdld9derUKSLxAdFkpsajPka3SVYWh9+ioaExxhP/FpD0w5aPXzS3gGLV8wewE8M1NX369JHL5ZLL5dJFF12kpk1/eGplZaV27NihvLw8U28+bdo0PfDAA0Gv2bRpk9577z0dPnxY06dPN/X606dP19SpU6u+96/UAGbE27+QG1rjYXSb5Lk1O3VuZpuwtjecdhSZOUmAPRhOakaNGiVJ2rBhg4YNG6bTTjut6mfNmzdXly5dTB/pvu222zR+/Pig13Tt2lWrVq3S2rVrlZhYcxJuv379dO2112rhwoX1PjcxMbHOcwAz4rHQtaE1HqFO1FQ3c3mxhmalm07iYl2HEgnMSQLin+mamoULF+qaa66JarKwa9euGg3/vvnmGw0bNkyvvfaaBgwYoDPOOMPQ6zD7CWbE68wdK2o8CopKNem/c4pCCae2Jl7qUAA4Q8RqarKysrRhw4Y6j3/66af65z//afblDDnzzDOVnZ1d9fWjH/1IktStWzfDCQ1gRjzP3LGixiMvO0P/c34XQ+8XzhZRvNShAGhcTCc1N954o3bv3l3n8T179ujGG2+0JCgg1uK90NWKY75Ds9INvVe4W0RGYqSjMQArmW6+V1xcrJycnDqP9+nTR8XFxZYEFUqXLl1kctcMMMUOha4NrfFoaLdaIwXUwWKMx3olAPYW1kDLvXv3qmvXrjUeLy0trXEiCrAzuxS6NmR4YkO61ZpJSOqLMVodjQE0Lqa3n37+859r+vTp8ng8VY8dOnRIv/vd7zR06FBLgwNixb+KEWjNw6VTN3G7z9wJZxuroZ2C47leCYC9mV5aefjhh3XBBReoc+fO6tOnj6RTx7zbt2+vF1980fIAgVhoTDN3zGxjhUpIXAp9DDyWAyIBOJvppKZjx4764osv9NJLL2njxo1q0aKFrr/+eo0ePVrNmjWLRIxATDSmhmtGt7GsSEjsUK8EwJ7CKoJp1aqVfv3rX1sdCxB3aLhWkxUJiV3qlQDYj6Gk5s0339Tw4cPVrFkzvfnmm0GvveyyyywJDIgXDSnGjTcNHflgRULS0FNXABCIoaRm1KhRKisrU1paWtW4hPq4XK6ITekG0DBWHKG2IiFpTPVKAKLL0Oknr9ertLS0qj8H+iKhAeJTQ08s+VnVKdiK5oEAUJvp2U92xuwnNEb+OUyBCnzDmcMUaNXnmnPPVJd2LQ1vbcXbBHQA8cno/dvQ9tNjjz1m+I1vvvlmw9cCiLxIHKGuXUC9c/8RLVm/S3NXbq26xsjWlpPqlQDEnqGkZu7cuTW+37dvn44eParTTz9d0qnmey1btlRaWhpJDRBnInWE2p+QFBSV6pGVX9MdGEDMGaqp2bFjR9XXH/7wB/Xu3VubNm3SwYMHdfDgQW3atEk5OTmaPXt2pOMFYFIkj1DTHRhAPDE9JuGee+7R448/rh49elQ91qNHD82dO1czZsywNDgADRfJkQ/xPs0cQONiOqkpLS3VyZMn6zxeWVmpvXv3WhIUAOv4TywFWivxKfwj1PHaHbjS69PakgNatmGP1pYcYKUIaCRMdxS+6KKL9Jvf/EbPPvuscnJyJEmfffaZJk+erCFDhlgeIIDYMHIyyeiW1c79RyIRYr2s6McDwJ5Mr9Q8//zzSk9PV79+/ZSYmKjExET1799f7du317PPPhuJGAE0gL/uJRD/EMrqqxkFRaUa+MAqjZ6/Tr/9ywaNnr9OAx9YVaefTf/MNkpPCZ3YLFm/KyqrJVb14wFgT6aTmtTUVL3zzjvavHmzXn31Vb366qvatGmT3nnnnaoGfQDih9m6FzOJQZMEl0b3PzNkDGXlFRGvq6FoGUBYAy0lqUuXLvL5fOrWrZuaNg37ZQBEmJm6l1CJgX9VZ2hWetVWVJd2LS2NI1yR6McDwF5Mr9QcPXpUEyZMUMuWLXX22Wdr165dkqQpU6bo/vvvtzxAAA1j5kh3OKeZojl1O1gBcLwWLQOIHtNJzfTp07Vx40Z9+OGHSkr64S+pIUOG6JVXXrE0OAANZ+ZIdziJQSSPjFcXqs4nmskVgPhkOql544039MQTT2jgwIFyuX74a+zss89WSUmJpcEBaDgzQyjDSQysGnIZjJE6n2glVwDil+mkZt++ffUWBB85cqRGkgMgfhidih1uYhDJqdtGC4AlRTy5AhDfTFf49uvXT2+//bamTJkiSVWJzLPPPqvc3FxrowNgmdpDKOvrPeNfdZm8uFAuqUYiESoxMPL64TBT5+NPrmr3qUmnTw3QKJhOav74xz9q+PDhKi4u1smTJ/Xoo4+quLhYf//73/XRRx9FIkYAFjEyFbshiUGw1zfSzK8+Zut8IpVcAYh/ppOagQMHauPGjZozZ45+8pOf6L333lNOTo7Wrl2rn/zkJ5GIEUCUWZ0YNKTLb7h1PhzbBhofl8/nM9yJ6sSJE/rNb36je+65R5mZmZGMKyLKy8vldrvl8XiUkpIS63CARsFf5Fv7Lxp/ehSq5qbS69PAB1apzHOs3roal06tIq2+azCrMYBDGb1/myoUbtasmZYuXdrg4AA0DlZ0+Y3G6SoAzmD69NOoUaP0xhtvRCAUwDmYEn1KOM386hPJ01UAnMN0TU337t01a9YsrVmzRn379lWrVq1q/Pzmm2+2LDjAjpgS/QMru/xSAAwgFFM1NZKC1tK4XC5t3769wUFFCjU1iLSG1o84zdqSAxo9f13I65ZMPI/CXgABGb1/m16p2bFjR4MCA5wqnGGQTudv5heqyJcuvwCsYLqmpjqfzyeTCz1AVEWztsWq+hEnocgXQDSFldQ899xzys7OVlJSkpKSkpSdna1nn33W6tiABgk1ANFqkZwSbefCY4p8AUSL6e2ne++9V3/60580ZcqUqrEIa9eu1a233qpdu3Zp1qxZlgcJmBWotsU/ADESN9NITYmur/C4TavmGtW7g4ZmpduiWJYiXwDRYLpQODU1VY899phGjx5d4/ElS5ZoypQp2r9/v6UBWolC4cbB36wt0FZQpJq1RaJJXKDkrLrGerIKQOMRkeZ70qmuwv369avzeN++fXXy5EmzLwdYLla1LVbXjwQrPK6u9L+rT5HaVgMAuzCd1Fx33XWaN29encefeeYZXXvttZYEBTREJGtb/ALVuFhZPxIqOastVGdeAHA60zU10qlC4ffee0/nnXeeJOnTTz/Vrl27NHbsWE2dOrXquj/96U/WRAmYEKnaFr9QzfWsqh8xk3RVX32i3wuAxsp0UlNUVKScnBxJUklJiSSpXbt2ateunYqKiqquc7koAERsRLI3itECZCumRIeTdDVk9QkA7M50UvPBBx9EIg7AMv7alsmLC+WSaiQgDemNEu3meqGSs/qEu/oEAE7QoOZ7QLyKRG+UaBcgBys8rs2lU1tgdOYF0JiFVVMD2IHVvVGiUYBcmz85q13DUx2deQHgFJIaOJoVtS1+kS5ADqR6crayuEx/3bBHB4+cqPp5On1qAEASSQ1gWCyHM/qTs9xubfW7S7LozAsA9SCpAQwKVoCs/35/zblnRiUOjm0DQF0UCgMmBCpA9pu7cmtEh2YCAAIjqQHqEWwqdl52hlbfNVi3Dule73PLGFsAADHB9hNQS6iOwX5/+cfuep8fiZ41AIDQWKkBqvF3DK59fLr26kushmYCAAIjqQH+K1THYOmHoZGx6FkDAAiOpAb4LzOrL7HqWQMACIykBvgvM6sv/p41gaplGFsAANFHUgP8l5nVl2BzmRhbAACxQVID/JfZ1ZdIDM0EAISPI93AfwXrGBxo9cXqoZkAgPC5fD5ffYc9HKm8vFxut1sej0cpKSmxDgdxymifGgBAdBi9f7NSA9TC6gsA2JOtamrefvttDRgwQC1atFDr1q01atSoWIcEh/IPjRzZu6Nyu7UloQEAG7DNSs3SpUs1ceJE/fGPf9TgwYN18uRJFRUVxTosAAAQJ2yR1Jw8eVK//e1v9dBDD2nChAlVj2dlZcUwKgAAEE9ssf1UWFioPXv2KCEhQX369FFGRoaGDx8ecqWmoqJC5eXlNb4AAIAz2SKp2b59uyTpvvvu04wZM/TWW2+pdevWGjRokA4eDDwwcM6cOXK73VVfnTp1ilbIAAAgymKa1EybNk0ulyvo1+bNm+X1eiVJd999ty6//HL17dtXCxYskMvl0quvvhrw9adPny6Px1P1tXv37mj9agAAIMpiWlNz2223afz48UGv6dq1q0pLSyXVrKFJTExU165dtWvXroDPTUxMVGJioiWxAgCA+BbTpCY1NVWpqakhr+vbt68SExO1ZcsWDRw4UJJ04sQJ7dy5U507d450mAAAwAZscfopJSVFkyZNUn5+vjp16qTOnTvroYcekiRdeeWVMY4OAADEA1skNZL00EMPqWnTprruuuv0/fffa8CAAVq1apVat24d69AAAEAcYPYTAACIa0bv37Y40g0AABAKSQ0AAHAEkhoAAOAIJDUAAMARSGoAAIAjkNQAAABHIKkBAACOQFIDAAAcwTYdhQGjKr0+rd9xUN8ePqa05CT1z2yjJgmuWIcFAIgwkho4SkFRqWYuL1ap51jVYxnuJOWPyFJedkYMIwMARBrbT3CMgqJSTV5cWCOhkaQyzzFNXlyogqLSGEUGAIgGkho4QqXXp5nLi1XfIDP/YzOXF6vS22hGnQFAo0NSA0dYv+NgnRWa6nySSj3HtH7HwegFBQCIKpIaOMK3hwMnNOFcBwCwH5IaOEJacpKl1wEA7IekBo7QP7ONMtxJCnRw26VTp6D6Z7aJZlgAgCgiqYEjNElwKX9EliTVSWz83+ePyKJfDQA4GEkNHCMvO0PzxuQo3V1ziyndnaR5Y3LoUwMADkfzPThKXnaGhmal01EYABohkho4TpMEl3K7tY11GACAKGP7CQAAOAJJDQAAcASSGgAA4AgkNQAAwBFIagAAgCNw+skhKr0+jjEDABo1khoHKCgq1czlxTWmVGe4k5Q/IouGcwCARoPtJ5srKCrV5MWFNRIaSSrzHNPkxYUqKCqNUWQAAEQXSY2NVXp9mrm8WL56fuZ/bObyYlV667sCAABnIamxsfU7DtZZoanOJ6nUc0zrdxyMXlAAAMQISY2NfXs4cEITznUAANgZSY2NpSUnhb7IxHUAANgZSY2N9c9sowx3kgId3Hbp1Cmo/pltohkWAAAxQVJjY00SXMofkSVJdRIb//f5I7Is6VdT6fVpbckBLduwR2tLDlB8DACIO/Spsbm87AzNG5NTp09NuoV9auiDAwCwA5fP52s0/+QuLy+X2+2Wx+NRSkpKrMOxVKQ6Cvv74NT+H4n/leeNySGxAQBElNH7Nys1DtEkwaXcbm0tfc1QfXBcOtUHZ2hWOiMZAAAxR00NAqIPDgDATkhqEBB9cAAAdkJSg4DogwMAsBOSGgREHxwAgJ2Q1CCgaPbBAQCgoUhqEJS/D066u+YWU7o7iePcAIC4wpFuhJSXnaGhWekR6YMDAIBVSGpgSCT64AAAYCW2nwAAgCOQ1AAAAEcgqQEAAI5AUgMAAByBQuEYidRUbQAAGiuSmhgoKCrVzOXFNYZFZriTlD8ii74vAACEie2nKCsoKtXkxYV1pl+XeY5p8uJCFRSVxigyAADsjaQmiiq9Ps1cXixfPT/zPzZzebEqvfVdAQAAgiGpiaL1Ow7WWaGpziep1HNM63ccjF5QAAA4BElNFH17OHBCE851AADgByQ1UZSWnBT6IhPXAQCAH5DURFH/zDbKcCcp0MFtl06dguqf2SaaYQEA4Ai2SWq2bt2qkSNHql27dkpJSdHAgQP1wQcfxDosU5okuJQ/IkuS6iQ2/u/zR2TRrwYAgDDYJqm59NJLdfLkSa1atUqfffaZevXqpUsvvVRlZWWxDs2UvOwMzRuTo3R3zS2mdHeS5o3JoU8NAABhcvl8vrg/P7x//36lpqbq448/1k9/+lNJ0uHDh5WSkqIVK1ZoyJAhhl6nvLxcbrdbHo9HKSkpkQw5JDoKAwBgjNH7ty06Crdt21Y9evTQokWLlJOTo8TERD399NNKS0tT3759Az6voqJCFRUVVd+Xl5dHI1xDmiS4lNutbazDAADAMWyR1LhcLq1cuVKjRo1ScnKyEhISlJaWpoKCArVu3Trg8+bMmaOZM2dGMVIAABArMa2pmTZtmlwuV9CvzZs3y+fz6cYbb1RaWpo++eQTrV+/XqNGjdKIESNUWhp4rMD06dPl8Xiqvnbv3h3F3w4AAERTTGtq9u3bpwMHDgS9pmvXrvrkk0/085//XP/5z39q7KV1795dEyZM0LRp0wy9XzzV1AAAAGNsUVOTmpqq1NTUkNcdPXpUkpSQUHNhKSEhQV6vNyKxAQAAe7HFke7c3Fy1bt1a48aN08aNG7V161bdcccd2rFjhy655JJYhwcAAOKALZKadu3aqaCgQN99950GDx6sfv36afXq1Vq2bJl69eoV6/AAAEAcsEWfGqtEoqaGfjMAAESWLWpq7K6gqFQzlxer1PPDVO0Md5LyR2TRGRgAgCizxfZTPCooKtXkxYU1EhpJKvMc0+TFhSooCnzUHAAAWI+kJgyVXp9mLi9Wfft2/sdmLi9WpbfR7OwBABBzJDVhWL/jYJ0Vmup8kko9x7R+x8HoBQUAQCNHUhOGbw8HTmjCuQ4AADQcSU0Y0pKTLL0OAAA0HElNGPpntlGGO0mBDm67dOoUVP/MNtEMCwCARo2kJgxNElzKH5ElSXUSG//3+SOy6FcDAEAUkdSEKS87Q/PG5CjdXXOLKd2dpHljcuhTAwBAlNF8rwHysjM0NCudjsIAAMQBkpoGapLgUm63trEOAwCARo/tJwAA4AgkNQAAwBFIagAAgCOQ1AAAAEcgqQEAAI5AUgMAAByBpAYAADgCSQ0AAHAEkhoAAOAIjaqjsM/nkySVl5fHOBIAAGCU/77tv48H0qiSmsOHD0uSOnXqFONIAACAWYcPH5bb7Q74c5cvVNrjIF6vV998842Sk5Plcjln6GR5ebk6deqk3bt3KyUlJdbhOBafc3TwOUcen3F08Dlbx+fz6fDhw+rQoYMSEgJXzjSqlZqEhASdccYZsQ4jYlJSUvg/ThTwOUcHn3Pk8RlHB5+zNYKt0PhRKAwAAByBpAYAADgCSY0DJCYmKj8/X4mJibEOxdH4nKODzzny+Iyjg885+hpVoTAAAHAuVmoAAIAjkNQAAABHIKkBAACOQFIDAAAcgaTGYbZu3aqRI0eqXbt2SklJ0cCBA/XBBx/EOixHevvttzVgwAC1aNFCrVu31qhRo2IdkmNVVFSod+/ecrlc2rBhQ6zDcZSdO3dqwoQJyszMVIsWLdStWzfl5+fr+PHjsQ7N9p588kl16dJFSUlJGjBggNavXx/rkByPpMZhLr30Up08eVKrVq3SZ599pl69eunSSy9VWVlZrENzlKVLl+q6667T9ddfr40bN2rNmjX61a9+FeuwHOvOO+9Uhw4dYh2GI23evFler1dPP/20vvrqK82dO1dPPfWUfve738U6NFt75ZVXNHXqVOXn56uwsFC9evXSsGHD9O2338Y6NGfzwTH27dvnk+T7+OOPqx4rLy/3SfKtWLEihpE5y4kTJ3wdO3b0Pfvss7EOpVF45513fD/+8Y99X331lU+S7/PPP491SI734IMP+jIzM2Mdhq3179/fd+ONN1Z9X1lZ6evQoYNvzpw5MYzK+VipcZC2bduqR48eWrRokY4cOaKTJ0/q6aefVlpamvr27Rvr8ByjsLBQe/bsUUJCgvr06aOMjAwNHz5cRUVFsQ7Ncfbu3auJEyfqxRdfVMuWLWMdTqPh8XjUpk2bWIdhW8ePH9dnn32mIUOGVD2WkJCgIUOGaO3atTGMzPlIahzE5XJp5cqV+vzzz5WcnKykpCT96U9/UkFBgVq3bh3r8Bxj+/btkqT77rtPM2bM0FtvvaXWrVtr0KBBOnjwYIyjcw6fz6fx48dr0qRJ6tevX6zDaTS2bdumxx9/XL/5zW9iHYpt7d+/X5WVlWrfvn2Nx9u3b08pQISR1NjAtGnT5HK5gn5t3rxZPp9PN954o9LS0vTJJ59o/fr1GjVqlEaMGKHS0tJY/xpxz+jn7PV6JUl33323Lr/8cvXt21cLFiyQy+XSq6++GuPfIv4Z/Zwff/xxHT58WNOnT491yLZk9HOubs+ePcrLy9OVV16piRMnxihyIHyMSbCBffv26cCBA0Gv6dq1qz755BP9/Oc/13/+858aY+67d++uCRMmaNq0aZEO1daMfs5r1qzR4MGD9cknn2jgwIFVPxswYICGDBmiP/zhD5EO1daMfs5XXXWVli9fLpfLVfV4ZWWlmjRpomuvvVYLFy6MdKi2ZvRzbt68uSTpm2++0aBBg3TeeefphRdeUEIC/+YN1/Hjx9WyZUu99tprNU5Fjhs3TocOHdKyZctiF5zDNY11AAgtNTVVqampIa87evSoJNX5yyghIaFqdQGBGf2c+/btq8TERG3ZsqUqqTlx4oR27typzp07RzpM2zP6OT/22GP6/e9/X/X9N998o2HDhumVV17RgAEDIhmiIxj9nKVTKzQ/+9nPqlYdSWgapnnz5urbt6/ef//9qqTG6/Xq/fff10033RTb4ByOpMZBcnNz1bp1a40bN0733nuvWrRoofnz52vHjh265JJLYh2eY6SkpGjSpEnKz89Xp06d1LlzZz300EOSpCuvvDLG0TnHmWeeWeP70047TZLUrVs3nXHGGbEIyZH27NmjQYMGqXPnznr44Ye1b9++qp+lp6fHMDJ7mzp1qsaNG6d+/fqpf//+euSRR3TkyBFdf/31sQ7N0UhqHKRdu3YqKCjQ3XffrcGDB+vEiRM6++yztWzZMvXq1SvW4TnKQw89pKZNm+q6667T999/rwEDBmjVqlUUZMN2VqxYoW3btmnbtm11kkWqE8J39dVXa9++fbr33ntVVlam3r17q6CgoE7xMKxFTQ0AAHAENk4BAIAjkNQAAABHIKkBAACOQFIDAAAcgaQGAAA4AkkNAABwBJIaAADgCCQ1AADAEUhqADjKhx9+KJfLpUOHDhl+zvjx42sMHqzPoEGDdMstt1R936VLFz3yyCNV37tcLr3xxhumYgVgLcYkAKhh0KBB6t27d40bNqTXX39dzZo1C/jz0tLSqjEZO3fuVGZmpj7//HP17t07ShECIKkBYJrP51NlZaWaNm08f4W0adMm6M8Z/gjEHttPAKqMHz9eH330kR599FG5XC65XC7t3Lmzakvn3XffVd++fZWYmKjVq1fXu21zyy23aNCgQVXfe71ezZkzR5mZmWrRooV69eql1157LWgcf/7zn9W9e3clJSWpffv2uuKKK6p+VlFRoZtvvllpaWlKSkrSwIED9Y9//CPga9133311VkseeeQRdenSpc61M2fOVGpqatUk9uPHj1f9rPb2U23Vt58yMzMlSX369JHL5dKgQYP08ccfq1mzZiorK6vxvFtuuUU//elPA74uAONIagBUefTRR5Wbm6uJEyeqtLRUpaWl6tSpU9XPp02bpvvvv1+bNm3SOeecY+g158yZo0WLFumpp57SV199pVtvvVVjxozRRx99VO/1//znP3XzzTdr1qxZ2rJliwoKCnTBBRdU/fzOO+/U0qVLtXDhQhUWFuqss87SsGHDdPDgwQb97u+//742bdqkDz/8UEuWLNHrr7+umTNnhvVa69evlyStXLlSpaWlev3113XBBReoa9euevHFF6uuO3HihF566SX9z//8T4NiB3BK41k7BhCS2+1W8+bN1bJly3q3U2bNmqWhQ4cafr2Kigr98Y9/1MqVK5WbmytJ6tq1q1avXq2nn35aF154YZ3n7Nq1S61atdKll16q5ORkde7cWX369JEkHTlyRPPmzdMLL7yg4cOHS5Lmz5+vFStW6LnnntMdd9wRzq8tSWrevLmef/55tWzZUmeffbZmzZqlO+64Q7Nnz1ZCgrl//6WmpkqS2rZtW+NznDBhghYsWFAV5/Lly3Xs2DFdddVVYccN4Aes1AAwrF+/fqau37Ztm44ePaqhQ4fqtNNOq/patGiRSkpK6n3O0KFD1blzZ3Xt2lXXXXedXnrpJR09elSSVFJSohMnTuj888+vur5Zs2bq37+/Nm3aFP4vJqlXr15q2bJl1fe5ubn67rvvtHv37ga9bnXjx4/Xtm3btG7dOknSCy+8oKuuukqtWrWy7D2AxoyVGgCG1b75JiQkyOfz1XjsxIkTVX/+7rvvJElvv/22OnbsWOO6xMTEet8jOTlZhYWF+vDDD/Xee+/p3nvv1X333Re0biaYUDFGU1pamkaMGKEFCxYoMzNT7777rj788MOYxAI4ESs1AGpo3ry5KisrDV2bmpqq0tLSGo9t2LCh6s9ZWVlKTEzUrl27dNZZZ9X4ql6rU1vTpk01ZMgQPfjgg/riiy+0c+dOrVq1St26dVPz5s21Zs2aqmtPnDihf/zjH8rKygoYY1lZWY3EpnqMfhs3btT3339f9f26det02mmnBY0zkObNm0tSvZ/jDTfcoFdeeUXPPPOMunXrVmPVCUDDsFIDoIYuXbro008/1c6dO3XaaacFPco8ePBgPfTQQ1q0aJFyc3O1ePFiFRUVVdXAJCcn6/bbb9ett94qr9ergQMHyuPxaM2aNUpJSdG4cePqvOZbb72l7du364ILLlDr1q31zjvvyOv1qkePHmrVqpUmT56sO+64Q23atNGZZ56pBx98UEePHtWECRPqjXHQoEHat2+fHnzwQV1xxRUqKCjQu+++q5SUlBrXHT9+XBMmTNCMGTO0c+dO5efn66abbjJdTyOdWpFp0aKFCgoKdMYZZygpKUlut1uSNGzYMKWkpOj3v/+9Zs2aZfq1AQTGSg2AGm6//XY1adJEWVlZSk1N1a5duwJeO2zYMN1zzz268847de655+rw4cMaO3ZsjWtmz56te+65R3PmzFHPnj2Vl5ent99+u+rYc22nn366Xn/9dQ0ePFg9e/bUU089pSVLlujss8+WJN1///26/PLLdd111yknJ0fbtm3T3/72t6rGd7X17NlTf/7zn/Xkk0+qV69eWr9+vW6//fY611100UXq3r27LrjgAl199dW67LLLdN999xn81Gpq2rSpHnvsMT399NPq0KGDRo4cWfWzhIQEjR8/XpWVlXU+KwAN4/LV3mwGAETUhAkTtG/fPr355puxDgVwFLafACBKPB6PvvzyS7388sskNEAEkNQAQJSMHDlS69ev16RJk0z1+wFgDNtPAADAESgUBgAAjkBSAwAAHIGkBgAAOAJJDQAAcASSGgAA4AgkNQAAwBFIagAAgCOQ1AAAAEf4/1mUKw4PuRndAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a test, let's try to use our model on some unseen, possibly out-of-distribution data: Vitamin D and Vitamin C:"
      ],
      "metadata": {
        "id": "vxkglLmobt0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = np.array([features(smi) for smi in ['CC1=C(C(CCC1)(C)C)/C=C/C(=C/C=C/C(=C/CO)/C)/C',\n",
        "                                        'C(C(C1C(=C(C(=O)O1)O)O)O)O']])\n",
        "x2 = x2 / standardize_by\n",
        "model.predict(x2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r6OVL30bAZs",
        "outputId": "4b9ddf83-bc4a-4396-d917-9310fd32ebe3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-4.95416975, -0.49820662])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Neural network\n",
        "Let's use a neural network instead of a simple linear regression:"
      ],
      "metadata": {
        "id": "jVkWo449cCYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLPRegressor()\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
        "\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel(\"true solubility\")\n",
        "plt.ylabel(\"predicted solubility\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "SDatps3eb-dR",
        "outputId": "d0ae532c-2367-444a-f461-2c249564d9a1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.4711409692015118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'predicted solubility')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(x2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hsIJ8KzcdGS",
        "outputId": "d754ecb4-64d0-4e12-e541-888e356ac0b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-5.89601147,  0.70191978])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Custom models, e.g. neural network"
      ],
      "metadata": {
        "id": "HFr0k0HWco60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric as tg\n",
        "from torch_geometric.data import Dataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.nn import global_add_pool"
      ],
      "metadata": {
        "id": "4YXULRQTdOZq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first make a custom NN, similar to the MLPRegressor from scikit-learn:"
      ],
      "metadata": {
        "id": "B32TrEc2Wmd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        super(FeatureDataset, self).__init__()\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def process_key(self, key):\n",
        "        data = tg.data.Data()\n",
        "        data.f = torch.tensor([self.features[key]], dtype=torch.float)\n",
        "        data.y = torch.tensor([self.labels[key]], dtype=torch.float)\n",
        "        return data\n",
        "\n",
        "    def get(self,key):\n",
        "        return self.process_key(key)\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def construct_loader(data_path, shuffle=True, batch_size=50, std=None):\n",
        "    data_df = pd.read_csv(data_path)\n",
        "    x_raw = np.array([features(smi) for smi in data_df['smiles']])\n",
        "    try:\n",
        "      f = x_raw / std\n",
        "    except:\n",
        "      std = np.max(np.abs(x_raw),axis=0)+0.1\n",
        "      f = x_raw / std\n",
        "\n",
        "    labels = data_df.iloc[:, 1].values.astype(np.float32)\n",
        "    dataset = FeatureDataset(f, labels)\n",
        "    loader = DataLoader(dataset=dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=shuffle,\n",
        "                            num_workers=0,\n",
        "                            pin_memory=True,\n",
        "                            sampler=None)\n",
        "    return loader, std\n",
        "\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(NN, self).__init__()\n",
        "\n",
        "        self.hidden_size = 300\n",
        "        self.n1 = nn.Linear(num_features, self.hidden_size)\n",
        "        self.n2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.n3 = nn.Linear(self.hidden_size, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.f\n",
        "        x = self.n1(x)\n",
        "        x = self.n2(x)\n",
        "        x = self.n3(x)\n",
        "        return x.squeeze(-1)\n",
        "\n",
        "def train_epoch(model, loader, optimizer, loss):\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(data)\n",
        "        result = loss(out, data.y)\n",
        "        result.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        loss_all += loss(out, data.y)\n",
        "\n",
        "    return math.sqrt(loss_all / len(loader.dataset))\n",
        "\n",
        "def pred(model, loader, loss):\n",
        "    model.eval()\n",
        "\n",
        "    preds, ys = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            pred = model(data)\n",
        "            preds.extend(pred.cpu().detach().tolist())\n",
        "\n",
        "    return preds\n",
        "\n",
        "def train(folder):\n",
        "    torch.manual_seed(0)\n",
        "    train_loader, std = construct_loader(folder+\"/train_full.csv\", True, batch_size=10000)\n",
        "    val_loader, _ = construct_loader(folder+\"/val_full.csv\", False, std=std)\n",
        "    test_loader, _ = construct_loader(folder+\"/test_full.csv\", False, std=std)\n",
        "\n",
        "\n",
        "    model = NN(210)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss = nn.MSELoss(reduction='sum')\n",
        "    print(model)\n",
        "\n",
        "    for epoch in range(0, 200):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, loss)\n",
        "        preds = pred(model, val_loader, loss)\n",
        "        print(\"Epoch\",epoch,\"  Train RMSE\", train_loss,\"   Val RMSE\", root_mean_squared_error(preds,val_loader.dataset.labels))\n",
        "\n",
        "    preds = pred(model, test_loader, loss)\n",
        "    print(\"Test RMSE\", root_mean_squared_error(preds,test_loader.dataset.labels))\n",
        "    print(\"Test MAE\", mean_absolute_error(preds,test_loader.dataset.labels))\n",
        "\n"
      ],
      "metadata": {
        "id": "RASL8ux0gx5v"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(\"https://github.com/hesther/rxn_workshop/raw/main/data/esol\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tYKuFoiSXwg",
        "outputId": "98c13ea4-2582-449a-b2df-fcf6ea61905a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN(\n",
            "  (n1): Linear(in_features=210, out_features=300, bias=True)\n",
            "  (n2): Linear(in_features=300, out_features=300, bias=True)\n",
            "  (n3): Linear(in_features=300, out_features=1, bias=True)\n",
            ")\n",
            "Epoch 0   Train RMSE 3.7968616877820898    Val RMSE 3.256101157492623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-538a6a5c1cbd>:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  data.f = torch.tensor([self.features[key]], dtype=torch.float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1   Train RMSE 3.4772354674765493    Val RMSE 2.945145494262089\n",
            "Epoch 2   Train RMSE 3.1549058828205268    Val RMSE 2.612967220405371\n",
            "Epoch 3   Train RMSE 2.807338980781469    Val RMSE 2.2661855089971543\n",
            "Epoch 4   Train RMSE 2.437485328043283    Val RMSE 1.9623630634918374\n",
            "Epoch 5   Train RMSE 2.096693560372712    Val RMSE 1.8353918653544812\n",
            "Epoch 6   Train RMSE 1.9153514648098895    Val RMSE 1.9783130922065961\n",
            "Epoch 7   Train RMSE 2.007964963356528    Val RMSE 2.191997612840564\n",
            "Epoch 8   Train RMSE 2.1989352380601983    Val RMSE 2.2410544354371407\n",
            "Epoch 9   Train RMSE 2.242710491155167    Val RMSE 2.1175282495602996\n",
            "Epoch 10   Train RMSE 2.12263941078962    Val RMSE 1.9075271451976918\n",
            "Epoch 11   Train RMSE 1.92541840947869    Val RMSE 1.7042601174176222\n",
            "Epoch 12   Train RMSE 1.7466637963108813    Val RMSE 1.574974152798548\n",
            "Epoch 13   Train RMSE 1.650909306207642    Val RMSE 1.5342522556186826\n",
            "Epoch 14   Train RMSE 1.6432399437052707    Val RMSE 1.545738023389206\n",
            "Epoch 15   Train RMSE 1.6790098020802522    Val RMSE 1.5597146166669975\n",
            "Epoch 16   Train RMSE 1.706410589800317    Val RMSE 1.5439409276414466\n",
            "Epoch 17   Train RMSE 1.6942524914159605    Val RMSE 1.4895361715114546\n",
            "Epoch 18   Train RMSE 1.634678992160133    Val RMSE 1.40621255618748\n",
            "Epoch 19   Train RMSE 1.5374309586321022    Val RMSE 1.3174473710899999\n",
            "Epoch 20   Train RMSE 1.4253027544146344    Val RMSE 1.2543280060348128\n",
            "Epoch 21   Train RMSE 1.3297043777990474    Val RMSE 1.2407588068370485\n",
            "Epoch 22   Train RMSE 1.2789590870097496    Val RMSE 1.2726942799387804\n",
            "Epoch 23   Train RMSE 1.2774370258583687    Val RMSE 1.3154039588355213\n",
            "Epoch 24   Train RMSE 1.2965534684865843    Val RMSE 1.3282565711017997\n",
            "Epoch 25   Train RMSE 1.296310945584512    Val RMSE 1.2911144340894027\n",
            "Epoch 26   Train RMSE 1.2551145821959515    Val RMSE 1.2123610110392977\n",
            "Epoch 27   Train RMSE 1.1810097263332964    Val RMSE 1.1211550502373686\n",
            "Epoch 28   Train RMSE 1.1038582329049553    Val RMSE 1.0509009825818358\n",
            "Epoch 29   Train RMSE 1.0558570875904456    Val RMSE 1.0186389363403743\n",
            "Epoch 30   Train RMSE 1.048161129405549    Val RMSE 1.0142638712786967\n",
            "Epoch 31   Train RMSE 1.0629316462935812    Val RMSE 1.0129796006003637\n",
            "Epoch 32   Train RMSE 1.0708927335975102    Val RMSE 0.9971028697093793\n",
            "Epoch 33   Train RMSE 1.0534226356347096    Val RMSE 0.96699075113401\n",
            "Epoch 34   Train RMSE 1.0115438184878034    Val RMSE 0.9384848326971877\n",
            "Epoch 35   Train RMSE 0.9631720512725014    Val RMSE 0.9300612955604448\n",
            "Epoch 36   Train RMSE 0.9316131139887865    Val RMSE 0.9452039852253378\n",
            "Epoch 37   Train RMSE 0.927266374298932    Val RMSE 0.9663721505526418\n",
            "Epoch 38   Train RMSE 0.9372314704331194    Val RMSE 0.9697691532681331\n",
            "Epoch 39   Train RMSE 0.9380911552733624    Val RMSE 0.9446240845837652\n",
            "Epoch 40   Train RMSE 0.9177423400870985    Val RMSE 0.8997092387821181\n",
            "Epoch 41   Train RMSE 0.8838498257987689    Val RMSE 0.8555397753503323\n",
            "Epoch 42   Train RMSE 0.8552145035800296    Val RMSE 0.828695488690036\n",
            "Epoch 43   Train RMSE 0.8445297100042511    Val RMSE 0.8199279203542962\n",
            "Epoch 44   Train RMSE 0.847573342202126    Val RMSE 0.8177453932428809\n",
            "Epoch 45   Train RMSE 0.8497611074773833    Val RMSE 0.812224089590417\n",
            "Epoch 46   Train RMSE 0.8407986557283734    Val RMSE 0.8034166372468803\n",
            "Epoch 47   Train RMSE 0.8222231095075831    Val RMSE 0.7987502113703593\n",
            "Epoch 48   Train RMSE 0.8043395466019775    Val RMSE 0.8037806783782646\n",
            "Epoch 49   Train RMSE 0.7964737573863555    Val RMSE 0.8151107039902378\n",
            "Epoch 50   Train RMSE 0.7982865342006185    Val RMSE 0.8228693797594744\n",
            "Epoch 51   Train RMSE 0.8008696716617185    Val RMSE 0.8197722467270839\n",
            "Epoch 52   Train RMSE 0.7963079417060585    Val RMSE 0.8070812645580026\n",
            "Epoch 53   Train RMSE 0.7848611533323734    Val RMSE 0.7926150724213402\n",
            "Epoch 54   Train RMSE 0.7733763468292727    Val RMSE 0.7835052671567536\n",
            "Epoch 55   Train RMSE 0.7677801899987857    Val RMSE 0.7805496314052818\n",
            "Epoch 56   Train RMSE 0.7674515792160392    Val RMSE 0.7792892376355651\n",
            "Epoch 57   Train RMSE 0.7668877134380742    Val RMSE 0.7756307960589254\n",
            "Epoch 58   Train RMSE 0.7618385807561268    Val RMSE 0.7696719316891201\n",
            "Epoch 59   Train RMSE 0.7531474351509151    Val RMSE 0.7646286205459054\n",
            "Epoch 60   Train RMSE 0.7452707194967496    Val RMSE 0.76275928798296\n",
            "Epoch 61   Train RMSE 0.7415158897184166    Val RMSE 0.7627292004494863\n",
            "Epoch 62   Train RMSE 0.7408268150569195    Val RMSE 0.7612154779422713\n",
            "Epoch 63   Train RMSE 0.739444683850972    Val RMSE 0.7566917194012788\n",
            "Epoch 64   Train RMSE 0.7352546942183901    Val RMSE 0.7510016703717475\n",
            "Epoch 65   Train RMSE 0.7297827207202309    Val RMSE 0.7471887102600224\n",
            "Epoch 66   Train RMSE 0.7260169165035457    Val RMSE 0.7462380332898196\n",
            "Epoch 67   Train RMSE 0.7249289987590024    Val RMSE 0.7463672802672064\n",
            "Epoch 68   Train RMSE 0.7246766174867921    Val RMSE 0.7453054311109234\n",
            "Epoch 69   Train RMSE 0.7230013049339453    Val RMSE 0.742653469198498\n",
            "Epoch 70   Train RMSE 0.719691199335818    Val RMSE 0.7398612720916244\n",
            "Epoch 71   Train RMSE 0.7164538235608862    Val RMSE 0.7382717670244229\n",
            "Epoch 72   Train RMSE 0.7147131628608963    Val RMSE 0.7376489714824046\n",
            "Epoch 73   Train RMSE 0.7140162947582483    Val RMSE 0.7367877673631938\n",
            "Epoch 74   Train RMSE 0.712814661446672    Val RMSE 0.7351820221279963\n",
            "Epoch 75   Train RMSE 0.7104252237679022    Val RMSE 0.7335775243794798\n",
            "Epoch 76   Train RMSE 0.7077031604786292    Val RMSE 0.7328776216225369\n",
            "Epoch 77   Train RMSE 0.7058132914460431    Val RMSE 0.7329479069503858\n",
            "Epoch 78   Train RMSE 0.7048237307324291    Val RMSE 0.7328027769494093\n",
            "Epoch 79   Train RMSE 0.7038176833309888    Val RMSE 0.7317889514679744\n",
            "Epoch 80   Train RMSE 0.702150220165722    Val RMSE 0.7302472701775774\n",
            "Epoch 81   Train RMSE 0.7001734348427953    Val RMSE 0.7289708223831525\n",
            "Epoch 82   Train RMSE 0.6986517285352835    Val RMSE 0.7282644687024663\n",
            "Epoch 83   Train RMSE 0.6977586472432311    Val RMSE 0.7277642711401843\n",
            "Epoch 84   Train RMSE 0.6969395363864905    Val RMSE 0.7270906288881963\n",
            "Epoch 85   Train RMSE 0.6957007406494817    Val RMSE 0.7263590015328487\n",
            "Epoch 86   Train RMSE 0.6942080824000397    Val RMSE 0.7259059733113007\n",
            "Epoch 87   Train RMSE 0.6929633070464263    Val RMSE 0.7257113982913164\n",
            "Epoch 88   Train RMSE 0.6921045903825153    Val RMSE 0.7253462582915765\n",
            "Epoch 89   Train RMSE 0.6912834575689047    Val RMSE 0.7244847434845839\n",
            "Epoch 90   Train RMSE 0.6901899728272154    Val RMSE 0.7232746574461875\n",
            "Epoch 91   Train RMSE 0.6889487088245109    Val RMSE 0.7221225040781586\n",
            "Epoch 92   Train RMSE 0.6878882525618116    Val RMSE 0.7212414681013977\n",
            "Epoch 93   Train RMSE 0.6870753754254602    Val RMSE 0.7205506043416988\n",
            "Epoch 94   Train RMSE 0.6862657912753041    Val RMSE 0.719957080555248\n",
            "Epoch 95   Train RMSE 0.6852812510548737    Val RMSE 0.7195367364577265\n",
            "Epoch 96   Train RMSE 0.6842391101275825    Val RMSE 0.7193631134992122\n",
            "Epoch 97   Train RMSE 0.6833406252200591    Val RMSE 0.7192947361690099\n",
            "Epoch 98   Train RMSE 0.6825753391519451    Val RMSE 0.7190741867265987\n",
            "Epoch 99   Train RMSE 0.6817664218892947    Val RMSE 0.7186094498386019\n",
            "Epoch 100   Train RMSE 0.6808413215909412    Val RMSE 0.7180551065665572\n",
            "Epoch 101   Train RMSE 0.6799152255859077    Val RMSE 0.7176134147633161\n",
            "Epoch 102   Train RMSE 0.6790970616469881    Val RMSE 0.7173481851454818\n",
            "Epoch 103   Train RMSE 0.6783361264490769    Val RMSE 0.7172216738833636\n",
            "Epoch 104   Train RMSE 0.6775221258681923    Val RMSE 0.717217412203208\n",
            "Epoch 105   Train RMSE 0.6766567622675089    Val RMSE 0.7173314926427526\n",
            "Epoch 106   Train RMSE 0.6758343890691838    Val RMSE 0.7174783044157135\n",
            "Epoch 107   Train RMSE 0.6750900393938823    Val RMSE 0.7175045900602246\n",
            "Epoch 108   Train RMSE 0.6743580158276248    Val RMSE 0.7173266117695708\n",
            "Epoch 109   Train RMSE 0.6735855992938348    Val RMSE 0.7170046777744694\n",
            "Epoch 110   Train RMSE 0.6728077114305125    Val RMSE 0.7166623619685912\n",
            "Epoch 111   Train RMSE 0.6720783236490914    Val RMSE 0.716370712028563\n",
            "Epoch 112   Train RMSE 0.6713837002773055    Val RMSE 0.7161349572635003\n",
            "Epoch 113   Train RMSE 0.670672071712257    Val RMSE 0.7159456240922117\n",
            "Epoch 114   Train RMSE 0.6699373115562774    Val RMSE 0.7157851053845352\n",
            "Epoch 115   Train RMSE 0.6692196026105702    Val RMSE 0.7155950741989422\n",
            "Epoch 116   Train RMSE 0.6685347139237308    Val RMSE 0.7152951174689399\n",
            "Epoch 117   Train RMSE 0.6678535183567053    Val RMSE 0.7148552908630658\n",
            "Epoch 118   Train RMSE 0.6671551216362885    Val RMSE 0.714328034152769\n",
            "Epoch 119   Train RMSE 0.666459145632415    Val RMSE 0.7137975605544891\n",
            "Epoch 120   Train RMSE 0.6657882033662916    Val RMSE 0.7133172791086795\n",
            "Epoch 121   Train RMSE 0.6651318859237851    Val RMSE 0.7129015803487794\n",
            "Epoch 122   Train RMSE 0.6644689998715045    Val RMSE 0.7125454020372974\n",
            "Epoch 123   Train RMSE 0.6638035213135122    Val RMSE 0.712224165536992\n",
            "Epoch 124   Train RMSE 0.6631535761048423    Val RMSE 0.7118896826977914\n",
            "Epoch 125   Train RMSE 0.6625187601276338    Val RMSE 0.7114985860153241\n",
            "Epoch 126   Train RMSE 0.6618829750826918    Val RMSE 0.7110505604746945\n",
            "Epoch 127   Train RMSE 0.6612432886226556    Val RMSE 0.710586744201818\n",
            "Epoch 128   Train RMSE 0.6606118926495627    Val RMSE 0.7101510150022988\n",
            "Epoch 129   Train RMSE 0.6599927846557515    Val RMSE 0.7097631443037584\n",
            "Epoch 130   Train RMSE 0.6593761688149746    Val RMSE 0.7094207634352926\n",
            "Epoch 131   Train RMSE 0.6587571888192993    Val RMSE 0.709106344197209\n",
            "Epoch 132   Train RMSE 0.6581435586933626    Val RMSE 0.7087872588166687\n",
            "Epoch 133   Train RMSE 0.6575399618007248    Val RMSE 0.708428338078623\n",
            "Epoch 134   Train RMSE 0.6569404829652726    Val RMSE 0.7080158919223303\n",
            "Epoch 135   Train RMSE 0.656340547400267    Val RMSE 0.7075662362989993\n",
            "Epoch 136   Train RMSE 0.655744516881313    Val RMSE 0.7071084448322162\n",
            "Epoch 137   Train RMSE 0.6551564278432913    Val RMSE 0.7066618411693185\n",
            "Epoch 138   Train RMSE 0.6545726365814591    Val RMSE 0.7062309127952265\n",
            "Epoch 139   Train RMSE 0.6539894178727198    Val RMSE 0.7058072948452336\n",
            "Epoch 140   Train RMSE 0.6534095326900874    Val RMSE 0.7053721015942361\n",
            "Epoch 141   Train RMSE 0.6528359800346286    Val RMSE 0.704904357427706\n",
            "Epoch 142   Train RMSE 0.6522664692496656    Val RMSE 0.7043961197257247\n",
            "Epoch 143   Train RMSE 0.6516983357179237    Val RMSE 0.7038598584783846\n",
            "Epoch 144   Train RMSE 0.6511335969233996    Val RMSE 0.7033179555911883\n",
            "Epoch 145   Train RMSE 0.6505741627938054    Val RMSE 0.7027882800999773\n",
            "Epoch 146   Train RMSE 0.6500184423324575    Val RMSE 0.7022767246758347\n",
            "Epoch 147   Train RMSE 0.6494646325176578    Val RMSE 0.7017776279240804\n",
            "Epoch 148   Train RMSE 0.6489142538156211    Val RMSE 0.7012767273758065\n",
            "Epoch 149   Train RMSE 0.6483683951449439    Val RMSE 0.7007592693349937\n",
            "Epoch 150   Train RMSE 0.6478258258352513    Val RMSE 0.7002208104850284\n",
            "Epoch 151   Train RMSE 0.6472855182139194    Val RMSE 0.6996703071935545\n",
            "Epoch 152   Train RMSE 0.6467485838757713    Val RMSE 0.699122414310803\n",
            "Epoch 153   Train RMSE 0.6462157921803862    Val RMSE 0.6985869481659178\n",
            "Epoch 154   Train RMSE 0.6456860456382908    Val RMSE 0.6980642022163566\n",
            "Epoch 155   Train RMSE 0.6451588898131488    Val RMSE 0.697545838561009\n",
            "Epoch 156   Train RMSE 0.6446353250342025    Val RMSE 0.6970186469355641\n",
            "Epoch 157   Train RMSE 0.6441154988641392    Val RMSE 0.6964728425685296\n",
            "Epoch 158   Train RMSE 0.6435985405529017    Val RMSE 0.6959085981674865\n",
            "Epoch 159   Train RMSE 0.643084480188064    Val RMSE 0.6953345844497902\n",
            "Epoch 160   Train RMSE 0.6425738580904115    Val RMSE 0.6947611487866258\n",
            "Epoch 161   Train RMSE 0.642066612838722    Val RMSE 0.6941934686693848\n",
            "Epoch 162   Train RMSE 0.6415621950099571    Val RMSE 0.693629335686032\n",
            "Epoch 163   Train RMSE 0.6410607972347008    Val RMSE 0.6930613035409222\n",
            "Epoch 164   Train RMSE 0.6405627290187653    Val RMSE 0.6924820130773652\n",
            "Epoch 165   Train RMSE 0.6400679050124592    Val RMSE 0.6918896491285188\n",
            "Epoch 166   Train RMSE 0.6395758667757514    Val RMSE 0.6912898921856923\n",
            "Epoch 167   Train RMSE 0.639087017120669    Val RMSE 0.6906916487418432\n",
            "Epoch 168   Train RMSE 0.6386014567060191    Val RMSE 0.6901011150405321\n",
            "Epoch 169   Train RMSE 0.6381189128200567    Val RMSE 0.6895188269983056\n",
            "Epoch 170   Train RMSE 0.6376393455726789    Val RMSE 0.688940250456428\n",
            "Epoch 171   Train RMSE 0.6371629955524587    Val RMSE 0.6883591775412015\n",
            "Epoch 172   Train RMSE 0.6366898699804608    Val RMSE 0.6877725802632872\n",
            "Epoch 173   Train RMSE 0.6362196481506223    Val RMSE 0.6871828624211828\n",
            "Epoch 174   Train RMSE 0.6357525240150118    Val RMSE 0.6865954288709291\n",
            "Epoch 175   Train RMSE 0.6352885747739941    Val RMSE 0.686014323970592\n",
            "Epoch 176   Train RMSE 0.6348276900246504    Val RMSE 0.6854395548947471\n",
            "Epoch 177   Train RMSE 0.6343698294668466    Val RMSE 0.6848670642481371\n",
            "Epoch 178   Train RMSE 0.6339150466665897    Val RMSE 0.6842917592285652\n",
            "Epoch 179   Train RMSE 0.6334634658693201    Val RMSE 0.6837108803892766\n",
            "Epoch 180   Train RMSE 0.6330149056079304    Val RMSE 0.6831259077095045\n",
            "Epoch 181   Train RMSE 0.6325693251949521    Val RMSE 0.6825408149799463\n",
            "Epoch 182   Train RMSE 0.6321269430892918    Val RMSE 0.6819589271583936\n",
            "Epoch 183   Train RMSE 0.6316875772948881    Val RMSE 0.6813804341629582\n",
            "Epoch 184   Train RMSE 0.6312512341100129    Val RMSE 0.6808029640182615\n",
            "Epoch 185   Train RMSE 0.6308180615387725    Val RMSE 0.680223515758164\n",
            "Epoch 186   Train RMSE 0.6303879006506937    Val RMSE 0.679641308789672\n",
            "Epoch 187   Train RMSE 0.6299607812693581    Val RMSE 0.6790585814612926\n",
            "Epoch 188   Train RMSE 0.6295367569255289    Val RMSE 0.6784789073361313\n",
            "Epoch 189   Train RMSE 0.6291157865056175    Val RMSE 0.6779046335892852\n",
            "Epoch 190   Train RMSE 0.6286978761442633    Val RMSE 0.6773357012491246\n",
            "Epoch 191   Train RMSE 0.6282830082305009    Val RMSE 0.6767699212038515\n",
            "Epoch 192   Train RMSE 0.6278712362609127    Val RMSE 0.6762053704163471\n",
            "Epoch 193   Train RMSE 0.6274624950857683    Val RMSE 0.6756419204973133\n",
            "Epoch 194   Train RMSE 0.6270567193408851    Val RMSE 0.6750813160050434\n",
            "Epoch 195   Train RMSE 0.6266540336815092    Val RMSE 0.6745256525738063\n",
            "Epoch 196   Train RMSE 0.626254348892066    Val RMSE 0.6739754850155973\n",
            "Epoch 197   Train RMSE 0.6258577183401648    Val RMSE 0.673429551171626\n",
            "Epoch 198   Train RMSE 0.6254641478362292    Val RMSE 0.6728858461032503\n",
            "Epoch 199   Train RMSE 0.6250735716432814    Val RMSE 0.6723432796897055\n",
            "Test RMSE 0.7475748678406059\n",
            "Test MAE 0.591739060439631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Custom models, e.g. attention layer\n",
        "\n",
        "We can build any architecture or logic we like. For example, instead of the NN, we could make an attention layer (of course, one simple attention layer is not enough for a good model, but it is fun to build it):"
      ],
      "metadata": {
        "id": "O4njTxvaW1n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Att(nn.Module):\n",
        "    def __init__(self, k, v):\n",
        "        super(Att, self).__init__()\n",
        "\n",
        "        self.hidden_size = 300\n",
        "        self.feature_size = k.shape[1]\n",
        "        self.nq = nn.Linear(k.shape[1], self.hidden_size)\n",
        "        self.nk = nn.Linear(k.shape[1], self.hidden_size)\n",
        "        self.nv = nn.Linear(k.shape[0], k.shape[0])\n",
        "        self.k = k\n",
        "        self.v = v\n",
        "\n",
        "    def forward(self, data):\n",
        "        keys = self.nk(self.k)\n",
        "        queries = self.nq(data.f)\n",
        "        values = self.nv(self.v)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.feature_size, dtype=torch.float32))\n",
        "        # Apply softmax\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        # Multiply weights with values\n",
        "        x = torch.matmul(attention_weights, values)\n",
        "\n",
        "        return x.squeeze(-1)\n",
        "\n",
        "\n",
        "def train(folder):\n",
        "    torch.manual_seed(0)\n",
        "    train_loader, std = construct_loader(folder+\"/train_full.csv\", False)\n",
        "    val_loader, _ = construct_loader(folder+\"/val_full.csv\", False, std=std)\n",
        "    test_loader, _ = construct_loader(folder+\"/test_full.csv\", False, std=std)\n",
        "\n",
        "    model = Att(torch.tensor(train_loader.dataset.features, dtype=torch.float), torch.tensor(train_loader.dataset.labels, dtype=torch.float))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss = nn.MSELoss(reduction='sum')\n",
        "    print(model)\n",
        "\n",
        "    for epoch in range(0, 200):\n",
        "        train_loss = train_epoch(model, val_loader, optimizer, loss)\n",
        "        preds = pred(model, test_loader, loss)\n",
        "        print(\"Epoch\",epoch,\"  Train RMSE\", train_loss,\"   Val RMSE\", root_mean_squared_error(preds,test_loader.dataset.labels))\n",
        "\n",
        "    preds = pred(model, test_loader, loss)\n",
        "    print(\"Test RMSE\", root_mean_squared_error(preds,test_loader.dataset.labels))\n",
        "    print(\"Test MAE\", mean_absolute_error(preds,test_loader.dataset.labels))\n",
        "\n"
      ],
      "metadata": {
        "id": "OiQZthZOW9wR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(\"https://github.com/hesther/rxn_workshop/raw/main/data/esol\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKDrdZN2auw9",
        "outputId": "b7fa2ba9-4fe7-4e56-b1be-e00c05910dd6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Att(\n",
            "  (nq): Linear(in_features=210, out_features=300, bias=True)\n",
            "  (nk): Linear(in_features=210, out_features=300, bias=True)\n",
            "  (nv): Linear(in_features=902, out_features=902, bias=True)\n",
            ")\n",
            "Epoch 0   Train RMSE 2.7551320479468013    Val RMSE 4.019791508676953\n",
            "Epoch 1   Train RMSE 3.3279933276568063    Val RMSE 2.219993775913949\n",
            "Epoch 2   Train RMSE 2.100722134270165    Val RMSE 2.7597030539194636\n",
            "Epoch 3   Train RMSE 2.6909717048453103    Val RMSE 2.2139744959082557\n",
            "Epoch 4   Train RMSE 1.9824305341559654    Val RMSE 2.5981905752500882\n",
            "Epoch 5   Train RMSE 2.2946483173061547    Val RMSE 2.5403031419477955\n",
            "Epoch 6   Train RMSE 2.097284436794299    Val RMSE 2.1649842495998155\n",
            "Epoch 7   Train RMSE 2.025167778505626    Val RMSE 2.2516021100358694\n",
            "Epoch 8   Train RMSE 2.0753198492043254    Val RMSE 2.1630701707631625\n",
            "Epoch 9   Train RMSE 1.9103039643446265    Val RMSE 2.346453984958088\n",
            "Epoch 10   Train RMSE 1.9847086828630314    Val RMSE 2.1609314486267617\n",
            "Epoch 11   Train RMSE 1.791793404761464    Val RMSE 1.9429973677833068\n",
            "Epoch 12   Train RMSE 1.5446407481902866    Val RMSE 1.6406433287554458\n",
            "Epoch 13   Train RMSE 1.4286805656161687    Val RMSE 1.5735694744569142\n",
            "Epoch 14   Train RMSE 1.1845074139254719    Val RMSE 1.455660451534843\n",
            "Epoch 15   Train RMSE 0.9986428648071373    Val RMSE 1.47115278415026\n",
            "Epoch 16   Train RMSE 1.0367979454947356    Val RMSE 1.3686854216808109\n",
            "Epoch 17   Train RMSE 0.9487249086602868    Val RMSE 1.3219535349723437\n",
            "Epoch 18   Train RMSE 0.8644102145377461    Val RMSE 1.3070598036108894\n",
            "Epoch 19   Train RMSE 0.8519166682373731    Val RMSE 1.2369579032746427\n",
            "Epoch 20   Train RMSE 0.799538853474795    Val RMSE 1.1886302028329967\n",
            "Epoch 21   Train RMSE 0.7634356589496164    Val RMSE 1.157209561048259\n",
            "Epoch 22   Train RMSE 0.7437896701907828    Val RMSE 1.1416179733690912\n",
            "Epoch 23   Train RMSE 0.7372774030676983    Val RMSE 1.0919392139917474\n",
            "Epoch 24   Train RMSE 0.6925739621695145    Val RMSE 1.07532703727109\n",
            "Epoch 25   Train RMSE 0.6599543787621018    Val RMSE 1.073493185912059\n",
            "Epoch 26   Train RMSE 0.6468092917273276    Val RMSE 1.06359194377668\n",
            "Epoch 27   Train RMSE 0.6247348222846625    Val RMSE 1.0524928570039613\n",
            "Epoch 28   Train RMSE 0.6043925164651873    Val RMSE 1.042070486524974\n",
            "Epoch 29   Train RMSE 0.5834352341839314    Val RMSE 1.031712315090701\n",
            "Epoch 30   Train RMSE 0.5655346708723881    Val RMSE 1.0210299252090316\n",
            "Epoch 31   Train RMSE 0.5562741863693096    Val RMSE 1.0190941208345088\n",
            "Epoch 32   Train RMSE 0.5416878610523711    Val RMSE 1.0103186112798541\n",
            "Epoch 33   Train RMSE 0.5342829742162848    Val RMSE 1.0155475080136693\n",
            "Epoch 34   Train RMSE 0.5194928721543808    Val RMSE 1.0067427120466297\n",
            "Epoch 35   Train RMSE 0.514104044896441    Val RMSE 1.0131168430257946\n",
            "Epoch 36   Train RMSE 0.49989253617815815    Val RMSE 1.0002707950392395\n",
            "Epoch 37   Train RMSE 0.49902398992160746    Val RMSE 1.0097463980038273\n",
            "Epoch 38   Train RMSE 0.4837781089432855    Val RMSE 0.9954492076896021\n",
            "Epoch 39   Train RMSE 0.49042996460098953    Val RMSE 1.0166356064524398\n",
            "Epoch 40   Train RMSE 0.47112826383820944    Val RMSE 0.9992862556966426\n",
            "Epoch 41   Train RMSE 0.4909996351796211    Val RMSE 1.0388918483785419\n",
            "Epoch 42   Train RMSE 0.4656918746492924    Val RMSE 1.0113826051553734\n",
            "Epoch 43   Train RMSE 0.5105065701255244    Val RMSE 1.082961350286412\n",
            "Epoch 44   Train RMSE 0.4818795588385313    Val RMSE 1.044974971953216\n",
            "Epoch 45   Train RMSE 0.567430635363899    Val RMSE 1.172234318036874\n",
            "Epoch 46   Train RMSE 0.5485828435297717    Val RMSE 1.1228320512416017\n",
            "Epoch 47   Train RMSE 0.6770374331443969    Val RMSE 1.2952302749522606\n",
            "Epoch 48   Train RMSE 0.6694855228152216    Val RMSE 1.1941132058746013\n",
            "Epoch 49   Train RMSE 0.7854697896731591    Val RMSE 1.3016753392110878\n",
            "Epoch 50   Train RMSE 0.7274472581943224    Val RMSE 1.1130531200500482\n",
            "Epoch 51   Train RMSE 0.7134021752627239    Val RMSE 1.1162211390895096\n",
            "Epoch 52   Train RMSE 0.5857608274500311    Val RMSE 1.008359900564432\n",
            "Epoch 53   Train RMSE 0.48301545972743837    Val RMSE 1.0049713578600157\n",
            "Epoch 54   Train RMSE 0.45623063608125086    Val RMSE 1.0364787272171987\n",
            "Epoch 55   Train RMSE 0.4247055288758834    Val RMSE 1.007739477166473\n",
            "Epoch 56   Train RMSE 0.47909922919966785    Val RMSE 1.0623107854312506\n",
            "Epoch 57   Train RMSE 0.45389109010051115    Val RMSE 1.0096903636779764\n",
            "Epoch 58   Train RMSE 0.48121462667187365    Val RMSE 1.051790051696252\n",
            "Epoch 59   Train RMSE 0.4472144484913844    Val RMSE 1.0049478707540853\n",
            "Epoch 60   Train RMSE 0.4471748293208532    Val RMSE 1.0292153977281575\n",
            "Epoch 61   Train RMSE 0.4232365219236952    Val RMSE 1.0048480195987635\n",
            "Epoch 62   Train RMSE 0.41691358124669364    Val RMSE 1.0147331313425791\n",
            "Epoch 63   Train RMSE 0.4094328904897652    Val RMSE 1.0091789235087054\n",
            "Epoch 64   Train RMSE 0.40384968153055567    Val RMSE 1.0109747722998277\n",
            "Epoch 65   Train RMSE 0.4046412121138456    Val RMSE 1.01602148617482\n",
            "Epoch 66   Train RMSE 0.39824067658516143    Val RMSE 1.012877958311531\n",
            "Epoch 67   Train RMSE 0.40216102895343453    Val RMSE 1.0220915703791187\n",
            "Epoch 68   Train RMSE 0.394505320537307    Val RMSE 1.0142712690941722\n",
            "Epoch 69   Train RMSE 0.40017638638798947    Val RMSE 1.0250660212472018\n",
            "Epoch 70   Train RMSE 0.3908215028514969    Val RMSE 1.0135463628468877\n",
            "Epoch 71   Train RMSE 0.399218243115247    Val RMSE 1.027778802751352\n",
            "Epoch 72   Train RMSE 0.3872719478187215    Val RMSE 1.0131040422063502\n",
            "Epoch 73   Train RMSE 0.4002945761498264    Val RMSE 1.033789816446048\n",
            "Epoch 74   Train RMSE 0.38433275223203095    Val RMSE 1.0137854230239538\n",
            "Epoch 75   Train RMSE 0.40546251045968174    Val RMSE 1.0444580497961589\n",
            "Epoch 76   Train RMSE 0.38377188141781876    Val RMSE 1.0158761922798267\n",
            "Epoch 77   Train RMSE 0.41983590243990737    Val RMSE 1.0646590528665518\n",
            "Epoch 78   Train RMSE 0.39141986084684377    Val RMSE 1.0259186076496556\n",
            "Epoch 79   Train RMSE 0.45469747840445585    Val RMSE 1.1099023818414544\n",
            "Epoch 80   Train RMSE 0.4240273038388277    Val RMSE 1.0632134649118399\n",
            "Epoch 81   Train RMSE 0.5323307600303532    Val RMSE 1.2113799057794485\n",
            "Epoch 82   Train RMSE 0.5167820692644686    Val RMSE 1.1666501721916445\n",
            "Epoch 83   Train RMSE 0.6848413689911602    Val RMSE 1.4063118400541819\n",
            "Epoch 84   Train RMSE 0.7072805312960402    Val RMSE 1.3599723507576433\n",
            "Epoch 85   Train RMSE 0.9195574708893249    Val RMSE 1.6288371664897745\n",
            "Epoch 86   Train RMSE 0.9530239677037146    Val RMSE 1.4739362925663373\n",
            "Epoch 87   Train RMSE 1.0971078826518192    Val RMSE 1.5475435855479367\n",
            "Epoch 88   Train RMSE 0.9974452106945799    Val RMSE 1.2037113328380789\n",
            "Epoch 89   Train RMSE 0.8785814829659758    Val RMSE 1.119383682376185\n",
            "Epoch 90   Train RMSE 0.6342843446646206    Val RMSE 1.0039604718884914\n",
            "Epoch 91   Train RMSE 0.43307931986662973    Val RMSE 1.0035842599346227\n",
            "Epoch 92   Train RMSE 0.45910040829377996    Val RMSE 1.0984931670401785\n",
            "Epoch 93   Train RMSE 0.45513008705110336    Val RMSE 1.0432445950077278\n",
            "Epoch 94   Train RMSE 0.545934768041054    Val RMSE 1.0988103565071652\n",
            "Epoch 95   Train RMSE 0.4949861010613447    Val RMSE 1.0062000059145149\n",
            "Epoch 96   Train RMSE 0.4860080753605266    Val RMSE 1.0273836639774465\n",
            "Epoch 97   Train RMSE 0.423792701330745    Val RMSE 0.9916592194992481\n",
            "Epoch 98   Train RMSE 0.38405454349579143    Val RMSE 0.9913214826031199\n",
            "Epoch 99   Train RMSE 0.38386694042771985    Val RMSE 1.009784739847203\n",
            "Epoch 100   Train RMSE 0.36455583014518733    Val RMSE 0.9881289612508867\n",
            "Epoch 101   Train RMSE 0.3939611829852658    Val RMSE 1.0194010936214315\n",
            "Epoch 102   Train RMSE 0.3731622767078753    Val RMSE 0.9893377261854472\n",
            "Epoch 103   Train RMSE 0.3925188124151311    Val RMSE 1.0185310258056695\n",
            "Epoch 104   Train RMSE 0.37176529203720743    Val RMSE 0.993800636658297\n",
            "Epoch 105   Train RMSE 0.37929607277342386    Val RMSE 1.013761899308327\n",
            "Epoch 106   Train RMSE 0.3645733241569227    Val RMSE 0.9985407007252448\n",
            "Epoch 107   Train RMSE 0.366609199131407    Val RMSE 1.0085725528977205\n",
            "Epoch 108   Train RMSE 0.35943200861676616    Val RMSE 1.0010160608221124\n",
            "Epoch 109   Train RMSE 0.3593751243922806    Val RMSE 1.0049159888145613\n",
            "Epoch 110   Train RMSE 0.35685983879499833    Val RMSE 1.003060862562582\n",
            "Epoch 111   Train RMSE 0.3554503257923294    Val RMSE 1.0042877010446354\n",
            "Epoch 112   Train RMSE 0.35523698398495296    Val RMSE 1.0056779149955768\n",
            "Epoch 113   Train RMSE 0.3528686947835247    Val RMSE 1.0049029071025657\n",
            "Epoch 114   Train RMSE 0.35362958388528093    Val RMSE 1.0071368809835934\n",
            "Epoch 115   Train RMSE 0.3506731865150454    Val RMSE 1.0046905355444382\n",
            "Epoch 116   Train RMSE 0.3520328448378412    Val RMSE 1.0073805094984465\n",
            "Epoch 117   Train RMSE 0.348590335341913    Val RMSE 1.0042728645831442\n",
            "Epoch 118   Train RMSE 0.3505375969587549    Val RMSE 1.0080249652972633\n",
            "Epoch 119   Train RMSE 0.3464928351813364    Val RMSE 1.0044750518510688\n",
            "Epoch 120   Train RMSE 0.3491961418531354    Val RMSE 1.0092384814084785\n",
            "Epoch 121   Train RMSE 0.34426657918859194    Val RMSE 1.004466646399935\n",
            "Epoch 122   Train RMSE 0.34817778654557713    Val RMSE 1.0103061345068143\n",
            "Epoch 123   Train RMSE 0.3418950621037746    Val RMSE 1.0037276516078295\n",
            "Epoch 124   Train RMSE 0.3477942589280508    Val RMSE 1.0116973873643735\n",
            "Epoch 125   Train RMSE 0.33941052765464774    Val RMSE 1.0027951083454503\n",
            "Epoch 126   Train RMSE 0.34855727972314837    Val RMSE 1.0144614774125182\n",
            "Epoch 127   Train RMSE 0.3369602779811725    Val RMSE 1.0019163552287778\n",
            "Epoch 128   Train RMSE 0.35156908029120065    Val RMSE 1.0195014901873392\n",
            "Epoch 129   Train RMSE 0.3352199587259187    Val RMSE 1.0013276102184052\n",
            "Epoch 130   Train RMSE 0.359365027745419    Val RMSE 1.0292303693457372\n",
            "Epoch 131   Train RMSE 0.3365708966921194    Val RMSE 1.0033115629183187\n",
            "Epoch 132   Train RMSE 0.3776596839695001    Val RMSE 1.0502334268037021\n",
            "Epoch 133   Train RMSE 0.34831543909924334    Val RMSE 1.0152998879364965\n",
            "Epoch 134   Train RMSE 0.4190707252429151    Val RMSE 1.0980606695791084\n",
            "Epoch 135   Train RMSE 0.3897933308137526    Val RMSE 1.0579481761134428\n",
            "Epoch 136   Train RMSE 0.5084310432497717    Val RMSE 1.2074363562861352\n",
            "Epoch 137   Train RMSE 0.49989685842519294    Val RMSE 1.1791215003323554\n",
            "Epoch 138   Train RMSE 0.6833487807442948    Val RMSE 1.4355282270466374\n",
            "Epoch 139   Train RMSE 0.7262386297494736    Val RMSE 1.4374276122207126\n",
            "Epoch 140   Train RMSE 0.9733884881724808    Val RMSE 1.7865621876561806\n",
            "Epoch 141   Train RMSE 1.0699423268185513    Val RMSE 1.7404902045109705\n",
            "Epoch 142   Train RMSE 1.3093837801472754    Val RMSE 1.9623213331653009\n",
            "Epoch 143   Train RMSE 1.3320685238810945    Val RMSE 1.6198146646036893\n",
            "Epoch 144   Train RMSE 1.3255987487956273    Val RMSE 1.4824460743154917\n",
            "Epoch 145   Train RMSE 1.0596363521686485    Val RMSE 1.052793589197346\n",
            "Epoch 146   Train RMSE 0.7292244661220348    Val RMSE 0.9800372265333156\n",
            "Epoch 147   Train RMSE 0.48413127489403973    Val RMSE 1.0645248316772014\n",
            "Epoch 148   Train RMSE 0.4119371895155896    Val RMSE 1.0661707542015513\n",
            "Epoch 149   Train RMSE 0.5758436917073985    Val RMSE 1.1635456647968097\n",
            "Epoch 150   Train RMSE 0.5655942423766872    Val RMSE 1.0421424531390595\n",
            "Epoch 151   Train RMSE 0.589344242624285    Val RMSE 1.0573992551273128\n",
            "Epoch 152   Train RMSE 0.4883547155426175    Val RMSE 0.9670273807301206\n",
            "Epoch 153   Train RMSE 0.4098641218095458    Val RMSE 0.969865332640101\n",
            "Epoch 154   Train RMSE 0.3662759341734633    Val RMSE 0.9890152738805572\n",
            "Epoch 155   Train RMSE 0.33327724933682173    Val RMSE 0.9700678181682797\n",
            "Epoch 156   Train RMSE 0.38126142750482866    Val RMSE 1.0106018066713627\n",
            "Epoch 157   Train RMSE 0.35728226733657187    Val RMSE 0.968067016201252\n",
            "Epoch 158   Train RMSE 0.3839792454979402    Val RMSE 0.9996738254392223\n",
            "Epoch 159   Train RMSE 0.3530667759147997    Val RMSE 0.9669360467359394\n",
            "Epoch 160   Train RMSE 0.35328403284528187    Val RMSE 0.9850481070273315\n",
            "Epoch 161   Train RMSE 0.3350642409219825    Val RMSE 0.9763170799325013\n",
            "Epoch 162   Train RMSE 0.3277639832608964    Val RMSE 0.9795430138050103\n",
            "Epoch 163   Train RMSE 0.329122593601063    Val RMSE 0.9850951074024746\n",
            "Epoch 164   Train RMSE 0.32077615024992784    Val RMSE 0.9771410709866324\n",
            "Epoch 165   Train RMSE 0.32948613548486866    Val RMSE 0.9880794150813341\n",
            "Epoch 166   Train RMSE 0.32015710643469447    Val RMSE 0.9771460258543783\n",
            "Epoch 167   Train RMSE 0.32853035638644673    Val RMSE 0.9891848264201661\n",
            "Epoch 168   Train RMSE 0.3194110210470301    Val RMSE 0.9797732416706162\n",
            "Epoch 169   Train RMSE 0.32507101896917423    Val RMSE 0.9894731559083602\n",
            "Epoch 170   Train RMSE 0.3177698094138008    Val RMSE 0.9820014669333351\n",
            "Epoch 171   Train RMSE 0.3212809228300832    Val RMSE 0.9883667430189992\n",
            "Epoch 172   Train RMSE 0.31602002342379026    Val RMSE 0.9830219376926324\n",
            "Epoch 173   Train RMSE 0.3180797964105468    Val RMSE 0.9874381694539587\n",
            "Epoch 174   Train RMSE 0.31448161546996867    Val RMSE 0.9844386116312839\n",
            "Epoch 175   Train RMSE 0.3155453354792524    Val RMSE 0.987777870899626\n",
            "Epoch 176   Train RMSE 0.3129543220557692    Val RMSE 0.9861400035331687\n",
            "Epoch 177   Train RMSE 0.3134196577384998    Val RMSE 0.9884159540839104\n",
            "Epoch 178   Train RMSE 0.3113851210282752    Val RMSE 0.9871848404044891\n",
            "Epoch 179   Train RMSE 0.311544531224654    Val RMSE 0.9887696699632217\n",
            "Epoch 180   Train RMSE 0.3097758648636257    Val RMSE 0.9878289115058339\n",
            "Epoch 181   Train RMSE 0.30982341107102856    Val RMSE 0.9892830389679486\n",
            "Epoch 182   Train RMSE 0.30812596224234173    Val RMSE 0.9885504179118498\n",
            "Epoch 183   Train RMSE 0.308173859709246    Val RMSE 0.9900045578164434\n",
            "Epoch 184   Train RMSE 0.30642292774881896    Val RMSE 0.9891844168917967\n",
            "Epoch 185   Train RMSE 0.3065735202912245    Val RMSE 0.9906659893557925\n",
            "Epoch 186   Train RMSE 0.3046724853728623    Val RMSE 0.9896270350702471\n",
            "Epoch 187   Train RMSE 0.30502990898460497    Val RMSE 0.9913337328604268\n",
            "Epoch 188   Train RMSE 0.30287004937673107    Val RMSE 0.9900200615521382\n",
            "Epoch 189   Train RMSE 0.303558581244669    Val RMSE 0.9921283809676679\n",
            "Epoch 190   Train RMSE 0.30099120800668766    Val RMSE 0.9903445426061861\n",
            "Epoch 191   Train RMSE 0.3021958637559444    Val RMSE 0.9930300703715761\n",
            "Epoch 192   Train RMSE 0.29900966689819747    Val RMSE 0.9905101999756576\n",
            "Epoch 193   Train RMSE 0.30102067558512724    Val RMSE 0.9941181606824393\n",
            "Epoch 194   Train RMSE 0.2968948759151386    Val RMSE 0.9905109261172231\n",
            "Epoch 195   Train RMSE 0.3001822787094257    Val RMSE 0.9955865392661598\n",
            "Epoch 196   Train RMSE 0.2946195126137002    Val RMSE 0.9903018488445997\n",
            "Epoch 197   Train RMSE 0.2999753683627614    Val RMSE 0.997713494342056\n",
            "Epoch 198   Train RMSE 0.2922157146478564    Val RMSE 0.9898276419533569\n",
            "Epoch 199   Train RMSE 0.3010142402409162    Val RMSE 1.001100933704044\n",
            "Test RMSE 1.001100933704044\n",
            "Test MAE 0.7496795737703817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Custom models, e.g. message passing neural network\n",
        "\n",
        "We now need to transform our RDKit molecule objects to graphs with edge (bond) and node (atom) features. Here, we use a simply one-hot encoding of symbol, degree, hydrogen atoms, hybridization, and aromaticity (vector size = 29). For bond features, we simply use the bond type, and whether it is conjugated and in a ring (vector size = 6). In general, this initial featurization should be adapted for more complex problems."
      ],
      "metadata": {
        "id": "vXfyoAHxdanw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def atom_features(atom):\n",
        "    features = onek_encoding_unk(atom.GetSymbol(), ['C', 'N', 'O',  'P', 'S', 'Cl', 'Br', 'I']) + \\\n",
        "        onek_encoding_unk(atom.GetTotalDegree(), [0, 1, 2, 3, 4, 5]) + \\\n",
        "        onek_encoding_unk(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4]) + \\\n",
        "        onek_encoding_unk(int(atom.GetHybridization()),[Chem.rdchem.HybridizationType.SP,\n",
        "                                                        Chem.rdchem.HybridizationType.SP2,\n",
        "                                                        Chem.rdchem.HybridizationType.SP3,\n",
        "                                                        Chem.rdchem.HybridizationType.SP3D,\n",
        "                                                        Chem.rdchem.HybridizationType.SP3D2\n",
        "                                                        ]) + \\\n",
        "        [1 if atom.GetIsAromatic() else 0]\n",
        "    return features\n",
        "\n",
        "def bond_features(bond):\n",
        "    bt = bond.GetBondType()\n",
        "    fbond = [\n",
        "            bt == Chem.rdchem.BondType.SINGLE,\n",
        "            bt == Chem.rdchem.BondType.DOUBLE,\n",
        "            bt == Chem.rdchem.BondType.TRIPLE,\n",
        "            bt == Chem.rdchem.BondType.AROMATIC,\n",
        "            (bond.GetIsConjugated() if bt is not None else 0),\n",
        "            (bond.IsInRing() if bt is not None else 0)\n",
        "      ]\n",
        "    return fbond\n",
        "\n",
        "def onek_encoding_unk(value, choices):\n",
        "    encoding = [0] * (len(choices) + 1)\n",
        "    index = choices.index(value) if value in choices else -1\n",
        "    encoding[index] = 1\n",
        "    return encoding"
      ],
      "metadata": {
        "id": "Hfdi_aUBchS0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MolGraph:\n",
        "    def __init__(self, smiles):\n",
        "        self.smiles = smiles\n",
        "        self.f_atoms = []\n",
        "        self.f_bonds = []\n",
        "        self.edge_index = []\n",
        "\n",
        "        mol = Chem.MolFromSmiles(self.smiles)\n",
        "        n_atoms=mol.GetNumAtoms()\n",
        "\n",
        "        for a1 in range(n_atoms):\n",
        "            f_atom = atom_features(mol.GetAtomWithIdx(a1))\n",
        "            self.f_atoms.append(f_atom)\n",
        "\n",
        "            for a2 in range(a1 + 1, n_atoms):\n",
        "                bond = mol.GetBondBetweenAtoms(a1, a2)\n",
        "                if bond is None:\n",
        "                    continue\n",
        "                f_bond = bond_features(bond)\n",
        "                self.f_bonds.append(f_bond)\n",
        "                self.f_bonds.append(f_bond)\n",
        "                self.edge_index.extend([(a1, a2), (a2, a1)])"
      ],
      "metadata": {
        "id": "Xx8ekqiHdWfw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = MolGraph(\"CCO\")\n",
        "for i,f in enumerate(g.f_atoms):\n",
        "  print(i,f)\n",
        "for (i,j),f in zip(g.edge_index,g.f_bonds):\n",
        "  print(i,j,f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDvWq44fdpHL",
        "outputId": "c3509c94-e80d-4e59-bf31-f5617ea168e7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "1 [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "2 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "0 1 [True, False, False, False, False, False]\n",
            "1 0 [True, False, False, False, False, False]\n",
            "1 2 [True, False, False, False, False, False]\n",
            "2 1 [True, False, False, False, False, False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a dataset of molecules and their properties, we can now build a dataset object, and a loader which inherit from pytorch_geometric's Dataset and DataLoader. The ChemDataset class takes as input a list of SMILES and target values. Whenever we retrieve an element from this list via the get() function, we create a MolGraph from the respective SMILES strings. For this course, we do not implement any caching, but remake the graphs whenever we need them (we also don't hold them in memory). The molgraph2data function transforms our custom MolGraph into a format more convenient for pytorch_geometric's functionalities (and, importantly, into torch tensors). The construct_loader() functions takes a CSV file saved locally or from the internet, and uses the first column as list of SMILES, and the second as list of targets, for the sake of simplicity (for a real software package, don't hardcode this!)"
      ],
      "metadata": {
        "id": "Sr_8QCFLfEA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChemDataset(Dataset):\n",
        "    def __init__(self, smiles, labels):\n",
        "        super(ChemDataset, self).__init__()\n",
        "        self.smiles = smiles\n",
        "        self.labels = labels\n",
        "\n",
        "    def process_key(self, key):\n",
        "        smi = self.smiles[key]\n",
        "        mol = self.molgraph2data(MolGraph(smi), key)\n",
        "        return mol\n",
        "\n",
        "    def molgraph2data(self, molgraph, key):\n",
        "        data = tg.data.Data()\n",
        "        data.x = torch.tensor(molgraph.f_atoms, dtype=torch.float)\n",
        "        data.edge_index = torch.tensor(molgraph.edge_index, dtype=torch.long).t().contiguous()\n",
        "        data.edge_attr = torch.tensor(molgraph.f_bonds, dtype=torch.float)\n",
        "        data.y = torch.tensor([self.labels[key]], dtype=torch.float)\n",
        "        data.smiles = self.smiles[key]\n",
        "        return data\n",
        "\n",
        "    def get(self,key):\n",
        "        return self.process_key(key)\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.smiles)\n",
        "\n",
        "def construct_loader(data_path, shuffle=True, batch_size=50):\n",
        "    data_df = pd.read_csv(data_path)\n",
        "    smiles = data_df.iloc[:, 0].values\n",
        "    labels = data_df.iloc[:, 1].values.astype(np.float32)\n",
        "    dataset = ChemDataset(smiles, labels)\n",
        "    loader = DataLoader(dataset=dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=shuffle,\n",
        "                            num_workers=0,\n",
        "                            pin_memory=True,\n",
        "                            sampler=None)\n",
        "    return loader"
      ],
      "metadata": {
        "id": "LWEgiQ9mdpzy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect a dataset. For each batch, we get a list of SMILES strings, node features, edge features and connectivity lists. Note that the format is a bit unintuitive: SMILES, node features and labels are lists or lists of list of len(data), but the edge features and attributes are a single list for the full data chunk:"
      ],
      "metadata": {
        "id": "OPJtq7Jpfghj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = construct_loader(\"https://github.com/hesther/rxn_workshop/raw/main/data/esol/train_full.csv\")\n",
        "for data in loader:\n",
        "    print(data)\n",
        "    print(\"SMILES\",data.smiles[:3])\n",
        "    print(\"node features\",data.x[:3])\n",
        "    print(\"labels\",data.y[:3])\n",
        "    print(\"edges\",data.edge_index[:,:10])\n",
        "    print(\"edge features\",data.edge_attr[:10])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ayog4t8xe4f0",
        "outputId": "e3b4b322-f202-49a5-d0f6-b7b66b2cb895"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataBatch(x=[651, 29], edge_index=[2, 1340], edge_attr=[1340, 6], y=[50], smiles=[50], batch=[651], ptr=[51])\n",
            "SMILES ['CCCBr', 'CC(C)CC(C)(C)O', 'CC1CC(C)C(=O)C(C1)C(O)CC2CC(=O)NC(=O)C2']\n",
            "node features tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])\n",
            "labels tensor([-1.7300, -0.9200, -1.1300])\n",
            "edges tensor([[0, 1, 1, 2, 2, 3, 4, 5, 5, 6],\n",
            "        [1, 0, 2, 1, 3, 2, 5, 4, 6, 5]])\n",
            "edge features tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now construct a D-MPNN (directed message-passing neural network) based on pytorch_geometric. We will follow the framework of Chemprop for the implementation in this workshop, but there are many flavors of D-MPNNs differing in the messages, update functions, etc. Based on the edge_index list we provide in our dataset, pytorch_geometric does all the heavy lifting determining which pairs of atoms should pass messages, and which sets of atoms to aggregate over to make molecular from atomic embeddings."
      ],
      "metadata": {
        "id": "1EWYQ6yMf5r7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, num_node_features, num_edge_features):\n",
        "        super(GNN, self).__init__()\n",
        "\n",
        "        self.depth = 3\n",
        "        self.hidden_size = 300\n",
        "        self.dropout = 0.02\n",
        "\n",
        "        self.edge_init = nn.Linear(num_node_features + num_edge_features, self.hidden_size)\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for _ in range(self.depth):\n",
        "            self.convs.append(DMPNNConv(self.hidden_size))\n",
        "        self.edge_to_node = nn.Linear(num_node_features + self.hidden_size, self.hidden_size)\n",
        "        self.pool = global_add_pool\n",
        "        self.ffn = nn.Linear(self.hidden_size, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "\n",
        "        # initial edge features\n",
        "        row, col = edge_index\n",
        "        h_0 = F.relu(self.edge_init(torch.cat([x[row], edge_attr], dim=1)))\n",
        "        h = h_0\n",
        "\n",
        "        # convolutions\n",
        "        for l in range(self.depth):\n",
        "            _, h = self.convs[l](edge_index, h)\n",
        "            h += h_0\n",
        "            h = F.dropout(F.relu(h), self.dropout, training=self.training)\n",
        "\n",
        "        # dmpnn edge -> node aggregation\n",
        "        s, _ = self.convs[l](edge_index, h) #only use for summing\n",
        "        q  = torch.cat([x,s], dim=1)\n",
        "        h = F.relu(self.edge_to_node(q))\n",
        "        return self.ffn(self.pool(h, batch)).squeeze(-1)\n",
        "\n",
        "class DMPNNConv(MessagePassing):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(DMPNNConv, self).__init__(aggr='add')\n",
        "        self.lin = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, edge_index, edge_attr):\n",
        "        row, col = edge_index\n",
        "        a_message = self.propagate(edge_index, x=None, edge_attr=edge_attr)\n",
        "        rev_message = torch.flip(edge_attr.view(edge_attr.size(0) // 2, 2, -1), dims=[1]).view(edge_attr.size(0), -1)\n",
        "\n",
        "        return a_message, self.lin(a_message[row] - rev_message)\n",
        "\n",
        "    def message(self, edge_attr):\n",
        "        return edge_attr"
      ],
      "metadata": {
        "id": "S3vR_danfsrE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "GNN(29,6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ldx7PYEf9WT",
        "outputId": "4e2a7e06-eaaf-43fa-9397-6c043269a745"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GNN(\n",
              "  (edge_init): Linear(in_features=35, out_features=300, bias=True)\n",
              "  (convs): ModuleList(\n",
              "    (0-2): 3 x DMPNNConv()\n",
              "  )\n",
              "  (edge_to_node): Linear(in_features=329, out_features=300, bias=True)\n",
              "  (ffn): Linear(in_features=300, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(folder):\n",
        "    torch.manual_seed(0)\n",
        "    train_loader = construct_loader(folder+\"/train_full.csv\", True)\n",
        "    val_loader = construct_loader(folder+\"/val_full.csv\", False)\n",
        "    test_loader = construct_loader(folder+\"/test_full.csv\", False)\n",
        "\n",
        "\n",
        "    model = GNN(train_loader.dataset.num_node_features, train_loader.dataset.num_edge_features)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss = nn.MSELoss(reduction='sum')\n",
        "    print(model)\n",
        "\n",
        "    for epoch in range(0, 100):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, loss)\n",
        "        preds = pred(model, val_loader, loss)\n",
        "        print(\"Epoch\",epoch,\"  Train RMSE\", train_loss,\"   Val RMSE\", root_mean_squared_error(preds,val_loader.dataset.labels))\n",
        "\n",
        "    preds = pred(model, test_loader, loss)\n",
        "    print(\"Test RMSE\", root_mean_squared_error(preds,test_loader.dataset.labels))\n",
        "    print(\"Test MAE\", mean_absolute_error(preds,test_loader.dataset.labels))"
      ],
      "metadata": {
        "id": "ywK2foz5f_9X"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(\"https://github.com/hesther/rxn_workshop/raw/main/data/esol\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBV6XpBpgv6i",
        "outputId": "1dad4bcb-15ed-4e7b-e1ea-5cd803631dce"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GNN(\n",
            "  (edge_init): Linear(in_features=35, out_features=300, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-2): 3 x DMPNNConv()\n",
            "  )\n",
            "  (edge_to_node): Linear(in_features=329, out_features=300, bias=True)\n",
            "  (ffn): Linear(in_features=300, out_features=1, bias=True)\n",
            ")\n",
            "Epoch 0   Train RMSE 4.571772102663409    Val RMSE 1.6822713031183762\n",
            "Epoch 1   Train RMSE 1.629827079025758    Val RMSE 1.56345126996836\n",
            "Epoch 2   Train RMSE 1.3604469457569976    Val RMSE 1.3745694245186195\n",
            "Epoch 3   Train RMSE 1.2057019169517806    Val RMSE 1.0492405146948534\n",
            "Epoch 4   Train RMSE 1.056599727269256    Val RMSE 0.9475681925522085\n",
            "Epoch 5   Train RMSE 0.9899333067306085    Val RMSE 0.9272181014710869\n",
            "Epoch 6   Train RMSE 0.9675951195811794    Val RMSE 0.9151182440918637\n",
            "Epoch 7   Train RMSE 0.9331640427758001    Val RMSE 0.8943478475098111\n",
            "Epoch 8   Train RMSE 0.9393933251396158    Val RMSE 0.8709297127178662\n",
            "Epoch 9   Train RMSE 0.9189359723223525    Val RMSE 0.8600962462501766\n",
            "Epoch 10   Train RMSE 0.9110649386164804    Val RMSE 0.8406795909131888\n",
            "Epoch 11   Train RMSE 0.8471690944697131    Val RMSE 0.793500886923197\n",
            "Epoch 12   Train RMSE 0.8252364470031434    Val RMSE 0.7795463173748747\n",
            "Epoch 13   Train RMSE 0.792588730125863    Val RMSE 0.8662406476851333\n",
            "Epoch 14   Train RMSE 0.7758107236428354    Val RMSE 0.9085919528101283\n",
            "Epoch 15   Train RMSE 0.7716899356533266    Val RMSE 0.6975586901455371\n",
            "Epoch 16   Train RMSE 0.7223300506923264    Val RMSE 0.7777323469577233\n",
            "Epoch 17   Train RMSE 0.7183846498806705    Val RMSE 0.6924370249529681\n",
            "Epoch 18   Train RMSE 0.6912079435112821    Val RMSE 0.6675368854891173\n",
            "Epoch 19   Train RMSE 0.6668903725522767    Val RMSE 0.7458845971799397\n",
            "Epoch 20   Train RMSE 0.6479187924471835    Val RMSE 0.6520235436139524\n",
            "Epoch 21   Train RMSE 0.6329534868082389    Val RMSE 0.6716653288625577\n",
            "Epoch 22   Train RMSE 0.6680251203699126    Val RMSE 0.6500594399607396\n",
            "Epoch 23   Train RMSE 0.6487025213416332    Val RMSE 0.634994477643555\n",
            "Epoch 24   Train RMSE 0.6480302565951914    Val RMSE 0.6948222214608395\n",
            "Epoch 25   Train RMSE 0.6109386580668447    Val RMSE 0.6336525140520433\n",
            "Epoch 26   Train RMSE 0.632144174755315    Val RMSE 0.6952792224134259\n",
            "Epoch 27   Train RMSE 0.6905612631337972    Val RMSE 0.6979253114552932\n",
            "Epoch 28   Train RMSE 0.6256336338083885    Val RMSE 0.6352979794115655\n",
            "Epoch 29   Train RMSE 0.5972007561554309    Val RMSE 0.5840197597215071\n",
            "Epoch 30   Train RMSE 0.6066144870056965    Val RMSE 0.6722930326997909\n",
            "Epoch 31   Train RMSE 0.6036869534257894    Val RMSE 0.6052885884596058\n",
            "Epoch 32   Train RMSE 0.5773177826323685    Val RMSE 0.7366463747538042\n",
            "Epoch 33   Train RMSE 0.609130370115034    Val RMSE 0.6021491484493029\n",
            "Epoch 34   Train RMSE 0.5893038116293616    Val RMSE 0.6588565122423996\n",
            "Epoch 35   Train RMSE 0.5970094209866976    Val RMSE 0.6616684039464504\n",
            "Epoch 36   Train RMSE 0.575099218142606    Val RMSE 0.6170196722323948\n",
            "Epoch 37   Train RMSE 0.5677222649734293    Val RMSE 0.5887522675908842\n",
            "Epoch 38   Train RMSE 0.5337052269321375    Val RMSE 0.5998067167031227\n",
            "Epoch 39   Train RMSE 0.5574420446798067    Val RMSE 0.6259797410043217\n",
            "Epoch 40   Train RMSE 0.559422551696605    Val RMSE 0.6941309581650227\n",
            "Epoch 41   Train RMSE 0.5991255457680833    Val RMSE 0.574991380682618\n",
            "Epoch 42   Train RMSE 0.5500472568670689    Val RMSE 0.5965043916254564\n",
            "Epoch 43   Train RMSE 0.5288614826048256    Val RMSE 0.5927829173972533\n",
            "Epoch 44   Train RMSE 0.5246785099152251    Val RMSE 0.5898760688343212\n",
            "Epoch 45   Train RMSE 0.5116730007075725    Val RMSE 0.6188743221456708\n",
            "Epoch 46   Train RMSE 0.5053769318616591    Val RMSE 0.6218689722112023\n",
            "Epoch 47   Train RMSE 0.49680417173821684    Val RMSE 0.593660934476311\n",
            "Epoch 48   Train RMSE 0.5210478881145215    Val RMSE 0.5981080684702119\n",
            "Epoch 49   Train RMSE 0.5349661496407451    Val RMSE 0.5739182872375501\n",
            "Epoch 50   Train RMSE 0.5484432893974585    Val RMSE 0.5889456574038998\n",
            "Epoch 51   Train RMSE 0.49344047127542395    Val RMSE 0.5958163736507922\n",
            "Epoch 52   Train RMSE 0.48177321837238674    Val RMSE 0.5608822333700845\n",
            "Epoch 53   Train RMSE 0.46261353387978665    Val RMSE 0.6457705082830524\n",
            "Epoch 54   Train RMSE 0.48081179522114714    Val RMSE 0.5999794271115401\n",
            "Epoch 55   Train RMSE 0.4855098351170884    Val RMSE 0.5816519320693916\n",
            "Epoch 56   Train RMSE 0.5051524698367923    Val RMSE 0.6010851337165611\n",
            "Epoch 57   Train RMSE 0.49743009785439635    Val RMSE 0.6179843496490183\n",
            "Epoch 58   Train RMSE 0.47806150725405366    Val RMSE 0.6033223545626285\n",
            "Epoch 59   Train RMSE 0.4737099496222924    Val RMSE 0.6164152274441412\n",
            "Epoch 60   Train RMSE 0.48460135247088637    Val RMSE 0.5650272308898681\n",
            "Epoch 61   Train RMSE 0.48892157721092094    Val RMSE 0.6355774721341101\n",
            "Epoch 62   Train RMSE 0.49043324604817085    Val RMSE 0.5608349930259618\n",
            "Epoch 63   Train RMSE 0.4639413553473686    Val RMSE 0.5373094940593799\n",
            "Epoch 64   Train RMSE 0.4655738994540931    Val RMSE 0.5757271474664079\n",
            "Epoch 65   Train RMSE 0.430363037366359    Val RMSE 0.5748018100204821\n",
            "Epoch 66   Train RMSE 0.4519315821624596    Val RMSE 0.6544187070852505\n",
            "Epoch 67   Train RMSE 0.4573420095720138    Val RMSE 0.5614301034395563\n",
            "Epoch 68   Train RMSE 0.4203934261878775    Val RMSE 0.5846960375305678\n",
            "Epoch 69   Train RMSE 0.4388618900130441    Val RMSE 0.5663055905966576\n",
            "Epoch 70   Train RMSE 0.4588359511617485    Val RMSE 0.8560134880464241\n",
            "Epoch 71   Train RMSE 0.5026729483968707    Val RMSE 0.6210669670612938\n",
            "Epoch 72   Train RMSE 0.4471075619919603    Val RMSE 0.6360501835272682\n",
            "Epoch 73   Train RMSE 0.5062092635165829    Val RMSE 0.6273455898052281\n",
            "Epoch 74   Train RMSE 0.42152114439089483    Val RMSE 0.5743205052455567\n",
            "Epoch 75   Train RMSE 0.3963247457037237    Val RMSE 0.5359923616024314\n",
            "Epoch 76   Train RMSE 0.38647410813839495    Val RMSE 0.5617599381095382\n",
            "Epoch 77   Train RMSE 0.3817860017012264    Val RMSE 0.5591534454603083\n",
            "Epoch 78   Train RMSE 0.3917751926192097    Val RMSE 0.5434639974124318\n",
            "Epoch 79   Train RMSE 0.3985396048669813    Val RMSE 0.5775780778087912\n",
            "Epoch 80   Train RMSE 0.41861770732196596    Val RMSE 0.5519121747365968\n",
            "Epoch 81   Train RMSE 0.3980463576383663    Val RMSE 0.6090172179589045\n",
            "Epoch 82   Train RMSE 0.42738193641226235    Val RMSE 0.5552630152450276\n",
            "Epoch 83   Train RMSE 0.4060785959020799    Val RMSE 0.5551195941840382\n",
            "Epoch 84   Train RMSE 0.3998820913400917    Val RMSE 0.5754947376909618\n",
            "Epoch 85   Train RMSE 0.37415618134500117    Val RMSE 0.6008671238719516\n",
            "Epoch 86   Train RMSE 0.3975575959866137    Val RMSE 0.5301843970273767\n",
            "Epoch 87   Train RMSE 0.3956148732653526    Val RMSE 0.5875745709059208\n",
            "Epoch 88   Train RMSE 0.398902901748609    Val RMSE 0.559818564875832\n",
            "Epoch 89   Train RMSE 0.3622740658057861    Val RMSE 0.5698316594494734\n",
            "Epoch 90   Train RMSE 0.3616392291263153    Val RMSE 0.5443474312838971\n",
            "Epoch 91   Train RMSE 0.37827492256025097    Val RMSE 0.5794196285767171\n",
            "Epoch 92   Train RMSE 0.3552879670261687    Val RMSE 0.5502372021683802\n",
            "Epoch 93   Train RMSE 0.35823003154109295    Val RMSE 0.66839153736046\n",
            "Epoch 94   Train RMSE 0.3758340937204274    Val RMSE 0.5686910087384751\n",
            "Epoch 95   Train RMSE 0.34633552768814946    Val RMSE 0.5628109136712138\n",
            "Epoch 96   Train RMSE 0.3249540264368245    Val RMSE 0.568631452224864\n",
            "Epoch 97   Train RMSE 0.3309745241406516    Val RMSE 0.5895449714155233\n",
            "Epoch 98   Train RMSE 0.36331158172974365    Val RMSE 0.6228716839331088\n",
            "Epoch 99   Train RMSE 0.3511862436819611    Val RMSE 0.5456457250729334\n",
            "Test RMSE 0.6552811332350431\n",
            "Test MAE 0.4454241961702309\n"
          ]
        }
      ]
    }
  ]
}