{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9NL/U5JfhNlW3X2W28rLZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hesther/teaching/blob/main/med_inf/Lecture_demonstration.ipynb)"
      ],
      "metadata": {
        "id": "VxtY1r0mn5re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LV 12 of Methods for Data Generation and Analytics in Medicine and Life Sciences"
      ],
      "metadata": {
        "id": "nl9raM0TYo2d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jaGTT0zlXnZn",
        "outputId": "d9a3b9e5-4cc6-41cc-887f-c524663f189d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.1/144.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m827.9/827.9 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.0/188.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.6/831.6 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.0/176.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.4.1 which is incompatible.\n",
            "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.9/542.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q rdkit numpy scikit-learn chemprop torch==2.4.1\n",
        "!pip install -q torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-2.4.1+cpu.html\n",
        "!pip install -q torch_geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning in Chemistry\n",
        "\n",
        "### 1. Solubility data\n",
        "One great Python package for molecules is RDKit. We can input a molecule from an SMILES string, which is a in-line notation for molecules that is also readable by humans. For example:\n",
        "\n",
        "- Methane: C\n",
        "- Ethane: CC\n",
        "- Ethene: C=C\n",
        "- Propane: CCC\n",
        "- Butane: CCCC\n",
        "- Isobutane: CC(C)C\n",
        "- Cyclobutane: C1CCC1\n",
        "\n",
        "You get the picture."
      ],
      "metadata": {
        "id": "XOP4GWCgXw13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rdkit\n",
        "from rdkit.Chem import Descriptors\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CjNFK8__ZB4v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"https://github.com/hesther/rxn_workshop/raw/main/data/esol/train_full.csv\")\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "EYT48m0fY0aK",
        "outputId": "8c53e1af-cd52-4007-a41f-cb684ec10447"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                smiles  logSolubility\n",
              "0                                             CC/C=C\\C         -2.540\n",
              "1                       O=C1NC(=O)NC(=O)C1(CC)CC=C(C)C         -2.253\n",
              "2                     Cc1[nH]c(=O)n(c(=O)c1Cl)C(C)(C)C         -2.484\n",
              "3                                             CC/C=C/C         -2.540\n",
              "4                   ClC(Cl)C(c1ccc(Cl)cc1)c2ccc(Cl)cc2         -7.200\n",
              "..                                                 ...            ...\n",
              "897                 O2c1ccc(N)cc1N(C)C(=O)c3cc(C)ccc23         -3.928\n",
              "898                                         CCCCCCCC#C         -4.240\n",
              "899                                        CCCC(=O)OCC         -1.360\n",
              "900  CC(=O)OCC(=O)C1(O)CCC2C3CCC4=CC(=O)CCC4(C)C3C(...         -4.880\n",
              "901                                      Clc1ccc(I)cc1         -4.030\n",
              "\n",
              "[902 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-28c0586c-bd9f-44b1-92bc-9628c389f0b5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>smiles</th>\n",
              "      <th>logSolubility</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CC/C=C\\C</td>\n",
              "      <td>-2.540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>O=C1NC(=O)NC(=O)C1(CC)CC=C(C)C</td>\n",
              "      <td>-2.253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Cc1[nH]c(=O)n(c(=O)c1Cl)C(C)(C)C</td>\n",
              "      <td>-2.484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CC/C=C/C</td>\n",
              "      <td>-2.540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ClC(Cl)C(c1ccc(Cl)cc1)c2ccc(Cl)cc2</td>\n",
              "      <td>-7.200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>897</th>\n",
              "      <td>O2c1ccc(N)cc1N(C)C(=O)c3cc(C)ccc23</td>\n",
              "      <td>-3.928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>898</th>\n",
              "      <td>CCCCCCCC#C</td>\n",
              "      <td>-4.240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>899</th>\n",
              "      <td>CCCC(=O)OCC</td>\n",
              "      <td>-1.360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>900</th>\n",
              "      <td>CC(=O)OCC(=O)C1(O)CCC2C3CCC4=CC(=O)CCC4(C)C3C(...</td>\n",
              "      <td>-4.880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>901</th>\n",
              "      <td>Clc1ccc(I)cc1</td>\n",
              "      <td>-4.030</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>902 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28c0586c-bd9f-44b1-92bc-9628c389f0b5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-28c0586c-bd9f-44b1-92bc-9628c389f0b5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-28c0586c-bd9f-44b1-92bc-9628c389f0b5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-dc263553-c256-4e5e-a4c8-7d660222d97f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dc263553-c256-4e5e-a4c8-7d660222d97f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-dc263553-c256-4e5e-a4c8-7d660222d97f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_8200d605-de48-465f-ae63-cc291cac69c1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8200d605-de48-465f-ae63-cc291cac69c1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 902,\n  \"fields\": [\n    {\n      \"column\": \"smiles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 898,\n        \"samples\": [\n          \"ClC(Cl)(Cl)Cl\",\n          \"CCOP(=S)(OCC)SCSCC\",\n          \"Oc1cc(Cl)c(Cl)c(Cl)c1Cl\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"logSolubility\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.1019626263291658,\n        \"min\": -11.6,\n        \"max\": 1.58,\n        \"num_unique_values\": 618,\n        \"samples\": [\n          -7.92,\n          -4.23,\n          -1.38\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mol1 = rdkit.Chem.MolFromSmiles(data['smiles'][0])\n",
        "mol1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "2ZeThUjFYBQY",
        "outputId": "ebb94071-91fb-4b6a-8678-e2ac3d3b45ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<rdkit.Chem.rdchem.Mol at 0x7ce274ce5bd0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fit a neural network, we have to transform the SMILES strings into a list of features. Luckily, RDKit has a functionality to do that:"
      ],
      "metadata": {
        "id": "aIkaK3WNZgZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Descriptors.CalcMolDescriptors(mol1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST7DVN-wZPxj",
        "outputId": "e7ae569b-7284-49d0-806a-c922bad2d11a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MaxAbsEStateIndex': np.float64(2.125),\n",
              " 'MaxEStateIndex': np.float64(2.125),\n",
              " 'MinAbsEStateIndex': np.float64(1.1631944444444444),\n",
              " 'MinEStateIndex': np.float64(1.1631944444444444),\n",
              " 'qed': 0.41363968044003097,\n",
              " 'SPS': 10.0,\n",
              " 'MolWt': 70.135,\n",
              " 'HeavyAtomMolWt': 60.05499999999999,\n",
              " 'ExactMolWt': 70.07825032,\n",
              " 'NumValenceElectrons': 30,\n",
              " 'NumRadicalElectrons': 0,\n",
              " 'MaxPartialCharge': -0.03794287307367419,\n",
              " 'MinPartialCharge': -0.0916744470464646,\n",
              " 'MaxAbsPartialCharge': 0.0916744470464646,\n",
              " 'MinAbsPartialCharge': 0.03794287307367419,\n",
              " 'FpDensityMorgan1': 1.6,\n",
              " 'FpDensityMorgan2': 2.2,\n",
              " 'FpDensityMorgan3': 2.2,\n",
              " 'BCUT2D_MWHI': 13.594079354378078,\n",
              " 'BCUT2D_MWLOW': 10.430607628427651,\n",
              " 'BCUT2D_CHGHI': 1.5174139912845042,\n",
              " 'BCUT2D_CHGLO': -1.6468102360406212,\n",
              " 'BCUT2D_LOGPHI': 1.7323374263754383,\n",
              " 'BCUT2D_LOGPLOW': -1.4311843309765475,\n",
              " 'BCUT2D_MRHI': 4.730012035564743,\n",
              " 'BCUT2D_MRLOW': 1.2174768301743983,\n",
              " 'AvgIpc': 1.4056390622295665,\n",
              " 'BalabanJ': np.float64(2.622415807990217),\n",
              " 'BertzCT': 27.019550008653876,\n",
              " 'Chi0': np.float64(4.121320343559642),\n",
              " 'Chi0n': 3.861807319565799,\n",
              " 'Chi0v': 3.861807319565799,\n",
              " 'Chi1': np.float64(2.414213562373095),\n",
              " 'Chi1n': 2.02603867417337,\n",
              " 'Chi1v': 2.02603867417337,\n",
              " 'Chi2n': 0.9772838841927123,\n",
              " 'Chi2v': 0.9772838841927123,\n",
              " 'Chi3n': 0.47140452079103173,\n",
              " 'Chi3v': 0.47140452079103173,\n",
              " 'Chi4n': 0.23570226039551587,\n",
              " 'Chi4v': 0.23570226039551587,\n",
              " 'HallKierAlpha': -0.26,\n",
              " 'Ipc': np.float64(11.245112497836532),\n",
              " 'Kappa1': 4.74,\n",
              " 'Kappa2': 3.7400000000000007,\n",
              " 'Kappa3': 3.740000000000001,\n",
              " 'LabuteASA': 33.50941645323172,\n",
              " 'PEOE_VSA1': 0.0,\n",
              " 'PEOE_VSA10': 0.0,\n",
              " 'PEOE_VSA11': 0.0,\n",
              " 'PEOE_VSA12': 0.0,\n",
              " 'PEOE_VSA13': 0.0,\n",
              " 'PEOE_VSA14': 0.0,\n",
              " 'PEOE_VSA2': 0.0,\n",
              " 'PEOE_VSA3': 0.0,\n",
              " 'PEOE_VSA4': 0.0,\n",
              " 'PEOE_VSA5': 0.0,\n",
              " 'PEOE_VSA6': 19.075777413358384,\n",
              " 'PEOE_VSA7': 13.344558822616634,\n",
              " 'PEOE_VSA8': 0.0,\n",
              " 'PEOE_VSA9': 0.0,\n",
              " 'SMR_VSA1': 0.0,\n",
              " 'SMR_VSA10': 0.0,\n",
              " 'SMR_VSA2': 0.0,\n",
              " 'SMR_VSA3': 0.0,\n",
              " 'SMR_VSA4': 0.0,\n",
              " 'SMR_VSA5': 20.268296022307258,\n",
              " 'SMR_VSA6': 0.0,\n",
              " 'SMR_VSA7': 12.152040213667762,\n",
              " 'SMR_VSA8': 0.0,\n",
              " 'SMR_VSA9': 0.0,\n",
              " 'SlogP_VSA1': 0.0,\n",
              " 'SlogP_VSA10': 0.0,\n",
              " 'SlogP_VSA11': 0.0,\n",
              " 'SlogP_VSA12': 0.0,\n",
              " 'SlogP_VSA2': 0.0,\n",
              " 'SlogP_VSA3': 0.0,\n",
              " 'SlogP_VSA4': 0.0,\n",
              " 'SlogP_VSA5': 20.268296022307258,\n",
              " 'SlogP_VSA6': 12.152040213667762,\n",
              " 'SlogP_VSA7': 0.0,\n",
              " 'SlogP_VSA8': 0.0,\n",
              " 'SlogP_VSA9': 0.0,\n",
              " 'TPSA': 0.0,\n",
              " 'EState_VSA1': np.float64(0.0),\n",
              " 'EState_VSA10': np.float64(0.0),\n",
              " 'EState_VSA11': np.float64(0.0),\n",
              " 'EState_VSA2': np.float64(0.0),\n",
              " 'EState_VSA3': np.float64(0.0),\n",
              " 'EState_VSA4': np.float64(6.4208216229260096),\n",
              " 'EState_VSA5': np.float64(0.0),\n",
              " 'EState_VSA6': np.float64(0.0),\n",
              " 'EState_VSA7': np.float64(6.923737199690624),\n",
              " 'EState_VSA8': np.float64(19.075777413358384),\n",
              " 'EState_VSA9': np.float64(0.0),\n",
              " 'VSA_EState1': np.float64(0.0),\n",
              " 'VSA_EState10': np.float64(0.0),\n",
              " 'VSA_EState2': np.float64(0.0),\n",
              " 'VSA_EState3': np.float64(0.0),\n",
              " 'VSA_EState4': np.float64(0.0),\n",
              " 'VSA_EState5': np.float64(0.0),\n",
              " 'VSA_EState6': np.float64(0.0),\n",
              " 'VSA_EState7': np.float64(5.34375),\n",
              " 'VSA_EState8': np.float64(4.15625),\n",
              " 'VSA_EState9': np.float64(0.0),\n",
              " 'FractionCSP3': 0.6,\n",
              " 'HeavyAtomCount': 5,\n",
              " 'NHOHCount': 0,\n",
              " 'NOCount': 0,\n",
              " 'NumAliphaticCarbocycles': 0,\n",
              " 'NumAliphaticHeterocycles': 0,\n",
              " 'NumAliphaticRings': 0,\n",
              " 'NumAmideBonds': 0,\n",
              " 'NumAromaticCarbocycles': 0,\n",
              " 'NumAromaticHeterocycles': 0,\n",
              " 'NumAromaticRings': 0,\n",
              " 'NumAtomStereoCenters': 0,\n",
              " 'NumBridgeheadAtoms': 0,\n",
              " 'NumHAcceptors': 0,\n",
              " 'NumHDonors': 0,\n",
              " 'NumHeteroatoms': 0,\n",
              " 'NumHeterocycles': 0,\n",
              " 'NumRotatableBonds': 1,\n",
              " 'NumSaturatedCarbocycles': 0,\n",
              " 'NumSaturatedHeterocycles': 0,\n",
              " 'NumSaturatedRings': 0,\n",
              " 'NumSpiroAtoms': 0,\n",
              " 'NumUnspecifiedAtomStereoCenters': 0,\n",
              " 'Phi': 3.5455200000000007,\n",
              " 'RingCount': 0,\n",
              " 'MolLogP': 1.9725,\n",
              " 'MolMR': 25.10499999999999,\n",
              " 'fr_Al_COO': 0,\n",
              " 'fr_Al_OH': 0,\n",
              " 'fr_Al_OH_noTert': 0,\n",
              " 'fr_ArN': 0,\n",
              " 'fr_Ar_COO': 0,\n",
              " 'fr_Ar_N': 0,\n",
              " 'fr_Ar_NH': 0,\n",
              " 'fr_Ar_OH': 0,\n",
              " 'fr_COO': 0,\n",
              " 'fr_COO2': 0,\n",
              " 'fr_C_O': 0,\n",
              " 'fr_C_O_noCOO': 0,\n",
              " 'fr_C_S': 0,\n",
              " 'fr_HOCCN': 0,\n",
              " 'fr_Imine': 0,\n",
              " 'fr_NH0': 0,\n",
              " 'fr_NH1': 0,\n",
              " 'fr_NH2': 0,\n",
              " 'fr_N_O': 0,\n",
              " 'fr_Ndealkylation1': 0,\n",
              " 'fr_Ndealkylation2': 0,\n",
              " 'fr_Nhpyrrole': 0,\n",
              " 'fr_SH': 0,\n",
              " 'fr_aldehyde': 0,\n",
              " 'fr_alkyl_carbamate': 0,\n",
              " 'fr_alkyl_halide': 0,\n",
              " 'fr_allylic_oxid': 2,\n",
              " 'fr_amide': 0,\n",
              " 'fr_amidine': 0,\n",
              " 'fr_aniline': 0,\n",
              " 'fr_aryl_methyl': 0,\n",
              " 'fr_azide': 0,\n",
              " 'fr_azo': 0,\n",
              " 'fr_barbitur': 0,\n",
              " 'fr_benzene': 0,\n",
              " 'fr_benzodiazepine': 0,\n",
              " 'fr_bicyclic': 0,\n",
              " 'fr_diazo': 0,\n",
              " 'fr_dihydropyridine': 0,\n",
              " 'fr_epoxide': 0,\n",
              " 'fr_ester': 0,\n",
              " 'fr_ether': 0,\n",
              " 'fr_furan': 0,\n",
              " 'fr_guanido': 0,\n",
              " 'fr_halogen': 0,\n",
              " 'fr_hdrzine': 0,\n",
              " 'fr_hdrzone': 0,\n",
              " 'fr_imidazole': 0,\n",
              " 'fr_imide': 0,\n",
              " 'fr_isocyan': 0,\n",
              " 'fr_isothiocyan': 0,\n",
              " 'fr_ketone': 0,\n",
              " 'fr_ketone_Topliss': 0,\n",
              " 'fr_lactam': 0,\n",
              " 'fr_lactone': 0,\n",
              " 'fr_methoxy': 0,\n",
              " 'fr_morpholine': 0,\n",
              " 'fr_nitrile': 0,\n",
              " 'fr_nitro': 0,\n",
              " 'fr_nitro_arom': 0,\n",
              " 'fr_nitro_arom_nonortho': 0,\n",
              " 'fr_nitroso': 0,\n",
              " 'fr_oxazole': 0,\n",
              " 'fr_oxime': 0,\n",
              " 'fr_para_hydroxylation': 0,\n",
              " 'fr_phenol': 0,\n",
              " 'fr_phenol_noOrthoHbond': 0,\n",
              " 'fr_phos_acid': 0,\n",
              " 'fr_phos_ester': 0,\n",
              " 'fr_piperdine': 0,\n",
              " 'fr_piperzine': 0,\n",
              " 'fr_priamide': 0,\n",
              " 'fr_prisulfonamd': 0,\n",
              " 'fr_pyridine': 0,\n",
              " 'fr_quatN': 0,\n",
              " 'fr_sulfide': 0,\n",
              " 'fr_sulfonamd': 0,\n",
              " 'fr_sulfone': 0,\n",
              " 'fr_term_acetylene': 0,\n",
              " 'fr_tetrazole': 0,\n",
              " 'fr_thiazole': 0,\n",
              " 'fr_thiocyan': 0,\n",
              " 'fr_thiophene': 0,\n",
              " 'fr_unbrch_alkane': 0,\n",
              " 'fr_urea': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def features(smi):\n",
        "    mol = rdkit.Chem.MolFromSmiles(smi)\n",
        "    return np.array(list(Descriptors.CalcMolDescriptors(mol).values()))\n",
        "\n",
        "features(\"CCC\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8yrT3lMZlhN",
        "outputId": "aae2f685-62c6-4de8-cf3a-bcd9125dff66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.125     ,  2.125     ,  1.25      ,  1.25      ,  0.38547066,\n",
              "        6.        , 44.097     , 36.033     , 44.06260026, 20.        ,\n",
              "        0.        , -0.05903836, -0.06564544,  0.06564544,  0.05903836,\n",
              "        1.33333333,  1.33333333,  1.33333333, 13.42571365, 10.59728635,\n",
              "        1.35237444, -1.47605824,  1.55881365, -1.26961365,  3.91771365,\n",
              "        1.08928635,  0.91829583,  1.63299316,  0.        ,  2.70710678,\n",
              "        2.70710678,  2.70710678,  1.41421356,  1.41421356,  1.41421356,\n",
              "        0.70710678,  0.70710678,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  2.7548875 ,  3.        ,  2.        ,\n",
              "        0.        , 21.46913526,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        , 20.26829602,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        , 20.26829602,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        , 20.26829602,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        6.42082162,  0.        ,  0.        , 13.8474744 ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  1.25      ,  4.25      ,  0.        ,\n",
              "        1.        ,  3.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  2.        ,  0.        ,\n",
              "        1.4163    , 15.965     ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, let's now make one giant 2D array of x values, and try to predict y (the solubility):"
      ],
      "metadata": {
        "id": "WKajH2nEZysa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_raw = np.array([features(smi) for smi in data['smiles']])\n",
        "standardize_by = np.max(np.abs(x_raw),axis=0)+0.1\n",
        "x_train = x_raw / standardize_by\n",
        "y_train = np.array(data['logSolubility'])"
      ],
      "metadata": {
        "id": "VPQgeULBZpPi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also load a test set:"
      ],
      "metadata": {
        "id": "dEER2G3gaBjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"https://github.com/hesther/rxn_workshop/raw/main/data/esol/test_full.csv\")\n",
        "x_raw = np.array([features(smi) for smi in data['smiles']])\n",
        "x_test = x_raw / standardize_by\n",
        "y_test = np.array(data['logSolubility'])"
      ],
      "metadata": {
        "id": "L1mRfIP0aAjk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Linear regression\n",
        "Let's try a simple regression first:"
      ],
      "metadata": {
        "id": "uk7V7A_zaSqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "10KCFgvuaRzg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
        "\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel(\"true solubility\")\n",
        "plt.ylabel(\"predicted solubility\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "vtfwvOsFaNY7",
        "outputId": "38270e72-be42-4ea1-eb91-4d38ff8ee9da"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.547741651633407\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'predicted solubility')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a test, let's try to use our model on some unseen, possibly out-of-distribution data: Vitamin D and Vitamin C:"
      ],
      "metadata": {
        "id": "vxkglLmobt0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = np.array([features(smi) for smi in ['CC1=C(C(CCC1)(C)C)/C=C/C(=C/C=C/C(=C/CO)/C)/C',\n",
        "                                        'C(C(C1C(=C(C(=O)O1)O)O)O)O']])\n",
        "x2 = x2 / standardize_by\n",
        "model.predict(x2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r6OVL30bAZs",
        "outputId": "d920774e-d5e2-4ce7-da8b-199cf1909ca1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-4.83447755, -0.48096322])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Neural network\n",
        "Let's use a neural network instead of a simple linear regression:"
      ],
      "metadata": {
        "id": "jVkWo449cCYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLPRegressor()\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
        "\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel(\"true solubility\")\n",
        "plt.ylabel(\"predicted solubility\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "SDatps3eb-dR",
        "outputId": "caadd47d-e7a1-436d-92e7-c80ffb9b4af4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.45530060974329745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'predicted solubility')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGzCAYAAADXFObAAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAASG1JREFUeJzt3Xl4VPXZ//HPBEJCgAwEEhIR2aTWiGUtkEKVYijYitJqXZ6qQIFWHpeqtQq1GsAqdXlc6/ZYRRCr/qpURGpaEDd4QCwRNAIikBSKCQaQBFlCyMzvjzhjllnOmZxZzpn367pyXWRy5sydudC5+X6/9327vF6vVwAAADaXEu8AAAAArEBSAwAAHIGkBgAAOAJJDQAAcASSGgAA4AgkNQAAwBFIagAAgCOQ1AAAAEcgqQEAAI5AUgMAAByhbbwDMGr+/PlasmSJtm7dqvbt2+t73/ue7r77bp122mmG7+HxePT555+rU6dOcrlcUYwWAABYxev16tChQzrppJOUkhJ8PcZll9lPEyZM0KWXXqrvfve7OnHihH73u9+ptLRUmzdvVocOHQzd4z//+Y969uwZ5UgBAEA07N69WyeffHLQn9smqWmuqqpKOTk5euedd3TWWWcZek51dbU6d+6s3bt3KzMzM8oRAgAAK9TU1Khnz546ePCg3G530Otss/3UXHV1tSQpKysr6DW1tbWqra31f3/o0CFJUmZmJkkNAAA2E+7oiC0PCns8Hl1//fUaNWqUBgwYEPS6+fPny+12+7/YegIAwLlsuf00c+ZMvfHGG1q9enXIvbXmKzW+5avq6mpWagAAsImamhq53e6wn9+223665ppr9Prrr+vdd98NmdBIUlpamtLS0mIUGQAAiCfbJDVer1fXXnut/va3v+ntt99Wnz594h0SAABIILZJaq6++mr95S9/0dKlS9WpUydVVlZKktxut9q3bx/n6AAAQLzZ5kxNsBPPCxYs0JQpUwzdw+ieHAAASByOO1Njk9wLAADEiS1LugEAAJojqQEAAI5gm+0nAAAQXL3Hq/VlB/TFoWPK6ZSu4X2y1CYluYY3k9QAAGBzxaUVmrtssyqqj/kfy3Onq2hiviYMyItjZLHF9hMAADZWXFqhmYtLmiQ0klRZfUwzF5eouLQiTpHFHkkNAAA2Ve/xau6yzQpUH+x7bO6yzar3JEcFMUkNAAA2tb7sQIsVmsa8kiqqj2l92YHYBRVHJDUAANjUF4eCJzSRXGd3JDUAANhUTqd0S6+zO5IaAABsanifLOW50xWscNulhiqo4X2yYhlW3JDUAABgU21SXCqamC9JLRIb3/dFE/OTpl8NSQ0AADY2YUCeHr98iHLdTbeYct3pevzyIUnVp4bmewAA2NyEAXkal59LR+F4BwAAAFqvTYpLBf26xjuMuGL7CQAAOAJJDQAAcASSGgAA4AgkNQAAwBFIagAAgCOQ1AAAAEcgqQEAAI5AUgMAAByBpAYAADgCSQ0AAHAExiQAAIAW6j1e282SIqkBAABNFJdWaO6yzaqoPuZ/LM+drqKJ+Qk99ZvtJwAA4FdcWqGZi0uaJDSSVFl9TDMXl6i4tCJOkYVHUgMAACQ1bDnNXbZZ3gA/8z02d9lm1XsCXRF/JDUAAECStL7sQIsVmsa8kiqqj2l92YHYBWUCSQ0AAJAkfXEoeEITyXWxxkFhAAAgScrplB7RdYlSKUVSAwAAJEnD+2Qpz52uyupjAc/VuCTluhuSFp9EqpRi+wkAAEiS2qS4VDQxX1JDAtOY7/uiifn+VZhEq5QiqQEAAH4TBuTp8cuHKNfddIsp152uxy8f4l99ScRKKbafAABAExMG5Glcfm7IczJmKqUK+nWNQdQkNQAAIIA2Ka6QyUgiVkqx/QQAAEyLtFIqmkhqAACAab5KqWCF2y41VEE1rpSKNpIaAABgmtlKqVggqQEAAJIaKprW7tivpRv3aO2O/WErl4xWSsUKB4UBALApKzv5RtpEz0ilVKy4vF5vYo7ajIKamhq53W5VV1crMzMz3uEAABAxKzv5+proNU8IfGlJPFZdGjP6+c32EwAANmNlJ99EbKIXKZIaAABsxOokxEwTvURHUgMAgI1YnYQkYhO9SJHUAABgI1YnIYnYRC9SJDUAANiIlUlIvccrj9erzu1Tg14TjyZ6kaKkGwAAG/F18q2sPhbwXI1LDX1iwiUhgaqnAt1Lin0TvUixUgMAgI1Y0ck3WPVUc/FqohcpVmoAALAJX7O92hMeXV/YXy+s36XKmlr/z3MN9KkJVT3l0zkjVY9eNkQj+3W1xQqND0kNAAA2EGi7KDczXTcUfku9u2UY7uQbrnpKkg4eqVNKistWCY3E9hMAAAkv2HbR3ppjenDlNqW1TVGBwVUVJ5VwN0dSAwCAxcwOhgx3Lyub7TmphLs5tp8AALCQlTOZJHPN9gr6dQ14TePBl906pCk3M117a1pXPZWISGoAALBIsMGQvplMkVQStXa7KFCS1TkjVV41JDCNY7VbCXdzbD8BAGCBaA2GbM12UbCzONVH6iRJ7oymTffsVsLdHCs1AABYwIptokAibbYXLslySUpvm6Lnp4/Qvq9qDVdPJTJWagAAsEC0qooibbZnJMmqrKlVisulCwb1MFw9lchIagAAsEA0q4omDMjT45cPUa676XNDbRc5uXQ7GNttPz366KO69957VVlZqYEDB+qRRx7R8OHD4x0WACDJWTWTKZgJA/I0Lj/XX8UUbrvIyaXbwdhqpeall17SjTfeqKKiIpWUlGjgwIEaP368vvjii3iHBgBIclbMZDLyGgX9uhraLvIlWcGusNP0baNsldTcf//9mjFjhqZOnar8/Hw98cQTysjI0DPPPBPv0AAAiGibKJjWNvCLRZKVaGyz/XT8+HFt2LBBs2fP9j+WkpKiwsJCrV27NuBzamtrVVv7zaCvmpqaqMcJAEhuZreJAikurdCc1z5pOqwyM01zzj/DVGLkS7JazIxqRTPARGabpGbfvn2qr69X9+7dmzzevXt3bd26NeBz5s+fr7lz58YiPAAA/HzbRJEoLq3QVYtLWjxeWVOrqxaX6AmTKz5WJFl2YavtJ7Nmz56t6upq/9fu3bvjHRIAwIGsmvVU7/Fq1pKPQ14za8nHEW1FGT2LY2e2Wanp1q2b2rRpo7179zZ5fO/evcrNzQ34nLS0NKWlpcUiPABAkrJy1tO6Hft18Otuv8EcPFKndTv2a1T/bhHF62S2Walp166dhg4dqjfffNP/mMfj0ZtvvqmCgoI4RgYASFbBxhD4Zj0Vl1aYut+aHVWGrlu7c5+p+yYL2yQ1knTjjTfqqaee0sKFC7VlyxbNnDlThw8f1tSpU+MdGgAgyVg966m4tELPrC43+OrO3D5qLdtsP0nSJZdcoqqqKt1+++2qrKzUoEGDVFxc3OLwMAAA0WblrKdgh4ODifQQstPZKqmRpGuuuUbXXHNNvMMAACQ5q8YQ+FZ8jOqY1kYj+5LUBGKr7ScAABKFVWMIwq34NHfJsJ6OrV5qLdut1AAAEE69xxv1vixWzXoyO1CyMD9wxS9IagAADmNliXUovjEEMxeXyCU1SWzMjCEwM1DSabOarMb2EwDAMawusQ7HillP4QZP+rjkvFlNVnN5vd7I2h7aUE1Njdxut6qrq5WZmRnvcAAAFqr3eDX67lVBz6f4toNW3zLW8sSgtdtdvmRMUsCtrC4ZqZr/0zMdN6vJKKOf32w/AQBsr97j1bNryiwrsTarNbOepOCDJzu3T9XUUb11zdj+rNAYQFIDALC1QGdoQnnj6y2oRBvqmEyDJ6OF7ScAgG35tm0i+SCLxuFhRIfRz28OCgMAbCnUmAIjonV4GPFDUgMAsCWzTeuai2Q+ExIbZ2oAALZktmldIL7Dw8+sLlP10TpJXhX07aaR/bpylsWGSGoAALZkpmldOHf+fYv/z396a4c6Z6Tqj0lcQm1XbD8BAGwpXNM6l6SuHdpFdO+DR+p0FedtbIekBgBgS74xBZJaJDa+7++4YIChbr3BzHntE87b2AhJDQDAtsKNKfjRd/KCJj5GVNbUan3ZAQsiRSxwpgYAYGvhmtYF69ZrlBUHkhEbJDUAANsLN6ageeKz71Ct7li+Jej1jVl5IBnRRVIDAEgKjROfeo9XT71Xpsqa0KswuZlpGt4nKxbhwQIkNQAAR/FNzK6sPqoDh48rq2OacjObbkm1SXFpzvn5uurrydjBzDn/DPrV2AhJDQDAMUINtww066lzRqoOHqlrcS19auyJpAYA4AjhhltWfD3r6fHLh0hSyGvvmjSAhMaGSGoAAAnNt50UqLKp8TVGh1vOXbZZXq836LUuSXcs36LxA/LYerIZkhoAQMIKtJ0UaBvJ6HBL36wnI9esLzsQsqIKiYfmewCAhOTbTmqehFR+vY1UXFqheo9Xa3fs1xtRGGdAfxr7YaUGAGzAyBZMoook9lDbSb7HfvP/Nqld24/05ZETlscs0Z/GjkhqACDBGd2CSUSRxm5kO+nw8XodPm4uHpcaRih4vV7trakNmDT5rqE/jf2w/QQACczIFkyiChX7VYtLNG/ZJ1q7Y3/AgZHR3PopmpivOeefISn4IMyiifmtXgnzbY0t3bgn6O8Ja7FSAwAJKtwWjEsNlTzj8nMTbivKyPbRM2vK9cyacmV1SNVPBvVQYX6uf2sqGls/zVeIAs2DyrVoBczOq2t2RlIDAAkq3BZMIlfpGK1GkqQDh+v09JpyPb2m3P/BPy4/V3nudFVWHzNUph3I5SNPUZ+uHQJ2FJbCD8KMVLB+OZWN+uREO7Gx8xms1iCpAYAEZXQLJhGrdCKNqfEHf9HE8GMMQvnxmSeFTfbCDcI0KxFW15J5lYgzNQCQoIxuwSRilU6kMfmSgbnLNmvst7urc0aq6Xu41PAhHo+DvmZW16LBzmewrEBSAwAJanifLOW501scZvWJ54d3OOFiD8X3wf/c2vKAc5mMsOKgbyTiubpm5BzT3GWbHX1gmaQGABJUmxSXiibmS4pulU40hIrdqHc/qzL9nDx3ekzOrARjdIWqfN8Ry1873qtEiYCkBgAS2IQBeXr88iHKdTf9sMyN84e3EcFiN+qdbfsMXZeZ3la/GNVbL8wYqdW3jI3re2J0herBldss3wqy8xksq3BQGAASXLSqdGKhcewrN1fq6TXlpp6f4pJC7ZZkdUjVutmFatc2Mf6N7luhMnLA2eoDw3Y+g2WVxPhbAAAIyVelc8GgHiro19UWCY2PL/bbJp6hJy4fojwTKze+hCbQ9ptL0l0/OTNhEhqfCQPydENh/5DXRGMryM5nsKySWH8TAACONmFAnlbfMlYvzBips7+Vbeg500b1brGF1aVDqh65dJDc7dslZMfe3t06GLrOyq0gO5/BsgrbTwCAmGrcG+adbeEPAxfm52poryz9fmmpDnw97OnA4Tpd99LGJltTidSLJV5bQb5zTNHqlJzoSGoAAHHh2y4J1jXYN1jyy8O1uvovH7a4pvnCTCw79oZj9HeLxlaQnc9gtZbp7acFCxboyBHrS9EAAMnFyHbJbT/O1x3LtxgalZBIvVjivRVk5zNYrWE6qZk1a5Zyc3M1bdo0/d///V80YgIAJIlwJetdOrQzPENKSqxeLHYux7cr09tPe/bs0bJly/Tss89qzJgx6tu3r6ZOnarJkycrNzc3GjECABws1HbJ0o17IrpnovRiSeatoHhweb3eiNfo9u7dq8WLF2vhwoXaunWrJkyYoGnTpmnixIlKSUm8wqqamhq53W5VV1crMzMz3uEAAMJYu2O/LntqnennvTBjZMJNLkfkjH5+tyrz6N69u0aPHq2CggKlpKTo448/1uTJk9WvXz+9/fbbrbk1AMCAeo9Xa3fst7SsORr3jJTZGVLJ0IsFwUVU/bR3714999xzWrBggXbu3KlJkybp9ddfV2FhoQ4fPqx58+Zp8uTJ+ve//211vACArxWXVmjOa5+osqbW/1huZprmnH9GxOc1iksrWpQDx7NU2nfgdubiErmkkAeGk6UXC4Izvf00ceJE/eMf/9C3vvUtTZ8+XVdeeaWysppmxF988YVyc3Pl8XgsDba12H4CkGjqPd6IzlsUl1aEbMX/RAQHUYtLKzRzcUmLxMEXTTwPtwZKtpqPUEikPjWwltHPb9MrNTk5OXrnnXdUUFAQ9Jrs7GyVlZWZvTUAJBWjqyLNE5+hvbpo1pKPQ9571pKPTc0Vqvd4NXfZ5oArIV41JDZWzyoyI9CB26G9umjDv7/kAC78TCc1Z599toYMGdLi8ePHj+vFF1/UlVdeKZfLpV69elkSIAA4UbBVkeYN5AIlPl0y2urgkRMh73/wSJ3W7divUf27GYpnfdmBkKXTjUul43UAt3EnYh8OA6Mx0weFp06dqurq6haPHzp0SFOnTrUkKABwsnCrIlLDqsjfP/pcMxeXtEg2vgyT0Pis3bnPcExGS6B91yXSYWLAx/RKjdfrlcvVcnnvP//5j9xutyVBAYCTGV0V+f3SUkOddIMzvhVjZlZRoh0mBnwMJzWDBw+Wy+WSy+XSOeeco7Ztv3lqfX29ysrKNGHChKgECQBOYnRV5MDhula9jpmtmdbOYUqkuUtIXoaTmkmTJkmSNm7cqPHjx6tjx47+n7Vr1069e/fWhRdeaHmAAOA0Vk9mDqRLRqpG9jWe1IQqnW46hylxDxMDhpOaoqIiSVLv3r11ySWXKD09+v9RAoATGVkVyerQTvsPH4/4Neb/9EzTiYVvVlHzraXcr7eW3O1Dz2FKhMPESG6mz9RMnjw5GnEAQNIwsipyxwUDdMfyzSETH3dGqtLbtlFljXVnW6yYw5Qoc5eQfAwlNVlZWdq2bZu6deumLl26BDwo7HPgQPwnowJAogu3KjJhQJ5SUhQy8fnjT8+MyrDEQKXTkrnDxEA8GEpqHnjgAXXq1Mn/51BJDQDAmHATnI0kPlL0e7X4mv9VVh9VVod2+vLw8ZCHiZm7hHhp1ZRuu2FMAgA7inSUghUClW8HkgijFOBclo5JqKmpMfzCJAsAYK1g20HRFqzrcSDNV4+AeDCU1HTu3DnslpOvKV99fb0lgTVWXl6uO+64Q6tWrVJlZaVOOukkXX755br11lvVrl07y18PAJJdqK7HPlkdUnXbeWcoN5O5S0gMhpKat956K9pxhLR161Z5PB49+eSTOvXUU1VaWqoZM2bo8OHDuu++++IaGwDEQqy3oMJ1PZYamgPmZqZTvo2EYSipOfvss6MdR0gTJkxo0q24b9+++vTTT/X444+T1ABwvHiMJTA7CwpIBIaSmo8++kgDBgxQSkqKPvroo5DXfuc737EksHCqq6uVlcUJewDOZnSat1FGV3wo34YdGUpqBg0apMrKSuXk5GjQoEFyuVwKVDQVrTM1zW3fvl2PPPJI2FWa2tpa1dbW+r83c+AZAOIt3DRvs2MJzKz4GJ0FRfk2EkmKkYvKysqUnZ3t//POnTtVVlbW4mvnzp2mXnzWrFn+IZnBvrZu3drkOXv27NGECRP0s5/9TDNmzAh5//nz58vtdvu/evbsaSo+APZT7/Fq7Y79Wrpxj9bu2K96j327Vhid5r2+LHzTU9+KT/P7+VZ8iksrmjzu63ostZz17fu+aGI+h4ORUOLap6aqqkr79+8PeU3fvn39FU6ff/65xowZo5EjR+rZZ59VSkronCzQSk3Pnj3pUwM4VDzOnkTT0o179OsXN4a9btqo3rpt4hlBf17v8Wr03auCJki+VZfVt4xtkaQ47T2FPVnap6a5Tz/9VI888oi2bNkiSTr99NN17bXX6rTTTjN1n+zsbP8KUDh79uzRD37wAw0dOlQLFiwIm9BIUlpamtLS0kzFBMCerD57kgiMnld5ek25vtsnK+jvZ2bFp3klU7iux0AiMbT91Ngrr7yiAQMGaMOGDRo4cKAGDhyokpISDRgwQK+88ko0YtSePXs0ZswYnXLKKbrvvvtUVVWlyspKVVZWRuX1ANhLuLMnUsPZk9ZuRcV6a8t3rsVI+hDq92ttJZOv+d8Fg3qooF9XEhokLNMrNTfffLNmz56tefPmNXm8qKhIN998sy688ELLgvNZsWKFtm/fru3bt+vkk09u8rMkmvIAIIjWrEQYFY9tGN+5lqsWl4S9NtTvRyUTkoXplZqKigpdeeWVLR6//PLLVVFREeAZrTdlyhR5vd6AXwAQ7Z4qZg/ZWmnCgDz9YlRvQ9cG+/3Crfi41JCgUckEuzOd1IwZM0bvvfdei8dXr16t73//+5YEBQBmRHMlIlZbW6GMy881dF2w349KJiQLQ9tPr732mv/P559/vm655RZt2LBBI0eOlCStW7dOf/3rXzV37tzoRAkAIUSzp0prtrasGm3w5eHasNeEW2mZMCBPj18+pMUWGoMo4SSGSrqNVBpJsWu+FymjJWEA7Me3RSSpSWLjSyEirX4yWlb90KWDdMGgHk3iseIMTrhybJ/H/muIfvSd8PeN9QwpwApGP78NZSsej8fQVyInNACczbcSketuugWT605vVTl3JFtbVp7BMTJYUpK6dGhn6H5UMsHJIupTAwCJKBo9VcxubZkZbSApbKwMlgSMM53UNC/lbu7222+POBgAaC3fSoSV9yuamK+Zi0vkUuCtrcaHbI2ewfnTqs/04ge7w25PteYQNFtNSDamk5q//e1vTb6vq6tTWVmZ2rZtq379+pHUAHAcM4dsja6YPLDysxaPBep+HG6lSJI6t0+Vx+tVvcfrT1oYb4BkZMnsp5qaGk2ZMkU/+clPdMUVV1gRV1RwUBhAaxhZ+Vi7Y78ue2pdxK8RaA5TsEPQzeVmpmnO+Q0zoAKNjGjtoWkgXox+fls20PLjjz/WxIkTVV5ebsXtooKkBkC0+aqVQq2sGPHCjJEq6NdV9R6v1u3Yr+ffL9c7n+3T4drwBRmdM1J18EhdwJ+FGl4JJKqoDrQMpLq6WtXV1VbdDgBsKdwZHKOJzheHjqm4tEKzlnwcNEEJJtT1VoyMABKV6aTm4YcfbvK91+tVRUWFnnvuOZ177rmWBQYAdhXqDM6l3z1FD6zcFvYe5fuOGLouUlRLwYlMJzUPPPBAk+9TUlKUnZ2tyZMna/bs2ZYFBgB2Fqy8XJJe/GBXyBLx7plp+sv7/45qfK0dXkllFRKR6aSmrKwsGnEAgOM0Li9vnARc+t2eemDlZ0FLxC8bfkrA6igrtGZkhA+VVUhUNN8DgCgLlAR0zkiV1PT8i69EvPaEx5LXNdJXx+yKi68Sq/kqU6BydCDWDCU1P/3pTw3fcMmSJREHAwBOEywJqP46mbmh8Fvq3S2jSUKxdsf+Vr1ml4xU3TnpTN2xPHRfHbMrLma6JbMVhXgwlNS43e5oxwEAjmMkCXjxg10tyquH98lSbmaaKmvCT+cOZP5Pz9SEAXkaPyD4yIhIVlxaM7EciAVDSc2CBQuiHQcAOE6kSUCbFJfmnH+Grvq64V4wndun6uDRb7avmq+yBBsZEemKC3OokOgiPlNTVVWlTz/9VJJ02mmnKTs727KgAMAJWpMETBiQpycuHxKwT02XjFTN/+mZEQ/vjDTZas0cKiAWTCc1hw8f1rXXXqtFixbJ42k4zNamTRtdeeWVeuSRR5SRkWF5kABgR61NAnxl4et27NfanfskNay8jOzb1Z+8RLLNE2myZXZiORBrKWafcOONN+qdd97RsmXLdPDgQR08eFBLly7VO++8o9/85jfRiBFAHNV7vFq7Y7+WbtyjtTv2q95jyWSVpOBLAoKtnbjUsGUUKglok+LSqP7ddNP4b+um8adp1KndWn0IN9Jky9ctWVKL3ylQZRUQa6ZnP3Xr1k0vv/yyxowZ0+Txt956SxdffLGqqqqsjM9SzH4CzEn2fiRWNJgLNowynsMlw82nCjcfKtn/XiD2ojb76ciRI+revXuLx3NycnTkyBGztwOQoJK9H4mZD+5QyU+okQnxSgLCzaeSQq+4BOuWzAoN4s30Ss0555yjrl27atGiRUpPb1iaPHr0qCZPnqwDBw5o5cqVUQnUCqzUAMb4/iUf6jBpnk0mPUey2hIsoQu0umI0+Wntqk80xhKw4gK7MPr5bTqpKS0t1fjx41VbW6uBAwdKkjZt2qT09HT94x//0BlnnNG6yKOIpAYwZu2O/brsqXVhr7uhsL9+XfitGEQUmUg+tMMldI23ZlZsrjSc/MT692guWFLEDCfYQdSSGqlhC+r555/X1q1bJUmnn366fv7zn6t9+/aRRxwDJDWAMUs37tGvX9xo6NonEnQbysxqS2NGE7rnp4/QTX/dZCj5aU2SEOnv0fwerMjAzqJ2pkaSMjIyNGPGjIiDA5DYzPQZMdMWP1arAq1p52+03Hntjv1R765rxViCZD8bheRiuqR74cKFWr58uf/7m2++WZ07d9b3vvc9/fvf/7Y0OADx4StFNsL3wR1OcWmFRt+9Spc9tU6/fnGjLntqnUbfvUrFpRWtDbcFo83lnl1T1qJU3XhCZ2yRuzXddc00yQtUen/8hEe/+1tp0KRIakiKKNOHU5heqbnrrrv0+OOPS5LWrl2rP/3pT3rwwQf1+uuv64YbbmCgJeAAvuqYcG36fcJ9cMd6tcBoInHH8i3+P/u2Y8bl5xpqMFfQt5v+9NaOsK/RrUOawahbMvp7rNhcqRv/38YWU8Dr6j06XFsf9HnMaoLTmF6p2b17t0499VRJ0quvvqqLLrpIv/zlLzV//ny99957lgcIID4mDMjTDQYPAYda3Qi3hSJZv1oQSZv+yupjumpxie5cvlmXfvcUSaEbzI3s1zVkYz2f3/x1U8SrUUZ/j2fWlLdY0Tl4pC5kQtMYs5rgFKaTmo4dO2r//v2SpH/+858aN26cJCk9PV1Hjx61NjoAcXXN2FOVmxl8pcFIR1wzWyhWCdfJN1gcUkOC8MDKbXJnpMqdkdrkmlx3un9VKVR33cb21jSsRkWS2Bj5Paw4ksSsJjiF6aRm3Lhxmj59uqZPn65t27bpRz/6kSTpk08+Ue/eva2OD0Ac+aZFuxR5W/x4THY2mnCEUn2kTgeP1OmGwv566NJBen76CN130UDVnvD4z6z4Gut1D5H4tWY1yshYgtYscBlJSgE7MZ3UPProoyooKFBVVZVeeeUVde3asA+7YcMGXXbZZZYHCCC+fB/cuc0ODjdetQglXpOdg8VtlK+66MUPdis1JUU3/XWTfv70+y0OOU8YkKf/uXhQ2Hv5DiabTWxCvf/TRvU2da9AmNUEJ4moT41d0acGiFyk5ditnTNkZdz7DtU2ORzcGo37xNSe8Bju6xNpf5hA7//6sgOGeuoE0rVDO935kwGUc8MWotp8z65IaoD4SJShjvUer4b+YYUOHqmz5H6+hOy+iwbq50+/b/g5kjW/c7iEMZisDqlaN7tQ7dqaXqwH4sLo5zd/owFEXWu3sKyyYnOlZQmN9M22klwyfDDZyoovs2eHfGej7vrJmSQ0cCRWagDETDznDBkZ0hmphy4dpLS2KQFXo0J5YcZIS/rDBBqD0CUjVV6pSRLHaATYVVTHJABAJNqkuOLW5C1caXlr5HRKV0G/rnr88iEtkotQrKr4mjAgT+Pyc1skjJIYVomkQlIDIClEo8Gc70yNL4HwJRfPrikzdCDZyoqvYAkjnYKRTAwlNYMHD5bLZSy7Lykx1lYdAGKpfN8RS+8XrE9PmxSXpozqoz+vLgtb8UV/GMBahpKaSZMm+f987NgxPfbYY8rPz1dBQYEkad26dfrkk0/03//931EJEgDMaH5258vDtXpw5TZLXyM3xPkU3wHemYtL5FLgii/6wwDWM31QePr06crLy9Mdd9zR5PGioiLt3r1bzzzzjKUBWomDwoDzBTo0m+JqXefdQJ6fNkKj+nczHQuHdQHzotanxu1261//+pf69+/f5PHPPvtMw4YNU3V1dWQRxwBJDeBswaaBG9V8VSWUhy4dpAsG9Qh7XTwrvgCniFr1U/v27bVmzZoWSc2aNWuUns5QNADxEWoauBG/GNVbb5RWGq5cMnrIN54VX0CyMZ3UXH/99Zo5c6ZKSko0fPhwSdL777+vZ555RrfddpvlAQKAEa0t2R6Xn6tbf5yvdTv36+rnS3TwaOAmffE65MuKDxCe6aRm1qxZ6tu3rx566CEtXrxYknT66adrwYIFuvjiiy0PEACMiLRku3GS0ibFpVGndtMfLzwz5FiHWB/y5WwOYAwdhQE4wtod+00Pdww1hylREolg54RiPTcLiKeodhQ+ePCgXn75Ze3cuVM33XSTsrKyVFJSou7du6tHj/AH5wAgEqG2YIb3yVKeOz3kcMfmB4FDlWUH69IbyxWaUOeEvGr4feYu26xx+blsRQGKIKn56KOPVFhYKLfbrfLyck2fPl1ZWVlasmSJdu3apUWLFkUjTgBJLtzKSajeMD7ujFRN/V4f9e6WYShJifch33DnhHwDNdeXHeAwMqAIpnTfeOONmjJlij777LMm1U4/+tGP9O6771oaHABI32zBNP+Ar6w+ppmLS1RcWiHpm2ng7ozUgPepPlKnB1duU1rbFBX062rJ6ka9x6u1O/Zr6cY9Wrtjf4vJ2+F+HorRc0LRGAEB2JHplZoPPvhATz75ZIvHe/ToocrKSkuCAuAsrancMbsFMy4/V3Ne2yypZfWS1Vs24VaPWnsux2jZuJUzpAA7M53UpKWlqaampsXj27ZtU3Z2tiVBAXCO1n6wm92CWV92QJU10d+yCXaA17d69Muz+uh/3y0L+nMjB3zDnRNihhTQlOntp/PPP1/z5s1TXV3Dv4JcLpd27dqlW265RRdeeKHlAQKwL6PbRqGY3YKJxZZNuNUjr6Sn3muZ0Ph+LjWsFoXbivKdE5K+qXbyYYYU0JLppOZ//ud/9NVXXyknJ0dHjx7V2WefrVNPPVWdOnXSnXfeGY0YAdhQuA9+ydgHu9ktmFhs2Rhp9Bfq12q8WhSO75xQrrtpvLnudMq5gWZMbz+53W6tWLFCa9as0aZNm/TVV19pyJAhKiwsjEZ8AGzKqsods1swsdiysepgrtH7JEJ5OWAHpldqFi1apNraWo0aNUr//d//rZtvvlmFhYU6fvw45dwA/KzaBjK7BROLLRurDuaauY+vvPyCQT0sq9wCnMZ0UjN16tSAk7gPHTqkqVOnWhIUAPuzchvI7BZMtLdsfKtBodKKFFfLpMrHpYbD0hzwBaxlevvJ6/XK5Wr5n+p//vMfud1uS4ICYH9WbwOZ3YKJ5pZNqEZ/vrvP+H5D9VOwn3PAF7Ce4aRm8ODBcrlccrlcOuecc9S27TdPra+vV1lZmSZMmBCVIAHYj5EPfrMf7GY7/EazI7BvNah5uXrj0QuDT+kS8ucArGU4qZk0aZIkaePGjRo/frw6duzo/1m7du3Uu3dvSroBNGHkg9/Owq0GccAXiC3TU7oXLlyoSy+9VGlpadGKKaTa2lqNGDFCmzZt0ocffqhBgwYZfi5TuoH4aE1HYQAw+vlt+qBwfn6+Nm7c2OLx999/X//617/M3s60m2++WSeddFLUXweANUhoAMSK6aTm6quv1u7du1s8vmfPHl199dWWBBXMG2+8oX/+85+67777ovo6gF20ZlhiLBSXVmj03at02VPr9OsXN+qyp9Zp9N2rDHUSBgCzTFc/bd68WUOGDGnx+ODBg7V582ZLggpk7969mjFjhl599VVlZGQYek5tba1qa2v93weaWQXYVWtnKpkRyWpLuNlIkZRWs+oDIJSIBlru3btXffv2bfJ4RUVFk4ooK3m9Xk2ZMkVXXXWVhg0bpvLyckPPmz9/vubOnRuVmIB4ikbCEOq1zCZPZidrRysOAMnF9PbTD3/4Q82ePbtJA76DBw/qd7/7ncaNG2fqXrNmzfKXiQf72rp1qx555BEdOnRIs2fPNnV/X5y+r0DbZoDdWDVTyYhIB1KaGZEQzTgAJBfTSyv33XefzjrrLPXq1UuDBw+W1FDm3b17dz333HOm7vWb3/xGU6ZMCXlN3759tWrVKq1du7ZFxdWwYcP085//XAsXLgz43LS0tLhVaQHRYtVMpcYCbetIini1xcpJ2dFY9QHgTKaTmh49euijjz7S888/r02bNql9+/aaOnWqLrvsMqWmppq6V3Z2trKzs8Ne9/DDD+sPf/iD//vPP/9c48eP10svvaQRI0aY/RUAW7MyYZCCb+tc+t2eESdPVo5IiEYSB8CZIjoE06FDB/3yl7+0OpagTjnllCbf+xr/9evXTyeffHLM4gASgZUJQ6izOQ+s/MzQ6wRKnqwckWB1EgfAuQwlNa+99prOPfdcpaam6rXXXgt57fnnn29JYAACsyphMHI2x4hAyZOVIxKsTOIAOJuhpGbSpEmqrKxUTk6Of1xCIC6XS/X19VbFFlTv3r1lshEy4BhWJQzhtnXCCZc8WTUiwerBmACcy1BS4/F4Av4ZQHxYkTCY2a6JNHlqPPuosvqoDhw+rqyOaXK3b6d6j9fQSk00BmMCcKboNJYBEHWtHZZodLvmhsJv6cUPdoVNnoI1xmuT4lL10eO65x+fRtxjxumDMQFYw9BAy4cfftjwDa+77rpWBRRNDLQEvlHv8Wr03avCbuusvmWsJIVMnkI1xpMU8DCy79lmGgXSURhITkY/vw0lNX369GnyfVVVlY4cOaLOnTtLami+l5GRoZycHO3cubN1kUcRSQ3QlK/6SQq8rWMk4QhWQeXbKuqY1lZf1Z4I+NzGiRPJCYBgLJ3SXVZW5v+68847NWjQIG3ZskUHDhzQgQMHtGXLFg0ZMkR33HGHZb8AgOjzbevkuptuReW60w0lNEYqqIIlNL5rzHQWBoBQDK3UNNavXz+9/PLL/m7CPhs2bNBFF12ksrIySwO0Eis1QGCRbuus3bFflz21rtWv/9Clg3TBoB6tvg8AZzL6+W36oHBFRYVOnGj5L6/6+nrt3bvX7O0AJIA2Ka6IuvFa1fCOHjMArGB6oOU555yjX/3qVyopKfE/tmHDBs2cOVOFhYWWBgcgcdV7vNp3qLZV93BJ6tqhnSqrj2rtjv2WDOEEkLxMbz9VVVVp8uTJKi4u9s96OnHihMaPH69nn31WOTk5UQnUCmw/AdYIVO1kBTNl3gCSh6XVT4Fs27ZNW7dulSR9+9vf1re+9a3IIo0hkhqg9YJVO1khkjJvAM4XtTM1Pr5RBf369VPbtvTwg3PRG+UboaqdzGjeGdjH+/XP5i7brHH5uUn7PgOIjOls5MiRI7r22mu1cOFCSQ0rNn379tW1116rHj16aNasWZYHCcRLqKZyTlhJMJuwWTEvyqvQAzMbl3lHcngZQPIyfVB49uzZ2rRpk95++22lp39TsVBYWKiXXnrJ0uCAePJtszT/EK+sPqaZi0tUXFoRp8isUVxaodF3r9JlT63Tr1/cqMueWqfRd68K+Xu1ttop152uX4zqbehaqyqrACQP00nNq6++qj/96U8aPXq0XK5v/kV3xhlnaMeOHZYGB8SLkaZyc5dttm21TqQJW6Sl153bp+r56SO0+paxGpefa+g5lHkDMMt0UlNVVRWwwunw4cNNkhzAzsJts9i5E25rErbhfbKU506X0f/SXV9//fHCMzXq1G5qk+IKew+XGrb4hvfJMvgqANDAdFIzbNgwLV++3P+9L5H585//rIKCAusiA+LI6NaHHbdIjCZs63bu19od+7V04x5/D5k2KS7/kEojiU2gcQuh7uH7vmhiPoeEAZhm+qDwXXfdpXPPPVebN2/WiRMn9NBDD2nz5s36v//7P73zzjvRiBGIOaNbH3bcIjGaiF39fIkOHq3zf9/4gPTjlw8JeID6th/nq0uHdmEPHge7R66DDmEDiD3TSc3o0aO1adMmzZ8/X2eeeab++c9/asiQIVq7dq3OPPPMaMQIxJxvi6Sy+ljAbRrfdGm7bZGY6QLcOKGRvjlv41t5GZef26pSdyvuAQCNmWq+V1dXp1/96le67bbb1KdPn2jGFRU034MZvsO0UtMSZLs2iLOiC7AvmVt9y1iSDwAxY/Tz29SZmtTUVL3yyiutDg6wA98WSa676RZToHMiiS5YtZNZdj4gDcD5TG8/TZo0Sa+++qpuuOGGaMQDJBQnbJGY6QLcOSNVB4/Uhb3OjgekATif6aSmf//+mjdvntasWaOhQ4eqQ4cOTX5+3XXXWRYckAjapLhs3dnWaBfg2358ur6dm6mfP/1+2GvteEAagPOZTmqefvppde7cWRs2bNCGDRua/MzlcpHUAAnG6KpKt05pGtmvqyMPSANIDqaTmrKysmjEASBKunVIM3ydr4fMzMUlLYZO0kMGQKIz3XyvMa/XKxPFUwDiwUz7XznrgDSA5GJ6pUZq2IJ64IEH9Nlnn0lqOGdz/fXXa/r06ZYGB6D19n1lrC9N4+uccEAaQPIxndTcfvvtuv/++3Xttdf6xyKsXbtWN9xwg3bt2qV58+ZZHiSAyEXaHdnuB6QBJB9TzfckKTs7Ww8//LAuu+yyJo+/8MILuvbaa7Vv3z5LA7QSzfeQjOo9Xo2+e1XYw7801AOQqKLSfE9q6Co8bNiwFo8PHTpUJ06cMHs7wJHqPd4WwyDjhQGSAJKF6ZWaa6+9Vqmpqbr//vubPH7TTTfp6NGjevTRRy0N0Eqs1CAWAo0jyEuAQY2JGhcAhGP08zuipGbRokXq2bOnRo4cKUl6//33tWvXLl155ZVKTU31X9s88Yk3khpEm28cQfP/qBJlXlS9x8vhXwC2E7Wk5gc/+IGh61wul1atWmXm1lFHUoNo8p1dCda9l7MrABAZo5/fpquf3nrrrVYFBjhVuHEEjYdBUlUEANZrVfM9AN8wOo6AYZAAEB0kNYBFIu0HAwCwBkkNYJHhfbKU504POpXApYZqI4ZBAkB0kNQAFqEfDADEF0kNYCGGQQJA/EQ00BJAcAyDBID4IKkBAmhtkzqGQQJA7JHUAM0wTgAA7IkzNUAjvjEHzZvoVVYf08zFJSourYhTZACAcFipAb5W7/Fq7rLNLeY2SQ3dgF2S5i7brHH5uZIU0zMzjbfDunVMk7zSvsO1nNcBgEZIaoCvGR1z8KdV2/XiB7titj0VaDusMbbGAKAB20/A14yOL3hg5baYbU8F2w6LxWsDgN2Q1ABfa834At+W1dxlm1XvMTX4PqhQ22HRfm0AsCOSGuBr4cYchNN4CrcVwm2HRfO1AcCOSGqArxkZc2CEVVO4I7kPE8ABJDOSGjhavcertTv2a+nGPVq7Y3/Y7ZlQYw5uKOxv6DWtmsIdyX2YAA4gmVH9BMeKtIlesDEHkvTiB7tVWX0s4DkXlxqSH6umcPu2w4K9XjRfGwDsiJUaOFJrm+j5xhxcMKiHCvp1VZsUV8yncId6vWi/NgDYEUkNHCdcEz0p8kqhWE/hDvZ6sXhtALAbtp/gOEab6K0vOxDR0MlYT+Fu/np0FAaAwEhq4DhGK4BaUykU6yncTP0GgPDYfoLjGK0AolIIAJyFlRo4TriqIasrhRoPm2Q7CADih6QGjuOrGpq5uEQuqUliY3WlUKRl4wAA67H9BEeKRZVSa8vGAQDWstVKzfLlyzVv3jx99NFHSk9P19lnn61XX3013mEhQUWzSilc2bhLDWXj4/Jz2YoCgBixTVLzyiuvaMaMGbrrrrs0duxYnThxQqWlpfEOCwkuWlVD0S4bBwCYZ4uk5sSJE/r1r3+te++9V9OmTfM/np+fH8eokMxiUTYOADDHFmdqSkpKtGfPHqWkpGjw4MHKy8vTueeey0oN4oaycQBIPLZIanbu3ClJmjNnjn7/+9/r9ddfV5cuXTRmzBgdOHAg6PNqa2tVU1PT5Auwgq9sPNhpGZcaqqAYMAkAsRPXpGbWrFlyuVwhv7Zu3SqPxyNJuvXWW3XhhRdq6NChWrBggVwul/76178Gvf/8+fPldrv9Xz179ozVrwaHi/VwSwBAeC6v12t+qp9FqqqqtH///pDX9O3bV2vWrNHYsWP13nvvafTo0f6fjRgxQoWFhbrzzjsDPre2tla1tbX+72tqatSzZ09VV1crMzPTml8CSY0+NQAQfTU1NXK73WE/v+N6UDg7O1vZ2dlhrxs6dKjS0tL06aef+pOauro6lZeXq1evXkGfl5aWprS0NMviBZqL9XBLAEBwtqh+yszM1FVXXaWioiL17NlTvXr10r333itJ+tnPfhbn6JDsGDYJAInBFkmNJN17771q27atrrjiCh09elQjRozQqlWr1KVLl3iHBgAAEkBcz9TEmtE9OQAAkDiMfn7boqQbAAAgHJIaAADgCCQ1AADAEUhqAACAI5DUAAAARyCpAQAAjkBSAwAAHIGkBgAAOAJJDQAAcASSGgAA4AgkNQAAwBFsM9ASodV7vFpfdkBfHDqmnE7pGt4nS21SXPEOCwCAmCGpcYDi0grNXbZZFdXH/I/ludNVNDFfEwbkxTEyAABih+0nmysurdDMxSVNEhpJqqw+ppmLS1RcWhGnyAAAiC2SGhur93g1d9lmeQP8zPfY3GWbVe8JdAUAAM5CUmNj68sOtFihacwrqaL6mNaXHYhdUAAAxAlJjY19cSh4QhPJdQAA2BlJjY3ldEq39DoAAOyM6icbG94nS3nudFVWHwt4rsYlKdfdUN6dTChvB4DkRFJjY21SXCqamK+Zi0vkkpokNr6P8KKJ+Un1gU55OwAkL7afbG7CgDw9fvkQ5bqbbjHlutP1+OVDkuqDnPJ2AEhurNQ4wIQBeRqXn5vUWy7hyttdaihvH5efm1TvCwAkE5Iah2iT4lJBv67xDiNuzJS3J/P7BABOxvYTHIHydgAASQ0cgfJ2AABJDRzBV94e7LSMSw1VUMlW3g4AyYSkBo7gK2+X1CKxSdbydgBINiQ1cAzK2wEguVH9FCd0vY0OytsBIHmR1MQBXW+jK9nL2wEgWbH9FGN0vQUAIDpIamIoXNdbqaHrbb0n0BUAACAUkpoYMtP1FgAAmENSE0N0vQUAIHpIamKIrrcAAEQPSU0M0fUWAIDoIamJITt3va33eLV2x34t3bhHa3fs5zAzACDh0Kcmxnxdb5v3qclN4D419NUBANiBy+v1Js0/uWtqauR2u1VdXa3MzMy4xmKXjsK+vjrN/5L4ImX8AAAg2ox+frNSEyd26Hobrq+OSw19dcbl5yZkQgYASC6cqUFQ9NUBANgJSQ2Coq8OAMBOSGoQFH11AAB2QlKDoOirAwCwE5IaBGXnvjoAgORDUoOQfH11ct1Nt5hy3emUcwMAEgol3QhrwoA8jcvPtUVfHQBA8iKpgSF26KsDAEhubD8BAABHIKkBAACOQFIDAAAcgaQGAAA4AkkNAABwBJIaAADgCCQ1AADAEUhqAACAI5DUAAAARyCpAQAAjkBSAwAAHME2Sc22bdt0wQUXqFu3bsrMzNTo0aP11ltvxTssAACQIGyT1Jx33nk6ceKEVq1apQ0bNmjgwIE677zzVFlZGde46j1erd2xX0s37tHaHftV7/HGNR4AAJKVy+v1Jvyn8L59+5Sdna13331X3//+9yVJhw4dUmZmplasWKHCwkJD96mpqZHb7VZ1dbUyMzNbHVdxaYXmLtusiupj/sfy3OkqmpivCQPyWn1/AABg/PPbFis1Xbt21WmnnaZFixbp8OHDOnHihJ588knl5ORo6NChQZ9XW1urmpqaJl9WKS6t0MzFJU0SGkmqrD6mmYtLVFxaYdlrAQCA8GyR1LhcLq1cuVIffvihOnXqpPT0dN1///0qLi5Wly5dgj5v/vz5crvd/q+ePXtaEk+9x6u5yzYr0BKX77G5yzazFQUAQAzFNamZNWuWXC5XyK+tW7fK6/Xq6quvVk5Ojt577z2tX79ekyZN0sSJE1VREXxFZPbs2aqurvZ/7d6925K415cdaLFC05hXUkX1Ma0vO2DJ6wEAgPDaxvPFf/Ob32jKlCkhr+nbt69WrVql119/XV9++aV/L+2xxx7TihUrtHDhQs2aNSvgc9PS0pSWlmZ12PriUPCEJpLrAABA68U1qcnOzlZ2dnbY644cOSJJSklpurCUkpIij8cTldhCyemUbul1AACg9WxxpqagoEBdunTR5MmTtWnTJm3btk2//e1vVVZWph//+Mcxj2d4nyzludPlCvJzlxqqoIb3yYplWAAAJDVbJDXdunVTcXGxvvrqK40dO1bDhg3T6tWrtXTpUg0cODDm8bRJcaloYr4ktUhsfN8XTcxXm5RgaQ8AALCaLfrUWIU+NQAA2I/Rz++4nqmxuwkD8jQuP1fryw7oi0PHlNOpYcuJFRoAAGKPpKaV2qS4VNCva7zDAAAg6dniTA0AAEA4JDUAAMARSGoAAIAjkNQAAABHIKkBAACOQFIDAAAcgaQGAAA4AkkNAABwBJIaAADgCEnVUdg35qqmpibOkQAAAKN8n9vhxlUmVVJz6NAhSVLPnj3jHAkAADDr0KFDcrvdQX+eVFO6PR6PPv/8c3Xq1Ekul3OGTtbU1Khnz57avXu3JdPHERjvc2zwPkcf73Fs8D5bx+v16tChQzrppJOUkhL85ExSrdSkpKTo5JNPjncYUZOZmcl/ODHA+xwbvM/Rx3scG7zP1gi1QuPDQWEAAOAIJDUAAMARSGocIC0tTUVFRUpLS4t3KI7G+xwbvM/Rx3scG7zPsZdUB4UBAIBzsVIDAAAcgaQGAAA4AkkNAABwBJIah9m2bZsuuOACdevWTZmZmRo9erTeeuuteIflSMuXL9eIESPUvn17denSRZMmTYp3SI5VW1urQYMGyeVyaePGjfEOx1HKy8s1bdo09enTR+3bt1e/fv1UVFSk48ePxzs023v00UfVu3dvpaena8SIEVq/fn28Q3I8khqHOe+883TixAmtWrVKGzZs0MCBA3XeeeepsrIy3qE5yiuvvKIrrrhCU6dO1aZNm7RmzRr913/9V7zDcqybb75ZJ510UrzDcKStW7fK4/HoySef1CeffKIHHnhATzzxhH73u9/FOzRbe+mll3TjjTeqqKhIJSUlGjhwoMaPH68vvvgi3qE5mxeOUVVV5ZXkfffdd/2P1dTUeCV5V6xYEcfInKWurs7bo0cP75///Od4h5IU/v73v3u//e1vez/55BOvJO+HH34Y75Ac75577vH26dMn3mHY2vDhw71XX321//v6+nrvSSed5J0/f34co3I+VmocpGvXrjrttNO0aNEiHT58WCdOnNCTTz6pnJwcDR06NN7hOUZJSYn27NmjlJQUDR48WHl5eTr33HNVWloa79AcZ+/evZoxY4aee+45ZWRkxDucpFFdXa2srKx4h2Fbx48f14YNG1RYWOh/LCUlRYWFhVq7dm0cI3M+khoHcblcWrlypT788EN16tRJ6enpuv/++1VcXKwuXbrEOzzH2LlzpyRpzpw5+v3vf6/XX39dXbp00ZgxY3TgwIE4R+ccXq9XU6ZM0VVXXaVhw4bFO5yksX37dj3yyCP61a9+Fe9QbGvfvn2qr69X9+7dmzzevXt3jgJEGUmNDcyaNUsulyvk19atW+X1enX11VcrJydH7733ntavX69JkyZp4sSJqqioiPevkfCMvs8ej0eSdOutt+rCCy/U0KFDtWDBArlcLv31r3+N82+R+Iy+z4888ogOHTqk2bNnxztkWzL6Pje2Z88eTZgwQT/72c80Y8aMOEUORI6OwjZQVVWl/fv3h7ymb9++eu+99/TDH/5QX375ZZOJsP3799e0adM0a9asaIdqa0bf5zVr1mjs2LF67733NHr0aP/PRowYocLCQt15553RDtXWjL7PF198sZYtWyaXy+V/vL6+Xm3atNHPf/5zLVy4MNqh2prR97ldu3aSpM8//1xjxozRyJEj9eyzzyolhX/zRur48ePKyMjQyy+/3KQqcvLkyTp48KCWLl0av+Acrm28A0B42dnZys7ODnvdkSNHJKnF/4xSUlL8qwsIzuj7PHToUKWlpenTTz/1JzV1dXUqLy9Xr169oh2m7Rl9nx9++GH94Q9/8H//+eefa/z48XrppZc0YsSIaIboCEbfZ6lhheYHP/iBf9WRhKZ12rVrp6FDh+rNN9/0JzUej0dvvvmmrrnmmvgG53AkNQ5SUFCgLl26aPLkybr99tvVvn17PfXUUyorK9OPf/zjeIfnGJmZmbrqqqtUVFSknj17qlevXrr33nslST/72c/iHJ1znHLKKU2+79ixoySpX79+Ovnkk+MRkiPt2bNHY8aMUa9evXTfffepqqrK/7Pc3Nw4RmZvN954oyZPnqxhw4Zp+PDhevDBB3X48GFNnTo13qE5GkmNg3Tr1k3FxcW69dZbNXbsWNXV1emMM87Q0qVLNXDgwHiH5yj33nuv2rZtqyuuuEJHjx7ViBEjtGrVKg5kw3ZWrFih7du3a/v27S2SRU4nRO6SSy5RVVWVbr/9dlVWVmrQoEEqLi5ucXgY1uJMDQAAcAQ2TgEAgCOQ1AAAAEcgqQEAAI5AUgMAAByBpAYAADgCSQ0AAHAEkhoAAOAIJDUAAMARSGoAOMrbb78tl8ulgwcPGn7OlClTmgweDGTMmDG6/vrr/d/37t1bDz74oP97l8ulV1991VSsAKzFmAQATYwZM0aDBg1q8oENacmSJUpNTQ3684qKCv+YjPLycvXp00cffvihBg0aFKMIAZDUADDN6/Wqvr5ebdsmz/9CsrKyQv6c4Y9A/LH9BMBvypQpeuedd/TQQw/J5XLJ5XKpvLzcv6XzxhtvaOjQoUpLS9Pq1asDbttcf/31GjNmjP97j8ej+fPnq0+fPmrfvr0GDhyol19+OWQcjz32mPr376/09HR1795dF110kf9ntbW1uu6665STk6P09HSNHj1aH3zwQdB7zZkzp8VqyYMPPqjevXu3uHbu3LnKzs72T2I/fvy4/2fNt5+aa7z91KdPH0nS4MGD5XK5NGbMGL377rtKTU1VZWVlk+ddf/31+v73vx/0vgCMI6kB4PfQQw+poKBAM2bMUEVFhSoqKtSzZ0//z2fNmqU//vGP2rJli77zne8Yuuf8+fO1aNEiPfHEE/rkk090ww036PLLL9c777wT8Pp//etfuu666zRv3jx9+umnKi4u1llnneX/+c0336xXXnlFCxcuVElJiU499VSNHz9eBw4caNXv/uabb2rLli16++239cILL2jJkiWaO3duRPdav369JGnlypWqqKjQkiVLdNZZZ6lv37567rnn/NfV1dXp+eef1y9+8YtWxQ6gQfKsHQMIy+12q127dsrIyAi4nTJv3jyNGzfO8P1qa2t11113aeXKlSooKJAk9e3bV6tXr9aTTz6ps88+u8Vzdu3apQ4dOui8885Tp06d1KtXLw0ePFiSdPjwYT3++ON69tlnde6550qSnnrqKa1YsUJPP/20fvvb30bya0uS2rVrp2eeeUYZGRk644wzNG/ePP32t7/VHXfcoZQUc//+y87OliR17dq1yfs4bdo0LViwwB/nsmXLdOzYMV188cURxw3gG6zUADBs2LBhpq7fvn27jhw5onHjxqljx47+r0WLFmnHjh0BnzNu3Dj16tVLffv21RVXXKHnn39eR44ckSTt2LFDdXV1GjVqlP/61NRUDR8+XFu2bIn8F5M0cOBAZWRk+L8vKCjQV199pd27d7fqvo1NmTJF27dv17p16yRJzz77rC6++GJ16NDBstcAkhkrNQAMa/7hm5KSIq/X2+Sxuro6/5+/+uorSdLy5cvVo0ePJtelpaUFfI1OnTqppKREb7/9tv75z3/q9ttv15w5c0KemwklXIyxlJOTo4kTJ2rBggXq06eP3njjDb399ttxiQVwIlZqADTRrl071dfXG7o2OztbFRUVTR7buHGj/8/5+flKS0vTrl27dOqppzb5anxWp7m2bduqsLBQ99xzjz766COVl5dr1apV6tevn9q1a6c1a9b4r62rq9MHH3yg/Pz8oDFWVlY2SWwax+izadMmHT161P/9unXr1LFjx5BxBtOuXTtJCvg+Tp8+XS+99JL+93//V/369Wuy6gSgdVipAdBE79699f7776u8vFwdO3YMWco8duxY3XvvvVq0aJEKCgq0ePFilZaW+s/AdOrUSTfddJNuuOEGeTwejR49WtXV1VqzZo0yMzM1efLkFvd8/fXXtXPnTp111lnq0qWL/v73v8vj8ei0005Thw4dNHPmTP32t79VVlaWTjnlFN1zzz06cuSIpk2bFjDGMWPGqKqqSvfcc48uuugiFRcX64033lBmZmaT644fP65p06bp97//vcrLy1VUVKRrrrnG9HkaqWFFpn379iouLtbJJ5+s9PR0ud1uSdL48eOVmZmpP/zhD5o3b57pewMIjpUaAE3cdNNNatOmjfLz85Wdna1du3YFvXb8+PG67bbbdPPNN+u73/2uDh06pCuvvLLJNXfccYduu+02zZ8/X6effromTJig5cuX+8uem+vcubOWLFmisWPH6vTTT9cTTzyhF154QWeccYYk6Y9//KMuvPBCXXHFFRoyZIi2b9+uf/zjH/7Gd82dfvrpeuyxx/Too49q4MCBWr9+vW666aYW151zzjnq37+/zjrrLF1yySU6//zzNWfOHIPvWlNt27bVww8/rCeffFInnXSSLrjgAv/PUlJSNGXKFNXX17d4rwC0jsvbfLMZABBV06ZNU1VVlV577bV4hwI4CttPABAj1dXV+vjjj/WXv/yFhAaIApIaAIiRCy64QOvXr9dVV11lqt8PAGPYfgIAAI7AQWEAAOAIJDUAAMARSGoAAIAjkNQAAABHIKkBAACOQFIDAAAcgaQGAAA4AkkNAABwBJIaAADgCP8fbMlpI5PE+CAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(x2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hsIJ8KzcdGS",
        "outputId": "c27afe5a-9bc0-4735-cce9-506f8b42a33e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-5.68479724,  0.06257979])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Custom models, e.g. neural network"
      ],
      "metadata": {
        "id": "HFr0k0HWco60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric as tg\n",
        "from torch_geometric.data import Dataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.nn import global_add_pool"
      ],
      "metadata": {
        "id": "4YXULRQTdOZq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first make a custom NN, similar to the MLPRegressor from scikit-learn:"
      ],
      "metadata": {
        "id": "B32TrEc2Wmd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        super(FeatureDataset, self).__init__()\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def process_key(self, key):\n",
        "        data = tg.data.Data()\n",
        "        data.f = torch.tensor([self.features[key]], dtype=torch.float)\n",
        "        data.y = torch.tensor([self.labels[key]], dtype=torch.float)\n",
        "        return data\n",
        "\n",
        "    def get(self,key):\n",
        "        return self.process_key(key)\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def construct_loader(data_path, shuffle=True, batch_size=50, std=None):\n",
        "    data_df = pd.read_csv(data_path)\n",
        "    x_raw = np.array([features(smi) for smi in data_df['smiles']])\n",
        "    try:\n",
        "      f = x_raw / std\n",
        "    except:\n",
        "      std = np.max(np.abs(x_raw),axis=0)+0.1\n",
        "      f = x_raw / std\n",
        "\n",
        "    labels = data_df.iloc[:, 1].values.astype(np.float32)\n",
        "    dataset = FeatureDataset(f, labels)\n",
        "    loader = DataLoader(dataset=dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=shuffle,\n",
        "                            num_workers=0,\n",
        "                            pin_memory=True,\n",
        "                            sampler=None)\n",
        "    return loader, std\n",
        "\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(NN, self).__init__()\n",
        "\n",
        "        self.hidden_size = 300\n",
        "        self.n1 = nn.Linear(num_features, self.hidden_size)\n",
        "        self.n2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.n3 = nn.Linear(self.hidden_size, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.f\n",
        "        x = self.n1(x)\n",
        "        x = self.n2(x)\n",
        "        x = self.n3(x)\n",
        "        return x.squeeze(-1)\n",
        "\n",
        "def train_epoch(model, loader, optimizer, loss):\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(data)\n",
        "        result = loss(out, data.y)\n",
        "        result.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        loss_all += loss(out, data.y)\n",
        "\n",
        "    return math.sqrt(loss_all / len(loader.dataset))\n",
        "\n",
        "def pred(model, loader, loss):\n",
        "    model.eval()\n",
        "\n",
        "    preds, ys = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            pred = model(data)\n",
        "            preds.extend(pred.cpu().detach().tolist())\n",
        "\n",
        "    return preds\n",
        "\n",
        "def train(folder):\n",
        "    torch.manual_seed(0)\n",
        "    train_loader, std = construct_loader(folder+\"/train_full.csv\", True, batch_size=10000)\n",
        "    val_loader, _ = construct_loader(folder+\"/val_full.csv\", False, std=std)\n",
        "    test_loader, _ = construct_loader(folder+\"/test_full.csv\", False, std=std)\n",
        "\n",
        "\n",
        "    model = NN(217)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss = nn.MSELoss(reduction='sum')\n",
        "    print(model)\n",
        "\n",
        "    for epoch in range(0, 200):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, loss)\n",
        "        preds = pred(model, val_loader, loss)\n",
        "        print(\"Epoch\",epoch,\"  Train RMSE\", train_loss,\"   Val RMSE\", root_mean_squared_error(preds,val_loader.dataset.labels))\n",
        "\n",
        "    preds = pred(model, test_loader, loss)\n",
        "    print(\"Test RMSE\", root_mean_squared_error(preds,test_loader.dataset.labels))\n",
        "    print(\"Test MAE\", mean_absolute_error(preds,test_loader.dataset.labels))\n",
        "\n"
      ],
      "metadata": {
        "id": "RASL8ux0gx5v"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(\"https://github.com/hesther/rxn_workshop/raw/main/data/esol\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tYKuFoiSXwg",
        "outputId": "893ac7af-3c4a-42ba-ed06-6e11268df77e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN(\n",
            "  (n1): Linear(in_features=217, out_features=300, bias=True)\n",
            "  (n2): Linear(in_features=300, out_features=300, bias=True)\n",
            "  (n3): Linear(in_features=300, out_features=1, bias=True)\n",
            ")\n",
            "Epoch 0   Train RMSE 3.802357298852543    Val RMSE 3.2494654655456543\n",
            "Epoch 1   Train RMSE 3.471708668347743    Val RMSE 2.931489944458008\n",
            "Epoch 2   Train RMSE 3.1403468374173804    Val RMSE 2.5941290855407715\n",
            "Epoch 3   Train RMSE 2.785004899244947    Val RMSE 2.246241807937622\n",
            "Epoch 4   Train RMSE 2.4098342358892886    Val RMSE 1.9533767700195312\n",
            "Epoch 5   Train RMSE 2.0728355149018407    Val RMSE 1.856888771057129\n",
            "Epoch 6   Train RMSE 1.9135465588314156    Val RMSE 2.0273890495300293\n",
            "Epoch 7   Train RMSE 2.031203518849195    Val RMSE 2.2340691089630127\n",
            "Epoch 8   Train RMSE 2.217385208100364    Val RMSE 2.2612013816833496\n",
            "Epoch 9   Train RMSE 2.242288726754621    Val RMSE 2.120316743850708\n",
            "Epoch 10   Train RMSE 2.108345062866553    Val RMSE 1.9029979705810547\n",
            "Epoch 11   Train RMSE 1.907616360116316    Val RMSE 1.7023284435272217\n",
            "Epoch 12   Train RMSE 1.7349306196032843    Val RMSE 1.5809615850448608\n",
            "Epoch 13   Train RMSE 1.6490413770972838    Val RMSE 1.5465642213821411\n",
            "Epoch 14   Train RMSE 1.6476959440786703    Val RMSE 1.5594291687011719\n",
            "Epoch 15   Train RMSE 1.6837962442573902    Val RMSE 1.5714002847671509\n",
            "Epoch 16   Train RMSE 1.708026098520968    Val RMSE 1.552934169769287\n",
            "Epoch 17   Train RMSE 1.6923065256401522    Val RMSE 1.4965529441833496\n",
            "Epoch 18   Train RMSE 1.630333583842427    Val RMSE 1.4119139909744263\n",
            "Epoch 19   Train RMSE 1.5319004623556818    Val RMSE 1.321531057357788\n",
            "Epoch 20   Train RMSE 1.418974044519411    Val RMSE 1.2553890943527222\n",
            "Epoch 21   Train RMSE 1.3219234863479201    Val RMSE 1.2372980117797852\n",
            "Epoch 22   Train RMSE 1.2686909403467497    Val RMSE 1.2647823095321655\n",
            "Epoch 23   Train RMSE 1.2646122872035153    Val RMSE 1.3050661087036133\n",
            "Epoch 24   Train RMSE 1.2826307811571456    Val RMSE 1.318112850189209\n",
            "Epoch 25   Train RMSE 1.283195042693582    Val RMSE 1.2829011678695679\n",
            "Epoch 26   Train RMSE 1.2436932728161492    Val RMSE 1.2063634395599365\n",
            "Epoch 27   Train RMSE 1.1705278919062931    Val RMSE 1.1164761781692505\n",
            "Epoch 28   Train RMSE 1.0923423973776838    Val RMSE 1.0463368892669678\n",
            "Epoch 29   Train RMSE 1.0415566704212376    Val RMSE 1.0138096809387207\n",
            "Epoch 30   Train RMSE 1.0312147423467986    Val RMSE 1.0099482536315918\n",
            "Epoch 31   Train RMSE 1.0454066782950961    Val RMSE 1.010116457939148\n",
            "Epoch 32   Train RMSE 1.0551460682058404    Val RMSE 0.9954514503479004\n",
            "Epoch 33   Train RMSE 1.0405465334156998    Val RMSE 0.9644393920898438\n",
            "Epoch 34   Train RMSE 1.0006680045575325    Val RMSE 0.9315605163574219\n",
            "Epoch 35   Train RMSE 0.9517306149559964    Val RMSE 0.9161160588264465\n",
            "Epoch 36   Train RMSE 0.9169921224998313    Val RMSE 0.9250375628471375\n",
            "Epoch 37   Train RMSE 0.9093065347493542    Val RMSE 0.9440439939498901\n",
            "Epoch 38   Train RMSE 0.918721738367655    Val RMSE 0.9494917392730713\n",
            "Epoch 39   Train RMSE 0.9221574706175145    Val RMSE 0.928102970123291\n",
            "Epoch 40   Train RMSE 0.9051587012541084    Val RMSE 0.8857240676879883\n",
            "Epoch 41   Train RMSE 0.872605043443981    Val RMSE 0.8415164351463318\n",
            "Epoch 42   Train RMSE 0.842380967150338    Val RMSE 0.8131659030914307\n",
            "Epoch 43   Train RMSE 0.8290590380938936    Val RMSE 0.8037336468696594\n",
            "Epoch 44   Train RMSE 0.8312940958703412    Val RMSE 0.8026785254478455\n",
            "Epoch 45   Train RMSE 0.8354975175301308    Val RMSE 0.7988792657852173\n",
            "Epoch 46   Train RMSE 0.829937552722625    Val RMSE 0.7904372215270996\n",
            "Epoch 47   Train RMSE 0.8137916788071865    Val RMSE 0.7839491367340088\n",
            "Epoch 48   Train RMSE 0.7959197807960695    Val RMSE 0.7862865328788757\n",
            "Epoch 49   Train RMSE 0.7863824862522663    Val RMSE 0.7965059280395508\n",
            "Epoch 50   Train RMSE 0.787201944591962    Val RMSE 0.8059372305870056\n",
            "Epoch 51   Train RMSE 0.7910167552798638    Val RMSE 0.8061683773994446\n",
            "Epoch 52   Train RMSE 0.7890958495693579    Val RMSE 0.7961767315864563\n",
            "Epoch 53   Train RMSE 0.7795290111235098    Val RMSE 0.7823312282562256\n",
            "Epoch 54   Train RMSE 0.7678106213097572    Val RMSE 0.772270917892456\n",
            "Epoch 55   Train RMSE 0.7605998894112863    Val RMSE 0.768517017364502\n",
            "Epoch 56   Train RMSE 0.7592419386088128    Val RMSE 0.7676711678504944\n",
            "Epoch 57   Train RMSE 0.7593528587347278    Val RMSE 0.7651053071022034\n",
            "Epoch 58   Train RMSE 0.7559832094462351    Val RMSE 0.7595813870429993\n",
            "Epoch 59   Train RMSE 0.7483480061520627    Val RMSE 0.7536598443984985\n",
            "Epoch 60   Train RMSE 0.7399767424820646    Val RMSE 0.7503842711448669\n",
            "Epoch 61   Train RMSE 0.7348739065625121    Val RMSE 0.7498738169670105\n",
            "Epoch 62   Train RMSE 0.733556047309628    Val RMSE 0.7493671178817749\n",
            "Epoch 63   Train RMSE 0.7329670009132713    Val RMSE 0.7464512586593628\n",
            "Epoch 64   Train RMSE 0.7301161623986303    Val RMSE 0.7416672706604004\n",
            "Epoch 65   Train RMSE 0.7251571271250343    Val RMSE 0.7377257347106934\n",
            "Epoch 66   Train RMSE 0.7207413743369967    Val RMSE 0.736475944519043\n",
            "Epoch 67   Train RMSE 0.7188238023102047    Val RMSE 0.7370783090591431\n",
            "Epoch 68   Train RMSE 0.7186472031432404    Val RMSE 0.7371985912322998\n",
            "Epoch 69   Train RMSE 0.7179262997568966    Val RMSE 0.7355250120162964\n",
            "Epoch 70   Train RMSE 0.7154604643529183    Val RMSE 0.7328127026557922\n",
            "Epoch 71   Train RMSE 0.7121818467937293    Val RMSE 0.7306810617446899\n",
            "Epoch 72   Train RMSE 0.7098005656752021    Val RMSE 0.7297646999359131\n",
            "Epoch 73   Train RMSE 0.7088048197891328    Val RMSE 0.729282557964325\n",
            "Epoch 74   Train RMSE 0.7080831132505127    Val RMSE 0.7282811999320984\n",
            "Epoch 75   Train RMSE 0.7064111846366712    Val RMSE 0.7268459796905518\n",
            "Epoch 76   Train RMSE 0.7038611690806535    Val RMSE 0.7258471250534058\n",
            "Epoch 77   Train RMSE 0.70152815583919    Val RMSE 0.7257262468338013\n",
            "Epoch 78   Train RMSE 0.7001314439999728    Val RMSE 0.7259092926979065\n",
            "Epoch 79   Train RMSE 0.6992652707101468    Val RMSE 0.7254805564880371\n",
            "Epoch 80   Train RMSE 0.6980615982526635    Val RMSE 0.7241889834403992\n",
            "Epoch 81   Train RMSE 0.6962696516800968    Val RMSE 0.7226114869117737\n",
            "Epoch 82   Train RMSE 0.6944445432556929    Val RMSE 0.7214163541793823\n",
            "Epoch 83   Train RMSE 0.6931661417164902    Val RMSE 0.7206821441650391\n",
            "Epoch 84   Train RMSE 0.6923386488416421    Val RMSE 0.7200261354446411\n",
            "Epoch 85   Train RMSE 0.6913987929809513    Val RMSE 0.7192321419715881\n",
            "Epoch 86   Train RMSE 0.6900616734695564    Val RMSE 0.7185057401657104\n",
            "Epoch 87   Train RMSE 0.6886177513418266    Val RMSE 0.7180907726287842\n",
            "Epoch 88   Train RMSE 0.6874798641724003    Val RMSE 0.7178407311439514\n",
            "Epoch 89   Train RMSE 0.6866489474740236    Val RMSE 0.7173315286636353\n",
            "Epoch 90   Train RMSE 0.6857775221380227    Val RMSE 0.7163426280021667\n",
            "Epoch 91   Train RMSE 0.6846601179698721    Val RMSE 0.7150838971138\n",
            "Epoch 92   Train RMSE 0.6834698810066583    Val RMSE 0.7139292359352112\n",
            "Epoch 93   Train RMSE 0.6824784035462159    Val RMSE 0.7130451202392578\n",
            "Epoch 94   Train RMSE 0.6816890885140557    Val RMSE 0.7123608589172363\n",
            "Epoch 95   Train RMSE 0.6808716992276692    Val RMSE 0.71181720495224\n",
            "Epoch 96   Train RMSE 0.6798976923961616    Val RMSE 0.7114802002906799\n",
            "Epoch 97   Train RMSE 0.6788902210586548    Val RMSE 0.7113766670227051\n",
            "Epoch 98   Train RMSE 0.6780149774908513    Val RMSE 0.7113388180732727\n",
            "Epoch 99   Train RMSE 0.677246951850067    Val RMSE 0.7111214995384216\n",
            "Epoch 100   Train RMSE 0.6764322133411488    Val RMSE 0.7106473445892334\n",
            "Epoch 101   Train RMSE 0.6755165507804123    Val RMSE 0.7100656032562256\n",
            "Epoch 102   Train RMSE 0.6746025610813232    Val RMSE 0.7095748782157898\n",
            "Epoch 103   Train RMSE 0.6737848240304768    Val RMSE 0.709257185459137\n",
            "Epoch 104   Train RMSE 0.6730216466035481    Val RMSE 0.7090967893600464\n",
            "Epoch 105   Train RMSE 0.6722180577439009    Val RMSE 0.7090777158737183\n",
            "Epoch 106   Train RMSE 0.6713700725919356    Val RMSE 0.7091831564903259\n",
            "Epoch 107   Train RMSE 0.6705569935114873    Val RMSE 0.7093222737312317\n",
            "Epoch 108   Train RMSE 0.6698156333736859    Val RMSE 0.7093426585197449\n",
            "Epoch 109   Train RMSE 0.6690947873816284    Val RMSE 0.7091496586799622\n",
            "Epoch 110   Train RMSE 0.6683422628204142    Val RMSE 0.7087859511375427\n",
            "Epoch 111   Train RMSE 0.6675787785651331    Val RMSE 0.7083755135536194\n",
            "Epoch 112   Train RMSE 0.6668534588395939    Val RMSE 0.7080144882202148\n",
            "Epoch 113   Train RMSE 0.6661653993510367    Val RMSE 0.7077341675758362\n",
            "Epoch 114   Train RMSE 0.6654710752863496    Val RMSE 0.7075323462486267\n",
            "Epoch 115   Train RMSE 0.6647546190701756    Val RMSE 0.7073882818222046\n",
            "Epoch 116   Train RMSE 0.6640461190391124    Val RMSE 0.7072417736053467\n",
            "Epoch 117   Train RMSE 0.6633683110444925    Val RMSE 0.7070041298866272\n",
            "Epoch 118   Train RMSE 0.66270368341971    Val RMSE 0.7066216468811035\n",
            "Epoch 119   Train RMSE 0.6620279222647407    Val RMSE 0.7061237692832947\n",
            "Epoch 120   Train RMSE 0.6613486091339005    Val RMSE 0.705595850944519\n",
            "Epoch 121   Train RMSE 0.6606886483156211    Val RMSE 0.7051145434379578\n",
            "Epoch 122   Train RMSE 0.6600483913454331    Val RMSE 0.7047157883644104\n",
            "Epoch 123   Train RMSE 0.6594087103954209    Val RMSE 0.7044011354446411\n",
            "Epoch 124   Train RMSE 0.6587639069582175    Val RMSE 0.7041451334953308\n",
            "Epoch 125   Train RMSE 0.6581280945707855    Val RMSE 0.7038942575454712\n",
            "Epoch 126   Train RMSE 0.6575087555059407    Val RMSE 0.7035888433456421\n",
            "Epoch 127   Train RMSE 0.6568952521732857    Val RMSE 0.7032039165496826\n",
            "Epoch 128   Train RMSE 0.6562785187836937    Val RMSE 0.7027674317359924\n",
            "Epoch 129   Train RMSE 0.6556649552090056    Val RMSE 0.7023341059684753\n",
            "Epoch 130   Train RMSE 0.6550631462845927    Val RMSE 0.7019467949867249\n",
            "Epoch 131   Train RMSE 0.6544693676764807    Val RMSE 0.70162034034729\n",
            "Epoch 132   Train RMSE 0.6538756195879265    Val RMSE 0.7013434171676636\n",
            "Epoch 133   Train RMSE 0.6532834987788716    Val RMSE 0.701083242893219\n",
            "Epoch 134   Train RMSE 0.652699950044939    Val RMSE 0.7007952928543091\n",
            "Epoch 135   Train RMSE 0.6521244936943466    Val RMSE 0.7004467844963074\n",
            "Epoch 136   Train RMSE 0.65155104482996    Val RMSE 0.7000383734703064\n",
            "Epoch 137   Train RMSE 0.6509791051687353    Val RMSE 0.6996000409126282\n",
            "Epoch 138   Train RMSE 0.6504132149470557    Val RMSE 0.6991676688194275\n",
            "Epoch 139   Train RMSE 0.6498543530291889    Val RMSE 0.6987625956535339\n",
            "Epoch 140   Train RMSE 0.6492984066420615    Val RMSE 0.6983858942985535\n",
            "Epoch 141   Train RMSE 0.6487443496663844    Val RMSE 0.6980205774307251\n",
            "Epoch 142   Train RMSE 0.6481950835296632    Val RMSE 0.6976383328437805\n",
            "Epoch 143   Train RMSE 0.647651931877144    Val RMSE 0.6972151398658752\n",
            "Epoch 144   Train RMSE 0.6471121468508244    Val RMSE 0.696747362613678\n",
            "Epoch 145   Train RMSE 0.6465746767530071    Val RMSE 0.6962541341781616\n",
            "Epoch 146   Train RMSE 0.6460414648522147    Val RMSE 0.6957629323005676\n",
            "Epoch 147   Train RMSE 0.6455135143218652    Val RMSE 0.6952933669090271\n",
            "Epoch 148   Train RMSE 0.6449890129499072    Val RMSE 0.6948495507240295\n",
            "Epoch 149   Train RMSE 0.6444669980477256    Val RMSE 0.6944196224212646\n",
            "Epoch 150   Train RMSE 0.6439488178003885    Val RMSE 0.6939826607704163\n",
            "Epoch 151   Train RMSE 0.6434351530768717    Val RMSE 0.6935203075408936\n",
            "Epoch 152   Train RMSE 0.6429248326669484    Val RMSE 0.6930289268493652\n",
            "Epoch 153   Train RMSE 0.6424171222862138    Val RMSE 0.6925207376480103\n",
            "Epoch 154   Train RMSE 0.6419130959576359    Val RMSE 0.692014753818512\n",
            "Epoch 155   Train RMSE 0.6414131805380033    Val RMSE 0.6915234327316284\n",
            "Epoch 156   Train RMSE 0.640916478906423    Val RMSE 0.6910476684570312\n",
            "Epoch 157   Train RMSE 0.6404226960605913    Val RMSE 0.6905761957168579\n",
            "Epoch 158   Train RMSE 0.639932560608537    Val RMSE 0.6900927424430847\n",
            "Epoch 159   Train RMSE 0.6394461974535983    Val RMSE 0.689585268497467\n",
            "Epoch 160   Train RMSE 0.6389629155846581    Val RMSE 0.6890542507171631\n",
            "Epoch 161   Train RMSE 0.6384826053066199    Val RMSE 0.6885107159614563\n",
            "Epoch 162   Train RMSE 0.6380058338711723    Val RMSE 0.6879686117172241\n",
            "Epoch 163   Train RMSE 0.6375325624714119    Val RMSE 0.6874359846115112\n",
            "Epoch 164   Train RMSE 0.6370621907568299    Val RMSE 0.6869112253189087\n",
            "Epoch 165   Train RMSE 0.6365949358235043    Val RMSE 0.6863850951194763\n",
            "Epoch 166   Train RMSE 0.6361310856356209    Val RMSE 0.6858471035957336\n",
            "Epoch 167   Train RMSE 0.6356704835550422    Val RMSE 0.6852930188179016\n",
            "Epoch 168   Train RMSE 0.6352128082281249    Val RMSE 0.6847278475761414\n",
            "Epoch 169   Train RMSE 0.6347583007389944    Val RMSE 0.6841620206832886\n",
            "Epoch 170   Train RMSE 0.6343071323415364    Val RMSE 0.6836046576499939\n",
            "Epoch 171   Train RMSE 0.6338590280622772    Val RMSE 0.6830581426620483\n",
            "Epoch 172   Train RMSE 0.6334139708791302    Val RMSE 0.6825175285339355\n",
            "Epoch 173   Train RMSE 0.6329720378442867    Val RMSE 0.6819744110107422\n",
            "Epoch 174   Train RMSE 0.6325332826218268    Val RMSE 0.6814227104187012\n",
            "Epoch 175   Train RMSE 0.6320974996618406    Val RMSE 0.680863082408905\n",
            "Epoch 176   Train RMSE 0.631664836657783    Val RMSE 0.6803014278411865\n",
            "Epoch 177   Train RMSE 0.6312351819933604    Val RMSE 0.6797444820404053\n",
            "Epoch 178   Train RMSE 0.6308087071718882    Val RMSE 0.6791952252388\n",
            "Epoch 179   Train RMSE 0.6303851586271618    Val RMSE 0.6786504983901978\n",
            "Epoch 180   Train RMSE 0.6299646368773799    Val RMSE 0.6781042218208313\n",
            "Epoch 181   Train RMSE 0.6295472189969612    Val RMSE 0.6775510311126709\n",
            "Epoch 182   Train RMSE 0.6291327690525536    Val RMSE 0.6769903898239136\n",
            "Epoch 183   Train RMSE 0.6287212929135028    Val RMSE 0.676426351070404\n",
            "Epoch 184   Train RMSE 0.6283128675708096    Val RMSE 0.6758643984794617\n",
            "Epoch 185   Train RMSE 0.6279073328574516    Val RMSE 0.6753073930740356\n",
            "Epoch 186   Train RMSE 0.6275048131112244    Val RMSE 0.6747542023658752\n",
            "Epoch 187   Train RMSE 0.6271052190905023    Val RMSE 0.6742009520530701\n",
            "Epoch 188   Train RMSE 0.6267086039453948    Val RMSE 0.6736442446708679\n",
            "Epoch 189   Train RMSE 0.6263149495432366    Val RMSE 0.6730837821960449\n",
            "Epoch 190   Train RMSE 0.6259241662435963    Val RMSE 0.6725231409072876\n",
            "Epoch 191   Train RMSE 0.6255363308916079    Val RMSE 0.6719662547111511\n",
            "Epoch 192   Train RMSE 0.6251514251377874    Val RMSE 0.6714155673980713\n",
            "Epoch 193   Train RMSE 0.624769358994074    Val RMSE 0.6708698272705078\n",
            "Epoch 194   Train RMSE 0.6243901854034531    Val RMSE 0.6703259348869324\n",
            "Epoch 195   Train RMSE 0.6240139335183198    Val RMSE 0.6697812676429749\n",
            "Epoch 196   Train RMSE 0.6236405369453156    Val RMSE 0.6692360639572144\n",
            "Epoch 197   Train RMSE 0.6232700008162315    Val RMSE 0.6686925292015076\n",
            "Epoch 198   Train RMSE 0.6229022345471136    Val RMSE 0.6681534647941589\n",
            "Epoch 199   Train RMSE 0.6225373866639192    Val RMSE 0.6676197052001953\n",
            "Test RMSE 0.7563788890838623\n",
            "Test MAE 0.5938556790351868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Custom models, e.g. attention layer\n",
        "\n",
        "We can build any architecture or logic we like. For example, instead of the NN, we could make an attention layer (of course, one simple attention layer is not enough for a good model, but it is fun to build it):"
      ],
      "metadata": {
        "id": "O4njTxvaW1n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Att(nn.Module):\n",
        "    def __init__(self, k, v):\n",
        "        super(Att, self).__init__()\n",
        "\n",
        "        self.hidden_size = 300\n",
        "        self.feature_size = k.shape[1]\n",
        "        self.nq = nn.Linear(k.shape[1], self.hidden_size)\n",
        "        self.nk = nn.Linear(k.shape[1], self.hidden_size)\n",
        "        self.nv = nn.Linear(k.shape[0], k.shape[0])\n",
        "        self.k = k\n",
        "        self.v = v\n",
        "\n",
        "    def forward(self, data):\n",
        "        keys = self.nk(self.k)\n",
        "        queries = self.nq(data.f)\n",
        "        values = self.nv(self.v)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.feature_size, dtype=torch.float32))\n",
        "        # Apply softmax\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        # Multiply weights with values\n",
        "        x = torch.matmul(attention_weights, values)\n",
        "\n",
        "        return x.squeeze(-1)\n",
        "\n",
        "\n",
        "def train(folder):\n",
        "    torch.manual_seed(0)\n",
        "    train_loader, std = construct_loader(folder+\"/train_full.csv\", False)\n",
        "    val_loader, _ = construct_loader(folder+\"/val_full.csv\", False, std=std)\n",
        "    test_loader, _ = construct_loader(folder+\"/test_full.csv\", False, std=std)\n",
        "\n",
        "    model = Att(torch.tensor(train_loader.dataset.features, dtype=torch.float), torch.tensor(train_loader.dataset.labels, dtype=torch.float))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss = nn.MSELoss(reduction='sum')\n",
        "    print(model)\n",
        "\n",
        "    for epoch in range(0, 200):\n",
        "        train_loss = train_epoch(model, val_loader, optimizer, loss)\n",
        "        preds = pred(model, test_loader, loss)\n",
        "        print(\"Epoch\",epoch,\"  Train RMSE\", train_loss,\"   Val RMSE\", root_mean_squared_error(preds,test_loader.dataset.labels))\n",
        "\n",
        "    preds = pred(model, test_loader, loss)\n",
        "    print(\"Test RMSE\", root_mean_squared_error(preds,test_loader.dataset.labels))\n",
        "    print(\"Test MAE\", mean_absolute_error(preds,test_loader.dataset.labels))\n",
        "\n"
      ],
      "metadata": {
        "id": "OiQZthZOW9wR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(\"https://github.com/hesther/rxn_workshop/raw/main/data/esol\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKDrdZN2auw9",
        "outputId": "8ca3c61a-8563-4672-dcb6-1d78d7bd21c6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Att(\n",
            "  (nq): Linear(in_features=217, out_features=300, bias=True)\n",
            "  (nk): Linear(in_features=217, out_features=300, bias=True)\n",
            "  (nv): Linear(in_features=902, out_features=902, bias=True)\n",
            ")\n",
            "Epoch 0   Train RMSE 2.761583381583381    Val RMSE 4.025290012359619\n",
            "Epoch 1   Train RMSE 3.3364270317871396    Val RMSE 2.2252161502838135\n",
            "Epoch 2   Train RMSE 2.097691256836618    Val RMSE 2.759619951248169\n",
            "Epoch 3   Train RMSE 2.6950903759525398    Val RMSE 2.218815565109253\n",
            "Epoch 4   Train RMSE 1.9861767016282672    Val RMSE 2.5919389724731445\n",
            "Epoch 5   Train RMSE 2.2916890461146844    Val RMSE 2.547104597091675\n",
            "Epoch 6   Train RMSE 2.1024774878132404    Val RMSE 2.1643099784851074\n",
            "Epoch 7   Train RMSE 2.0220256578306057    Val RMSE 2.2533295154571533\n",
            "Epoch 8   Train RMSE 2.080011873578147    Val RMSE 2.1647093296051025\n",
            "Epoch 9   Train RMSE 1.9167401327344655    Val RMSE 2.3500921726226807\n",
            "Epoch 10   Train RMSE 2.003285451286141    Val RMSE 2.197751045227051\n",
            "Epoch 11   Train RMSE 1.8699937744852975    Val RMSE 2.064741611480713\n",
            "Epoch 12   Train RMSE 1.814701781876065    Val RMSE 1.9347347021102905\n",
            "Epoch 13   Train RMSE 1.5909219970427333    Val RMSE 1.840740442276001\n",
            "Epoch 14   Train RMSE 1.4075141311125854    Val RMSE 1.4635441303253174\n",
            "Epoch 15   Train RMSE 1.132676004539319    Val RMSE 1.2981189489364624\n",
            "Epoch 16   Train RMSE 1.043009667897609    Val RMSE 1.3181750774383545\n",
            "Epoch 17   Train RMSE 1.0544522623585857    Val RMSE 1.2482327222824097\n",
            "Epoch 18   Train RMSE 0.9476753214075168    Val RMSE 1.177603840827942\n",
            "Epoch 19   Train RMSE 0.8235360160831291    Val RMSE 1.20527184009552\n",
            "Epoch 20   Train RMSE 0.789334505240058    Val RMSE 1.2212576866149902\n",
            "Epoch 21   Train RMSE 0.7995499984394833    Val RMSE 1.1803257465362549\n",
            "Epoch 22   Train RMSE 0.7536352985087792    Val RMSE 1.1299257278442383\n",
            "Epoch 23   Train RMSE 0.6884754803155281    Val RMSE 1.0978561639785767\n",
            "Epoch 24   Train RMSE 0.6696918415137121    Val RMSE 1.0567405223846436\n",
            "Epoch 25   Train RMSE 0.641612733601438    Val RMSE 1.0415940284729004\n",
            "Epoch 26   Train RMSE 0.6196972002971712    Val RMSE 1.0269564390182495\n",
            "Epoch 27   Train RMSE 0.5909353601345331    Val RMSE 1.029563307762146\n",
            "Epoch 28   Train RMSE 0.5687989832147398    Val RMSE 1.0304484367370605\n",
            "Epoch 29   Train RMSE 0.5588020789861405    Val RMSE 1.0261253118515015\n",
            "Epoch 30   Train RMSE 0.5403471580055058    Val RMSE 1.0061585903167725\n",
            "Epoch 31   Train RMSE 0.5299085341558029    Val RMSE 0.993052065372467\n",
            "Epoch 32   Train RMSE 0.513030024670466    Val RMSE 0.9771751165390015\n",
            "Epoch 33   Train RMSE 0.508577504169566    Val RMSE 0.9745742678642273\n",
            "Epoch 34   Train RMSE 0.49247742242909137    Val RMSE 0.9722359776496887\n",
            "Epoch 35   Train RMSE 0.49023416222799476    Val RMSE 0.981452226638794\n",
            "Epoch 36   Train RMSE 0.47396057428801347    Val RMSE 0.9789660573005676\n",
            "Epoch 37   Train RMSE 0.47617762298494437    Val RMSE 0.9881561994552612\n",
            "Epoch 38   Train RMSE 0.4584298537955209    Val RMSE 0.9783157110214233\n",
            "Epoch 39   Train RMSE 0.46988518271204005    Val RMSE 0.9941803216934204\n",
            "Epoch 40   Train RMSE 0.4489951696868832    Val RMSE 0.9853443503379822\n",
            "Epoch 41   Train RMSE 0.48025211629743814    Val RMSE 1.023087501525879\n",
            "Epoch 42   Train RMSE 0.4547811857365144    Val RMSE 1.0158599615097046\n",
            "Epoch 43   Train RMSE 0.5199177960379827    Val RMSE 1.0880588293075562\n",
            "Epoch 44   Train RMSE 0.49991092782569213    Val RMSE 1.0799307823181152\n",
            "Epoch 45   Train RMSE 0.605647817404288    Val RMSE 1.184388518333435\n",
            "Epoch 46   Train RMSE 0.5987322477364978    Val RMSE 1.1379683017730713\n",
            "Epoch 47   Train RMSE 0.6953817825946618    Val RMSE 1.19542396068573\n",
            "Epoch 48   Train RMSE 0.649363511491471    Val RMSE 1.0664962530136108\n",
            "Epoch 49   Train RMSE 0.6333210632323534    Val RMSE 1.0510390996932983\n",
            "Epoch 50   Train RMSE 0.5263919010624163    Val RMSE 0.981785774230957\n",
            "Epoch 51   Train RMSE 0.4488186588733321    Val RMSE 0.9786838889122009\n",
            "Epoch 52   Train RMSE 0.4332061616122121    Val RMSE 1.002777338027954\n",
            "Epoch 53   Train RMSE 0.41169155327802626    Val RMSE 0.9892290830612183\n",
            "Epoch 54   Train RMSE 0.45637164649786566    Val RMSE 1.0185515880584717\n",
            "Epoch 55   Train RMSE 0.43222951633773066    Val RMSE 0.9888431429862976\n",
            "Epoch 56   Train RMSE 0.4511723208734861    Val RMSE 1.0060114860534668\n",
            "Epoch 57   Train RMSE 0.42091751940458405    Val RMSE 0.9817877411842346\n",
            "Epoch 58   Train RMSE 0.41898752991276106    Val RMSE 0.9893427491188049\n",
            "Epoch 59   Train RMSE 0.401819090743608    Val RMSE 0.9803103804588318\n",
            "Epoch 60   Train RMSE 0.3966756650494411    Val RMSE 0.982302725315094\n",
            "Epoch 61   Train RMSE 0.39399232976244136    Val RMSE 0.9838345646858215\n",
            "Epoch 62   Train RMSE 0.38781688331137176    Val RMSE 0.9823427796363831\n",
            "Epoch 63   Train RMSE 0.39151591746173986    Val RMSE 0.9884381890296936\n",
            "Epoch 64   Train RMSE 0.38365184476093944    Val RMSE 0.9836709499359131\n",
            "Epoch 65   Train RMSE 0.38961210535598495    Val RMSE 0.9907376766204834\n",
            "Epoch 66   Train RMSE 0.37992242836995255    Val RMSE 0.9837351441383362\n",
            "Epoch 67   Train RMSE 0.38786836696661103    Val RMSE 0.9922136068344116\n",
            "Epoch 68   Train RMSE 0.3762327436120828    Val RMSE 0.984120786190033\n",
            "Epoch 69   Train RMSE 0.38748841806761564    Val RMSE 0.9957605004310608\n",
            "Epoch 70   Train RMSE 0.37304331992487566    Val RMSE 0.9857733845710754\n",
            "Epoch 71   Train RMSE 0.3901582217825492    Val RMSE 1.002158761024475\n",
            "Epoch 72   Train RMSE 0.3715653083637388    Val RMSE 0.9891321063041687\n",
            "Epoch 73   Train RMSE 0.3995920171911166    Val RMSE 1.014559030532837\n",
            "Epoch 74   Train RMSE 0.3759833199030025    Val RMSE 0.9992870688438416\n",
            "Epoch 75   Train RMSE 0.42407022760106855    Val RMSE 1.0437195301055908\n",
            "Epoch 76   Train RMSE 0.3982737896752939    Val RMSE 1.029852271080017\n",
            "Epoch 77   Train RMSE 0.4804752473469395    Val RMSE 1.1114861965179443\n",
            "Epoch 78   Train RMSE 0.46447428277336716    Val RMSE 1.1074371337890625\n",
            "Epoch 79   Train RMSE 0.5942948752467777    Val RMSE 1.2480500936508179\n",
            "Epoch 80   Train RMSE 0.6065111354912615    Val RMSE 1.249624490737915\n",
            "Epoch 81   Train RMSE 0.7734900177358974    Val RMSE 1.4231384992599487\n",
            "Epoch 82   Train RMSE 0.8017682575470091    Val RMSE 1.351050853729248\n",
            "Epoch 83   Train RMSE 0.9266128346246547    Val RMSE 1.420828938484192\n",
            "Epoch 84   Train RMSE 0.8729009997131063    Val RMSE 1.196216106414795\n",
            "Epoch 85   Train RMSE 0.8239395247160144    Val RMSE 1.1332343816757202\n",
            "Epoch 86   Train RMSE 0.6392324245037373    Val RMSE 0.9800003170967102\n",
            "Epoch 87   Train RMSE 0.48346794476410293    Val RMSE 0.9653734564781189\n",
            "Epoch 88   Train RMSE 0.4069680324086182    Val RMSE 0.9964636564254761\n",
            "Epoch 89   Train RMSE 0.37056421344301105    Val RMSE 0.989357590675354\n",
            "Epoch 90   Train RMSE 0.44589344411185305    Val RMSE 1.0363107919692993\n",
            "Epoch 91   Train RMSE 0.42448386870356236    Val RMSE 0.990610659122467\n",
            "Epoch 92   Train RMSE 0.4567829664033292    Val RMSE 1.0112857818603516\n",
            "Epoch 93   Train RMSE 0.40847066845289026    Val RMSE 0.9680063128471375\n",
            "Epoch 94   Train RMSE 0.3981111162061428    Val RMSE 0.9751491546630859\n",
            "Epoch 95   Train RMSE 0.3657684297527033    Val RMSE 0.9618450403213501\n",
            "Epoch 96   Train RMSE 0.353674057502324    Val RMSE 0.9629825949668884\n",
            "Epoch 97   Train RMSE 0.3525129398378779    Val RMSE 0.9692304134368896\n",
            "Epoch 98   Train RMSE 0.34237807858682195    Val RMSE 0.9633448719978333\n",
            "Epoch 99   Train RMSE 0.3543249108641893    Val RMSE 0.976095974445343\n",
            "Epoch 100   Train RMSE 0.3423908630921763    Val RMSE 0.9647791385650635\n",
            "Epoch 101   Train RMSE 0.35480758465897877    Val RMSE 0.9773439764976501\n",
            "Epoch 102   Train RMSE 0.3411024505267472    Val RMSE 0.9649233222007751\n",
            "Epoch 103   Train RMSE 0.3521607924170182    Val RMSE 0.9766491651535034\n",
            "Epoch 104   Train RMSE 0.3384102810933018    Val RMSE 0.9650090932846069\n",
            "Epoch 105   Train RMSE 0.348724812568851    Val RMSE 0.976353108882904\n",
            "Epoch 106   Train RMSE 0.33557271003075373    Val RMSE 0.9651976227760315\n",
            "Epoch 107   Train RMSE 0.34601532583596273    Val RMSE 0.9762794971466064\n",
            "Epoch 108   Train RMSE 0.33274257982093014    Val RMSE 0.9648412466049194\n",
            "Epoch 109   Train RMSE 0.344623290848809    Val RMSE 0.9767853617668152\n",
            "Epoch 110   Train RMSE 0.3302602547620757    Val RMSE 0.9643243551254272\n",
            "Epoch 111   Train RMSE 0.3451635168786208    Val RMSE 0.9788909554481506\n",
            "Epoch 112   Train RMSE 0.32868341204880536    Val RMSE 0.9643781185150146\n",
            "Epoch 113   Train RMSE 0.34874858056056435    Val RMSE 0.9835134744644165\n",
            "Epoch 114   Train RMSE 0.32899037415690296    Val RMSE 0.9657987356185913\n",
            "Epoch 115   Train RMSE 0.35782465102040734    Val RMSE 0.9929565787315369\n",
            "Epoch 116   Train RMSE 0.33411208717006424    Val RMSE 0.9712345600128174\n",
            "Epoch 117   Train RMSE 0.3776389489651828    Val RMSE 1.0130831003189087\n",
            "Epoch 118   Train RMSE 0.3513279238228013    Val RMSE 0.987785816192627\n",
            "Epoch 119   Train RMSE 0.418449998260117    Val RMSE 1.0561209917068481\n",
            "Epoch 120   Train RMSE 0.39581672943158963    Val RMSE 1.031231164932251\n",
            "Epoch 121   Train RMSE 0.4975721003652379    Val RMSE 1.1448822021484375\n",
            "Epoch 122   Train RMSE 0.49227504859843874    Val RMSE 1.1284152269363403\n",
            "Epoch 123   Train RMSE 0.6354709873014694    Val RMSE 1.3044747114181519\n",
            "Epoch 124   Train RMSE 0.661396171372276    Val RMSE 1.2914128303527832\n",
            "Epoch 125   Train RMSE 0.8331987669697106    Val RMSE 1.5041602849960327\n",
            "Epoch 126   Train RMSE 0.8764045205781862    Val RMSE 1.4310977458953857\n",
            "Epoch 127   Train RMSE 1.016152934263528    Val RMSE 1.5663655996322632\n",
            "Epoch 128   Train RMSE 0.9968282048832746    Val RMSE 1.340857982635498\n",
            "Epoch 129   Train RMSE 0.9952638048270069    Val RMSE 1.3146811723709106\n",
            "Epoch 130   Train RMSE 0.8315462301641672    Val RMSE 1.0470101833343506\n",
            "Epoch 131   Train RMSE 0.6773045960827836    Val RMSE 0.9962327480316162\n",
            "Epoch 132   Train RMSE 0.4826013022895692    Val RMSE 0.9323806762695312\n",
            "Epoch 133   Train RMSE 0.3597799176122122    Val RMSE 0.9331166744232178\n",
            "Epoch 134   Train RMSE 0.36431626574444514    Val RMSE 0.9877245426177979\n",
            "Epoch 135   Train RMSE 0.3519025006227896    Val RMSE 0.959229052066803\n",
            "Epoch 136   Train RMSE 0.42204501999921235    Val RMSE 1.0064027309417725\n",
            "Epoch 137   Train RMSE 0.3921376884409168    Val RMSE 0.9498717188835144\n",
            "Epoch 138   Train RMSE 0.41274709308993884    Val RMSE 0.9740157723426819\n",
            "Epoch 139   Train RMSE 0.36279524831966997    Val RMSE 0.9311805963516235\n",
            "Epoch 140   Train RMSE 0.35826079092657925    Val RMSE 0.9453660845756531\n",
            "Epoch 141   Train RMSE 0.3243960045388254    Val RMSE 0.9278032183647156\n",
            "Epoch 142   Train RMSE 0.3173334993052616    Val RMSE 0.933513879776001\n",
            "Epoch 143   Train RMSE 0.308830411918466    Val RMSE 0.9328793287277222\n",
            "Epoch 144   Train RMSE 0.30238509178904155    Val RMSE 0.9296737909317017\n",
            "Epoch 145   Train RMSE 0.3074875573610449    Val RMSE 0.9372937679290771\n",
            "Epoch 146   Train RMSE 0.2991918083591535    Val RMSE 0.9290028810501099\n",
            "Epoch 147   Train RMSE 0.30803955388687243    Val RMSE 0.9398837089538574\n",
            "Epoch 148   Train RMSE 0.2979157800705706    Val RMSE 0.9292387366294861\n",
            "Epoch 149   Train RMSE 0.3070919821274796    Val RMSE 0.9406209588050842\n",
            "Epoch 150   Train RMSE 0.296168201215454    Val RMSE 0.9290584325790405\n",
            "Epoch 151   Train RMSE 0.30522967053496497    Val RMSE 0.9404750466346741\n",
            "Epoch 152   Train RMSE 0.2939067954748386    Val RMSE 0.9287424087524414\n",
            "Epoch 153   Train RMSE 0.3033789141480662    Val RMSE 0.9407532811164856\n",
            "Epoch 154   Train RMSE 0.2916267430138526    Val RMSE 0.9283165335655212\n",
            "Epoch 155   Train RMSE 0.30229540347167383    Val RMSE 0.9415448904037476\n",
            "Epoch 156   Train RMSE 0.28960825975201815    Val RMSE 0.9275645017623901\n",
            "Epoch 157   Train RMSE 0.30258098531011784    Val RMSE 0.9433283805847168\n",
            "Epoch 158   Train RMSE 0.2881778376491448    Val RMSE 0.9269274473190308\n",
            "Epoch 159   Train RMSE 0.3051161074469686    Val RMSE 0.9472240209579468\n",
            "Epoch 160   Train RMSE 0.2882553895345556    Val RMSE 0.9269806742668152\n",
            "Epoch 161   Train RMSE 0.3117233878342794    Val RMSE 0.9547669887542725\n",
            "Epoch 162   Train RMSE 0.29186580308560167    Val RMSE 0.9291066527366638\n",
            "Epoch 163   Train RMSE 0.32602700996768047    Val RMSE 0.9696411490440369\n",
            "Epoch 164   Train RMSE 0.3038164550391977    Val RMSE 0.9372469782829285\n",
            "Epoch 165   Train RMSE 0.3550867396485757    Val RMSE 0.9996957182884216\n",
            "Epoch 166   Train RMSE 0.3344842950508356    Val RMSE 0.9607003331184387\n",
            "Epoch 167   Train RMSE 0.4114074876812298    Val RMSE 1.060084581375122\n",
            "Epoch 168   Train RMSE 0.4019698470393622    Val RMSE 1.0186638832092285\n",
            "Epoch 169   Train RMSE 0.5127196097293225    Val RMSE 1.1745039224624634\n",
            "Epoch 170   Train RMSE 0.5281519313017472    Val RMSE 1.1368389129638672\n",
            "Epoch 171   Train RMSE 0.6738378109443113    Val RMSE 1.3566359281539917\n",
            "Epoch 172   Train RMSE 0.720087424176102    Val RMSE 1.3103277683258057\n",
            "Epoch 173   Train RMSE 0.8814292806515199    Val RMSE 1.5511337518692017\n",
            "Epoch 174   Train RMSE 0.9313530948076536    Val RMSE 1.434813141822815\n",
            "Epoch 175   Train RMSE 1.0487472531415685    Val RMSE 1.5885597467422485\n",
            "Epoch 176   Train RMSE 1.025091509106944    Val RMSE 1.3384209871292114\n",
            "Epoch 177   Train RMSE 1.01354238752527    Val RMSE 1.3438756465911865\n",
            "Epoch 178   Train RMSE 0.8580677840793757    Val RMSE 1.0570842027664185\n",
            "Epoch 179   Train RMSE 0.7159516551010019    Val RMSE 1.0193760395050049\n",
            "Epoch 180   Train RMSE 0.5142711726045819    Val RMSE 0.9073017239570618\n",
            "Epoch 181   Train RMSE 0.3747524000374741    Val RMSE 0.9056659936904907\n",
            "Epoch 182   Train RMSE 0.31876868866380864    Val RMSE 0.9417126774787903\n",
            "Epoch 183   Train RMSE 0.29140382515151525    Val RMSE 0.9193933606147766\n",
            "Epoch 184   Train RMSE 0.35806154637635057    Val RMSE 0.9763563871383667\n",
            "Epoch 185   Train RMSE 0.34114107706185626    Val RMSE 0.9224336743354797\n",
            "Epoch 186   Train RMSE 0.3791956238990085    Val RMSE 0.9641289710998535\n",
            "Epoch 187   Train RMSE 0.3382501949125068    Val RMSE 0.910366415977478\n",
            "Epoch 188   Train RMSE 0.3459432813028654    Val RMSE 0.9397218227386475\n",
            "Epoch 189   Train RMSE 0.3051544670291189    Val RMSE 0.9041815400123596\n",
            "Epoch 190   Train RMSE 0.30282100545474644    Val RMSE 0.9223238825798035\n",
            "Epoch 191   Train RMSE 0.27868660364137315    Val RMSE 0.905631959438324\n",
            "Epoch 192   Train RMSE 0.2758491837685993    Val RMSE 0.9134968519210815\n",
            "Epoch 193   Train RMSE 0.2677993555971801    Val RMSE 0.9102014303207397\n",
            "Epoch 194   Train RMSE 0.26471980916697263    Val RMSE 0.9106693267822266\n",
            "Epoch 195   Train RMSE 0.2652980531369442    Val RMSE 0.9144608974456787\n",
            "Epoch 196   Train RMSE 0.2608648147541361    Val RMSE 0.9100255370140076\n",
            "Epoch 197   Train RMSE 0.2646718032488204    Val RMSE 0.9168113470077515\n",
            "Epoch 198   Train RMSE 0.2589251322470896    Val RMSE 0.909857988357544\n",
            "Epoch 199   Train RMSE 0.2637688635005923    Val RMSE 0.9180942177772522\n",
            "Test RMSE 0.9180942177772522\n",
            "Test MAE 0.6870036125183105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Custom models, e.g. message passing neural network\n",
        "\n",
        "We now need to transform our RDKit molecule objects to graphs with edge (bond) and node (atom) features. Here, we use a simply one-hot encoding of symbol, degree, hydrogen atoms, hybridization, and aromaticity (vector size = 29). For bond features, we simply use the bond type, and whether it is conjugated and in a ring (vector size = 6). In general, this initial featurization should be adapted for more complex problems."
      ],
      "metadata": {
        "id": "vXfyoAHxdanw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def atom_features(atom):\n",
        "    features = onek_encoding_unk(atom.GetSymbol(), ['C', 'N', 'O',  'P', 'S', 'Cl', 'Br', 'I']) + \\\n",
        "        onek_encoding_unk(atom.GetTotalDegree(), [0, 1, 2, 3, 4, 5]) + \\\n",
        "        onek_encoding_unk(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4]) + \\\n",
        "        onek_encoding_unk(int(atom.GetHybridization()),[Chem.rdchem.HybridizationType.SP,\n",
        "                                                        Chem.rdchem.HybridizationType.SP2,\n",
        "                                                        Chem.rdchem.HybridizationType.SP3,\n",
        "                                                        Chem.rdchem.HybridizationType.SP3D,\n",
        "                                                        Chem.rdchem.HybridizationType.SP3D2\n",
        "                                                        ]) + \\\n",
        "        [1 if atom.GetIsAromatic() else 0]\n",
        "    return features\n",
        "\n",
        "def bond_features(bond):\n",
        "    bt = bond.GetBondType()\n",
        "    fbond = [\n",
        "            bt == Chem.rdchem.BondType.SINGLE,\n",
        "            bt == Chem.rdchem.BondType.DOUBLE,\n",
        "            bt == Chem.rdchem.BondType.TRIPLE,\n",
        "            bt == Chem.rdchem.BondType.AROMATIC,\n",
        "            (bond.GetIsConjugated() if bt is not None else 0),\n",
        "            (bond.IsInRing() if bt is not None else 0)\n",
        "      ]\n",
        "    return fbond\n",
        "\n",
        "def onek_encoding_unk(value, choices):\n",
        "    encoding = [0] * (len(choices) + 1)\n",
        "    index = choices.index(value) if value in choices else -1\n",
        "    encoding[index] = 1\n",
        "    return encoding"
      ],
      "metadata": {
        "id": "Hfdi_aUBchS0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MolGraph:\n",
        "    def __init__(self, smiles):\n",
        "        self.smiles = smiles\n",
        "        self.f_atoms = []\n",
        "        self.f_bonds = []\n",
        "        self.edge_index = []\n",
        "\n",
        "        mol = Chem.MolFromSmiles(self.smiles)\n",
        "        n_atoms=mol.GetNumAtoms()\n",
        "\n",
        "        for a1 in range(n_atoms):\n",
        "            f_atom = atom_features(mol.GetAtomWithIdx(a1))\n",
        "            self.f_atoms.append(f_atom)\n",
        "\n",
        "            for a2 in range(a1 + 1, n_atoms):\n",
        "                bond = mol.GetBondBetweenAtoms(a1, a2)\n",
        "                if bond is None:\n",
        "                    continue\n",
        "                f_bond = bond_features(bond)\n",
        "                self.f_bonds.append(f_bond)\n",
        "                self.f_bonds.append(f_bond)\n",
        "                self.edge_index.extend([(a1, a2), (a2, a1)])"
      ],
      "metadata": {
        "id": "Xx8ekqiHdWfw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = MolGraph(\"CCO\")\n",
        "for i,f in enumerate(g.f_atoms):\n",
        "  print(i,f)\n",
        "for (i,j),f in zip(g.edge_index,g.f_bonds):\n",
        "  print(i,j,f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDvWq44fdpHL",
        "outputId": "0c9321a5-3da4-47b2-9aee-85fa191901b4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "1 [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "2 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "0 1 [True, False, False, False, False, False]\n",
            "1 0 [True, False, False, False, False, False]\n",
            "1 2 [True, False, False, False, False, False]\n",
            "2 1 [True, False, False, False, False, False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a dataset of molecules and their properties, we can now build a dataset object, and a loader which inherit from pytorch_geometric's Dataset and DataLoader. The ChemDataset class takes as input a list of SMILES and target values. Whenever we retrieve an element from this list via the get() function, we create a MolGraph from the respective SMILES strings. For this course, we do not implement any caching, but remake the graphs whenever we need them (we also don't hold them in memory). The molgraph2data function transforms our custom MolGraph into a format more convenient for pytorch_geometric's functionalities (and, importantly, into torch tensors). The construct_loader() functions takes a CSV file saved locally or from the internet, and uses the first column as list of SMILES, and the second as list of targets, for the sake of simplicity (for a real software package, don't hardcode this!)"
      ],
      "metadata": {
        "id": "Sr_8QCFLfEA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChemDataset(Dataset):\n",
        "    def __init__(self, smiles, labels):\n",
        "        super(ChemDataset, self).__init__()\n",
        "        self.smiles = smiles\n",
        "        self.labels = labels\n",
        "\n",
        "    def process_key(self, key):\n",
        "        smi = self.smiles[key]\n",
        "        mol = self.molgraph2data(MolGraph(smi), key)\n",
        "        return mol\n",
        "\n",
        "    def molgraph2data(self, molgraph, key):\n",
        "        data = tg.data.Data()\n",
        "        data.x = torch.tensor(molgraph.f_atoms, dtype=torch.float)\n",
        "        data.edge_index = torch.tensor(molgraph.edge_index, dtype=torch.long).t().contiguous()\n",
        "        data.edge_attr = torch.tensor(molgraph.f_bonds, dtype=torch.float)\n",
        "        data.y = torch.tensor([self.labels[key]], dtype=torch.float)\n",
        "        data.smiles = self.smiles[key]\n",
        "        return data\n",
        "\n",
        "    def get(self,key):\n",
        "        return self.process_key(key)\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.smiles)\n",
        "\n",
        "def construct_loader(data_path, shuffle=True, batch_size=50):\n",
        "    data_df = pd.read_csv(data_path)\n",
        "    smiles = data_df.iloc[:, 0].values\n",
        "    labels = data_df.iloc[:, 1].values.astype(np.float32)\n",
        "    dataset = ChemDataset(smiles, labels)\n",
        "    loader = DataLoader(dataset=dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=shuffle,\n",
        "                            num_workers=0,\n",
        "                            pin_memory=True,\n",
        "                            sampler=None)\n",
        "    return loader"
      ],
      "metadata": {
        "id": "LWEgiQ9mdpzy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect a dataset. For each batch, we get a list of SMILES strings, node features, edge features and connectivity lists. Note that the format is a bit unintuitive: SMILES, node features and labels are lists or lists of list of len(data), but the edge features and attributes are a single list for the full data chunk:"
      ],
      "metadata": {
        "id": "OPJtq7Jpfghj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = construct_loader(\"https://github.com/hesther/rxn_workshop/raw/main/data/esol/train_full.csv\")\n",
        "for data in loader:\n",
        "    print(data)\n",
        "    print(\"SMILES\",data.smiles[:3])\n",
        "    print(\"node features\",data.x[:3])\n",
        "    print(\"labels\",data.y[:3])\n",
        "    print(\"edges\",data.edge_index[:,:10])\n",
        "    print(\"edge features\",data.edge_attr[:10])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ayog4t8xe4f0",
        "outputId": "32073417-d8f3-4392-daa5-59825f6d3a31"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataBatch(x=[691, 29], edge_index=[2, 1430], edge_attr=[1430, 6], y=[50], smiles=[50], batch=[691], ptr=[51])\n",
            "SMILES ['Brc1ccccc1', 'CCCCCC1CCCC1', 'Cc1cccc(O)c1']\n",
            "node features tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.]])\n",
            "labels tensor([-2.5500, -6.0800, -0.6800])\n",
            "edges tensor([[0, 1, 1, 2, 1, 6, 2, 3, 3, 4],\n",
            "        [1, 0, 2, 1, 6, 1, 3, 2, 4, 3]])\n",
            "edge features tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now construct a D-MPNN (directed message-passing neural network) based on pytorch_geometric. We will follow the framework of Chemprop for the implementation in this workshop, but there are many flavors of D-MPNNs differing in the messages, update functions, etc. Based on the edge_index list we provide in our dataset, pytorch_geometric does all the heavy lifting determining which pairs of atoms should pass messages, and which sets of atoms to aggregate over to make molecular from atomic embeddings."
      ],
      "metadata": {
        "id": "1EWYQ6yMf5r7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, num_node_features, num_edge_features):\n",
        "        super(GNN, self).__init__()\n",
        "\n",
        "        self.depth = 3\n",
        "        self.hidden_size = 300\n",
        "        self.dropout = 0.02\n",
        "\n",
        "        self.edge_init = nn.Linear(num_node_features + num_edge_features, self.hidden_size)\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for _ in range(self.depth):\n",
        "            self.convs.append(DMPNNConv(self.hidden_size))\n",
        "        self.edge_to_node = nn.Linear(num_node_features + self.hidden_size, self.hidden_size)\n",
        "        self.pool = global_add_pool\n",
        "        self.ffn = nn.Linear(self.hidden_size, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "\n",
        "        # initial edge features\n",
        "        row, col = edge_index\n",
        "        h_0 = F.relu(self.edge_init(torch.cat([x[row], edge_attr], dim=1)))\n",
        "        h = h_0\n",
        "\n",
        "        # convolutions\n",
        "        for l in range(self.depth):\n",
        "            _, h = self.convs[l](edge_index, h)\n",
        "            h += h_0\n",
        "            h = F.dropout(F.relu(h), self.dropout, training=self.training)\n",
        "\n",
        "        # dmpnn edge -> node aggregation\n",
        "        s, _ = self.convs[l](edge_index, h) #only use for summing\n",
        "        q  = torch.cat([x,s], dim=1)\n",
        "        h = F.relu(self.edge_to_node(q))\n",
        "        return self.ffn(self.pool(h, batch)).squeeze(-1)\n",
        "\n",
        "class DMPNNConv(MessagePassing):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(DMPNNConv, self).__init__(aggr='add')\n",
        "        self.lin = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, edge_index, edge_attr):\n",
        "        row, col = edge_index\n",
        "        a_message = self.propagate(edge_index, x=None, edge_attr=edge_attr)\n",
        "        rev_message = torch.flip(edge_attr.view(edge_attr.size(0) // 2, 2, -1), dims=[1]).view(edge_attr.size(0), -1)\n",
        "\n",
        "        return a_message, self.lin(a_message[row] - rev_message)\n",
        "\n",
        "    def message(self, edge_attr):\n",
        "        return edge_attr"
      ],
      "metadata": {
        "id": "S3vR_danfsrE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "GNN(29,6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ldx7PYEf9WT",
        "outputId": "a8699b70-18fb-4ae2-e31c-1b817e38e747"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GNN(\n",
              "  (edge_init): Linear(in_features=35, out_features=300, bias=True)\n",
              "  (convs): ModuleList(\n",
              "    (0-2): 3 x DMPNNConv()\n",
              "  )\n",
              "  (edge_to_node): Linear(in_features=329, out_features=300, bias=True)\n",
              "  (ffn): Linear(in_features=300, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(folder):\n",
        "    torch.manual_seed(0)\n",
        "    train_loader = construct_loader(folder+\"/train_full.csv\", True)\n",
        "    val_loader = construct_loader(folder+\"/val_full.csv\", False)\n",
        "    test_loader = construct_loader(folder+\"/test_full.csv\", False)\n",
        "\n",
        "\n",
        "    model = GNN(train_loader.dataset.num_node_features, train_loader.dataset.num_edge_features)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss = nn.MSELoss(reduction='sum')\n",
        "    print(model)\n",
        "\n",
        "    for epoch in range(0, 100):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, loss)\n",
        "        preds = pred(model, val_loader, loss)\n",
        "        print(\"Epoch\",epoch,\"  Train RMSE\", train_loss,\"   Val RMSE\", root_mean_squared_error(preds,val_loader.dataset.labels))\n",
        "\n",
        "    preds = pred(model, test_loader, loss)\n",
        "    print(\"Test RMSE\", root_mean_squared_error(preds,test_loader.dataset.labels))\n",
        "    print(\"Test MAE\", mean_absolute_error(preds,test_loader.dataset.labels))"
      ],
      "metadata": {
        "id": "ywK2foz5f_9X"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(\"https://github.com/hesther/rxn_workshop/raw/main/data/esol\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBV6XpBpgv6i",
        "outputId": "5b072976-508c-48ac-9b25-033809f0da10"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GNN(\n",
            "  (edge_init): Linear(in_features=35, out_features=300, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-2): 3 x DMPNNConv()\n",
            "  )\n",
            "  (edge_to_node): Linear(in_features=329, out_features=300, bias=True)\n",
            "  (ffn): Linear(in_features=300, out_features=1, bias=True)\n",
            ")\n",
            "Epoch 0   Train RMSE 4.571772102663409    Val RMSE 1.6822712421417236\n",
            "Epoch 1   Train RMSE 1.629827079025758    Val RMSE 1.5634512901306152\n",
            "Epoch 2   Train RMSE 1.3604458504428998    Val RMSE 1.3745590448379517\n",
            "Epoch 3   Train RMSE 1.2057012742883018    Val RMSE 1.0492534637451172\n",
            "Epoch 4   Train RMSE 1.056592562952103    Val RMSE 0.9473162889480591\n",
            "Epoch 5   Train RMSE 0.989949142035968    Val RMSE 0.9265711903572083\n",
            "Epoch 6   Train RMSE 0.9677930227505485    Val RMSE 0.922477126121521\n",
            "Epoch 7   Train RMSE 0.9349645980721863    Val RMSE 0.8950542211532593\n",
            "Epoch 8   Train RMSE 0.9412390849378538    Val RMSE 0.8776540160179138\n",
            "Epoch 9   Train RMSE 0.9192325738972634    Val RMSE 0.8492763042449951\n",
            "Epoch 10   Train RMSE 0.9073925051326861    Val RMSE 0.8323759436607361\n",
            "Epoch 11   Train RMSE 0.8469123916535597    Val RMSE 0.7815366983413696\n",
            "Epoch 12   Train RMSE 0.8050444838867785    Val RMSE 0.7611867785453796\n",
            "Epoch 13   Train RMSE 0.7772673756254471    Val RMSE 0.8355435729026794\n",
            "Epoch 14   Train RMSE 0.7668259602033108    Val RMSE 0.9030914902687073\n",
            "Epoch 15   Train RMSE 0.7725445380226355    Val RMSE 0.702089786529541\n",
            "Epoch 16   Train RMSE 0.7297075354625242    Val RMSE 0.768517017364502\n",
            "Epoch 17   Train RMSE 0.7237680903333245    Val RMSE 0.6948432922363281\n",
            "Epoch 18   Train RMSE 0.699940655100078    Val RMSE 0.6741659641265869\n",
            "Epoch 19   Train RMSE 0.6772757085748519    Val RMSE 0.706518292427063\n",
            "Epoch 20   Train RMSE 0.6542028103297206    Val RMSE 0.6435015201568604\n",
            "Epoch 21   Train RMSE 0.641856057427061    Val RMSE 0.6618077158927917\n",
            "Epoch 22   Train RMSE 0.6640807878556614    Val RMSE 0.6418895125389099\n",
            "Epoch 23   Train RMSE 0.6496258383182697    Val RMSE 0.6357050538063049\n",
            "Epoch 24   Train RMSE 0.6428507774280084    Val RMSE 0.7018280029296875\n",
            "Epoch 25   Train RMSE 0.6180407505071901    Val RMSE 0.6409927606582642\n",
            "Epoch 26   Train RMSE 0.6376455851352157    Val RMSE 0.6960656046867371\n",
            "Epoch 27   Train RMSE 0.6983831303404714    Val RMSE 0.6945158839225769\n",
            "Epoch 28   Train RMSE 0.6331141188685347    Val RMSE 0.6393265128135681\n",
            "Epoch 29   Train RMSE 0.606087079720746    Val RMSE 0.5834678411483765\n",
            "Epoch 30   Train RMSE 0.6072326070902458    Val RMSE 0.6505529284477234\n",
            "Epoch 31   Train RMSE 0.5957421712116852    Val RMSE 0.5999157428741455\n",
            "Epoch 32   Train RMSE 0.5847672793072557    Val RMSE 0.7508915662765503\n",
            "Epoch 33   Train RMSE 0.6240118321158951    Val RMSE 0.6082234382629395\n",
            "Epoch 34   Train RMSE 0.5910175593231678    Val RMSE 0.6614285111427307\n",
            "Epoch 35   Train RMSE 0.6063562840766245    Val RMSE 0.6530007123947144\n",
            "Epoch 36   Train RMSE 0.5766572879522892    Val RMSE 0.5851893424987793\n",
            "Epoch 37   Train RMSE 0.5652219558753979    Val RMSE 0.5872732996940613\n",
            "Epoch 38   Train RMSE 0.5401662224998559    Val RMSE 0.5988971590995789\n",
            "Epoch 39   Train RMSE 0.5616508273042956    Val RMSE 0.6408308148384094\n",
            "Epoch 40   Train RMSE 0.5783278650617104    Val RMSE 0.7458906173706055\n",
            "Epoch 41   Train RMSE 0.6251570504280426    Val RMSE 0.5810800790786743\n",
            "Epoch 42   Train RMSE 0.5646788465874432    Val RMSE 0.5866377949714661\n",
            "Epoch 43   Train RMSE 0.5265084326631196    Val RMSE 0.6085857152938843\n",
            "Epoch 44   Train RMSE 0.5382733581141613    Val RMSE 0.6055282354354858\n",
            "Epoch 45   Train RMSE 0.5192530754927615    Val RMSE 0.6400114893913269\n",
            "Epoch 46   Train RMSE 0.5209007569223149    Val RMSE 0.666968047618866\n",
            "Epoch 47   Train RMSE 0.5345600175963813    Val RMSE 0.6077955961227417\n",
            "Epoch 48   Train RMSE 0.5240426906328898    Val RMSE 0.6018931865692139\n",
            "Epoch 49   Train RMSE 0.5327430388963836    Val RMSE 0.5876772403717041\n",
            "Epoch 50   Train RMSE 0.5406355090580713    Val RMSE 0.5948289036750793\n",
            "Epoch 51   Train RMSE 0.4983125314735241    Val RMSE 0.6174731850624084\n",
            "Epoch 52   Train RMSE 0.4869105057752872    Val RMSE 0.5546219944953918\n",
            "Epoch 53   Train RMSE 0.4683205385626791    Val RMSE 0.6747187972068787\n",
            "Epoch 54   Train RMSE 0.49568757377742284    Val RMSE 0.6246166229248047\n",
            "Epoch 55   Train RMSE 0.5060820218429436    Val RMSE 0.5854436159133911\n",
            "Epoch 56   Train RMSE 0.4944027593658349    Val RMSE 0.6322293877601624\n",
            "Epoch 57   Train RMSE 0.5198031982895905    Val RMSE 0.5912076830863953\n",
            "Epoch 58   Train RMSE 0.4850453537204793    Val RMSE 0.6185934543609619\n",
            "Epoch 59   Train RMSE 0.4804658346940582    Val RMSE 0.625504195690155\n",
            "Epoch 60   Train RMSE 0.5035509438923373    Val RMSE 0.5928422212600708\n",
            "Epoch 61   Train RMSE 0.4985422550484352    Val RMSE 0.5966632962226868\n",
            "Epoch 62   Train RMSE 0.468699993009264    Val RMSE 0.5725336074829102\n",
            "Epoch 63   Train RMSE 0.4608119373315511    Val RMSE 0.5637726187705994\n",
            "Epoch 64   Train RMSE 0.46732265596985006    Val RMSE 0.5760675072669983\n",
            "Epoch 65   Train RMSE 0.4401851406508109    Val RMSE 0.5767619013786316\n",
            "Epoch 66   Train RMSE 0.46129569990273744    Val RMSE 0.6535318493843079\n",
            "Epoch 67   Train RMSE 0.46303720205729065    Val RMSE 0.5663956999778748\n",
            "Epoch 68   Train RMSE 0.4299134267385579    Val RMSE 0.6080690622329712\n",
            "Epoch 69   Train RMSE 0.4536465241536057    Val RMSE 0.5726388096809387\n",
            "Epoch 70   Train RMSE 0.46180677075174376    Val RMSE 0.9088464975357056\n",
            "Epoch 71   Train RMSE 0.535580932958833    Val RMSE 0.6045717000961304\n",
            "Epoch 72   Train RMSE 0.48399256406049185    Val RMSE 0.6880162358283997\n",
            "Epoch 73   Train RMSE 0.5476273130425925    Val RMSE 0.5894676446914673\n",
            "Epoch 74   Train RMSE 0.42999433486001126    Val RMSE 0.5950164794921875\n",
            "Epoch 75   Train RMSE 0.4248388693200511    Val RMSE 0.5466184616088867\n",
            "Epoch 76   Train RMSE 0.40197018067233514    Val RMSE 0.5505954623222351\n",
            "Epoch 77   Train RMSE 0.39269218875405687    Val RMSE 0.5632539391517639\n",
            "Epoch 78   Train RMSE 0.397139172979215    Val RMSE 0.5318702459335327\n",
            "Epoch 79   Train RMSE 0.3973265099544945    Val RMSE 0.5761812329292297\n",
            "Epoch 80   Train RMSE 0.4225065837155282    Val RMSE 0.542470395565033\n",
            "Epoch 81   Train RMSE 0.40297085306270375    Val RMSE 0.5934869050979614\n",
            "Epoch 82   Train RMSE 0.4193081296203176    Val RMSE 0.561948299407959\n",
            "Epoch 83   Train RMSE 0.4096205164419822    Val RMSE 0.5366874933242798\n",
            "Epoch 84   Train RMSE 0.40326370691794183    Val RMSE 0.577279269695282\n",
            "Epoch 85   Train RMSE 0.3776265981742025    Val RMSE 0.6423396468162537\n",
            "Epoch 86   Train RMSE 0.41782057444060894    Val RMSE 0.5206736922264099\n",
            "Epoch 87   Train RMSE 0.38327333388491125    Val RMSE 0.5853496193885803\n",
            "Epoch 88   Train RMSE 0.39276623370726693    Val RMSE 0.5445764064788818\n",
            "Epoch 89   Train RMSE 0.3617215875304218    Val RMSE 0.5766616463661194\n",
            "Epoch 90   Train RMSE 0.37640289311477826    Val RMSE 0.5535158514976501\n",
            "Epoch 91   Train RMSE 0.3871039004280384    Val RMSE 0.5595583915710449\n",
            "Epoch 92   Train RMSE 0.35681866466724793    Val RMSE 0.5551543831825256\n",
            "Epoch 93   Train RMSE 0.34533318159905224    Val RMSE 0.588969886302948\n",
            "Epoch 94   Train RMSE 0.34355999290269107    Val RMSE 0.5608018040657043\n",
            "Epoch 95   Train RMSE 0.3470572658613326    Val RMSE 0.5763307809829712\n",
            "Epoch 96   Train RMSE 0.34153514105072186    Val RMSE 0.5856814980506897\n",
            "Epoch 97   Train RMSE 0.3474345089412592    Val RMSE 0.6172184348106384\n",
            "Epoch 98   Train RMSE 0.38894388849699424    Val RMSE 0.6549072265625\n",
            "Epoch 99   Train RMSE 0.3670129564061505    Val RMSE 0.5595762729644775\n",
            "Test RMSE 0.6628076434135437\n",
            "Test MAE 0.47406288981437683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wDFtAIzJW8Fv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}