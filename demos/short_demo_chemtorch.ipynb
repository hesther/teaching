{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZ0XXFSQZHrefnH4nm8yB1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hesther/teaching/blob/main/demos/short_demo_chemtorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChemTorch demo\n",
        "Connect to a T4 GPU for the best experience!"
      ],
      "metadata": {
        "id": "eg8cGG8_1ivW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit numpy==1.26.4 scikit-learn pandas\n",
        "!pip install torch==2.6.0\n",
        "!pip install hydra-core\n",
        "!pip install torch_geometric\n",
        "!pip install torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
        "!pip install wandb\n",
        "!pip install lightning\n",
        "!pip install ipykernel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_vEsedtZ82_",
        "outputId": "67cdffd5-479a-48b4-f86e-ae783214a43f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2025.3.3)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.2.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting hydra-core\n",
            "  Using cached hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.11/dist-packages (from hydra-core) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from hydra-core) (24.2)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.2)\n",
            "Using cached hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "Installing collected packages: hydra-core\n",
            "Successfully installed hydra-core-1.3.2\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.6.15)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_sparse-0.6.18%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_cluster-1.6.3%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-scatter, torch-sparse, torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt26cu124 torch-scatter-2.1.2+pt26cu124 torch-sparse-0.6.18+pt26cu124\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (24.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.30.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Collecting lightning\n",
            "  Downloading lightning-2.5.2-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (2025.3.2)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (24.2)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (2.6.0+cu124)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.7.3-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.14.0)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (3.10)\n",
            "Downloading lightning-2.5.2-py3-none-any.whl (821 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading torchmetrics-1.7.3-py3-none-any.whl (962 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.6/962.6 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.5.2 lightning-utilities-0.14.3 pytorch-lightning-2.5.2 torchmetrics-1.7.3\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (6.17.1)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel) (4.3.8)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel) (1.17.0)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/heid-lab/chemtorch.git\n",
        "%cd chemtorch\n",
        "!pip install ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQdXHH0CYp0l",
        "outputId": "d62d139b-afd2-42f8-fe34-948d50c74bc0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chemtorch'...\n",
            "remote: Enumerating objects: 346, done.\u001b[K\n",
            "remote: Counting objects: 100% (346/346), done.\u001b[K\n",
            "remote: Compressing objects: 100% (270/270), done.\u001b[K\n",
            "remote: Total 346 (delta 76), reused 325 (delta 61), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (346/346), 270.50 KiB | 10.02 MiB/s, done.\n",
            "Resolving deltas: 100% (76/76), done.\n",
            "/content/chemtorch\n",
            "Processing /content/chemtorch\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (from chemtorch==2025.6.23) (2025.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from chemtorch==2025.6.23) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from chemtorch==2025.6.23) (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from chemtorch==2025.6.23) (2.6.0+cu124)\n",
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.11/dist-packages (from chemtorch==2025.6.23) (2.1.2+pt26cu124)\n",
            "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.11/dist-packages (from chemtorch==2025.6.23) (0.6.18+pt26cu124)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (from chemtorch==2025.6.23) (2.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from chemtorch==2025.6.23) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->chemtorch==2025.6.23) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->chemtorch==2025.6.23) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->chemtorch==2025.6.23) (2025.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit->chemtorch==2025.6.23) (11.2.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->chemtorch==2025.6.23) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->chemtorch==2025.6.23) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->chemtorch==2025.6.23) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->chemtorch==2025.6.23) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->chemtorch==2025.6.23) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric->chemtorch==2025.6.23) (3.11.15)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric->chemtorch==2025.6.23) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric->chemtorch==2025.6.23) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric->chemtorch==2025.6.23) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric->chemtorch==2025.6.23) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->chemtorch==2025.6.23) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->chemtorch==2025.6.23) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->chemtorch==2025.6.23) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->chemtorch==2025.6.23) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->chemtorch==2025.6.23) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->chemtorch==2025.6.23) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->chemtorch==2025.6.23) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->chemtorch==2025.6.23) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->chemtorch==2025.6.23) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->chemtorch==2025.6.23) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->chemtorch==2025.6.23) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->chemtorch==2025.6.23) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->chemtorch==2025.6.23) (2025.6.15)\n",
            "Building wheels for collected packages: chemtorch\n",
            "  Building wheel for chemtorch (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chemtorch: filename=chemtorch-2025.6.23-py3-none-any.whl size=105831 sha256=846f3a00292182bc61a1743767f4cede353f556a8483648597f4e6e03f2b4d2e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h406z4r8/wheels/bb/49/2b/a719528ad2395fd98ddd8c902be8719ebca66ffd3e5d093b26\n",
            "Successfully built chemtorch\n",
            "Installing collected packages: chemtorch\n",
            "Successfully installed chemtorch-2025.6.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to download some datasets we can play around with:"
      ],
      "metadata": {
        "id": "PDZfMQLaaXyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/heid-lab/reaction_database.git\n",
        "!ln -s reaction_database/data data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqi412yrhQ73",
        "outputId": "d3bb31bf-09cb-43c2-a23e-515df98fe168"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'reaction_database'...\n",
            "remote: Enumerating objects: 37054, done.\u001b[K\n",
            "remote: Counting objects: 100% (37054/37054), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37002/37002), done.\u001b[K\n",
            "remote: Total 37054 (delta 35), reused 37038 (delta 25), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (37054/37054), 38.39 MiB | 15.72 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n",
            "Updating files: 100% (35801/35801), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "### Let's start with a GNN trained on CGRs to predict reaction barrier heights (on 5% of the RDB7 dataset):"
      ],
      "metadata": {
        "id": "KKd8nZQSo2vS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/main.py +experiment=graph dataset.subsample=0.05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2FmDlLJZvmk",
        "outputId": "3b51b4bb-64f1-48ac-b0aa-4d5fd77d34b0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "INFO: Data ingestor instantiated successfully\n",
            "INFO: Data ingestor finished successfully\n",
            "INFO: Data module factory instantiated successfully\n",
            "INFO: Precomputing 1014 items...\n",
            "INFO: Precomputation finished in 4.29s.\n",
            "INFO: Precomputing 60 items...\n",
            "INFO: Precomputation finished in 0.35s.\n",
            "INFO: Precomputing 119 items...\n",
            "INFO: Precomputation finished in 0.52s.\n",
            "INFO: Data modules instantiated successfully\n",
            "INFO: Dataloaders instantiated successfully\n",
            "INFO: Updating global config with properties of train dataset:\n",
            "INFO: Final config:\n",
            "data_ingestor:\n",
            "  data_source:\n",
            "    _target_: chemtorch.data_ingestor.data_source.SingleCSVSource\n",
            "    data_path: data/rdb7/barriers/forward_reverse_spiekermann_splits/data.csv\n",
            "  column_mapper:\n",
            "    _target_: chemtorch.data_ingestor.column_mapper.ColumnFilterAndRename\n",
            "    column_mapping:\n",
            "      smiles: rxn_smiles\n",
            "      label: ea\n",
            "  data_splitter:\n",
            "    _target_: chemtorch.data_ingestor.data_splitter.IndexSplitter\n",
            "    split_index_path: data/rdb7/barriers/forward_reverse_spiekermann_splits/seed0.pkl\n",
            "  _target_: chemtorch.data_ingestor.SimpleDataIngestor\n",
            "dataset:\n",
            "  representation:\n",
            "    atom_featurizer:\n",
            "      _target_: chemtorch.featurizer.FeaturizerCompose\n",
            "      featurizers:\n",
            "      - _target_: chemtorch.featurizer.OrganicAtomicNumberOneHotFeaturizer\n",
            "      - _target_: chemtorch.featurizer.AtomDegreeFeaturizer\n",
            "      - _target_: chemtorch.featurizer.AtomFormalChargeFeaturizer\n",
            "      - _target_: chemtorch.featurizer.AtomHCountFeaturizer\n",
            "      - _target_: chemtorch.featurizer.AtomHybridizationFeaturizer\n",
            "      - _target_: chemtorch.featurizer.AtomIsAromaticFeaturizer\n",
            "      - _target_: chemtorch.featurizer.CentiAtomMassFeaturizer\n",
            "      - _target_: chemtorch.featurizer.AtomIsInRingFeaturizer\n",
            "    bond_featurizer:\n",
            "      _target_: chemtorch.featurizer.FeaturizerCompose\n",
            "      featurizers:\n",
            "      - _target_: chemtorch.featurizer.BondTypeFeaturizer\n",
            "      - _target_: chemtorch.featurizer.BondIsConjugatedFeaturizer\n",
            "      - _target_: chemtorch.featurizer.BondInRingFeaturizer\n",
            "    _target_: chemtorch.representation.graph.cgr.CGR\n",
            "    in_channel_multiplier: 2\n",
            "  _target_: chemtorch.dataset.GraphDataset\n",
            "  _partial_: true\n",
            "  precompute_all: true\n",
            "  cache: true\n",
            "  max_size_cache: null\n",
            "  subsample: 0.05\n",
            "dataloader:\n",
            "  _target_: torch_geometric.loader.DataLoader\n",
            "  batch_size: 50\n",
            "  num_workers: 0\n",
            "  pin_memory: true\n",
            "  generator:\n",
            "    _target_: chemtorch.utils.get_generator\n",
            "    seed: 0\n",
            "model:\n",
            "  encoder:\n",
            "    _target_: chemtorch.encoder.directed_edge_enc.DirectedEdgeEncoder\n",
            "    in_channels: 110\n",
            "    out_channels: 128\n",
            "  layer_stack:\n",
            "    dmpnn_blocks:\n",
            "      layer:\n",
            "        graph_conv:\n",
            "          _target_: chemtorch.layer.gnn_layer.DMPNNConv\n",
            "          in_channels: 128\n",
            "          out_channels: 128\n",
            "          separate_nn: false\n",
            "        _target_: chemtorch.layer.gnn_layer.DMPNNBlock\n",
            "        residual: true\n",
            "        ffn: true\n",
            "        dropout: 0.1\n",
            "        act: relu\n",
            "        norm: null\n",
            "        hidden_channels: 128\n",
            "      _target_: chemtorch.layer.layer_stack.LayerStack\n",
            "      _recursive_: false\n",
            "      depth: 3\n",
            "    _target_: chemtorch.layer.gnn_layer.dmpnn_stack.DMPNNStack\n",
            "    edge_to_node_embedding:\n",
            "      _target_: chemtorch.layer.gnn_layer.dmpnn_stack.EdgeToNodeEmbedding\n",
            "      embedding_size: 128\n",
            "      num_node_features: 88\n",
            "  pool:\n",
            "    _target_: chemtorch.pool.pool.GlobalPool\n",
            "    aggr: add\n",
            "  head:\n",
            "    _target_: chemtorch.model.mlp.MLP\n",
            "    in_channels: 128\n",
            "    hidden_size: 128\n",
            "    out_channels: 1\n",
            "    num_hidden_layers: 1\n",
            "    dropout: 0.02\n",
            "    act: relu\n",
            "  _target_: chemtorch.model.gnn.GNN\n",
            "  hidden_channels: 128\n",
            "routine:\n",
            "  loss:\n",
            "    _target_: torch.nn.modules.loss.MSELoss\n",
            "    reduction: sum\n",
            "  optimizer:\n",
            "    _target_: torch.optim.AdamW\n",
            "    _partial_: true\n",
            "    weight_decay: 0.01\n",
            "    amsgrad: false\n",
            "    lr: 0.001\n",
            "    betas:\n",
            "    - 0.9\n",
            "    - 0.999\n",
            "    eps: 1.0e-08\n",
            "    foreach: null\n",
            "    maximize: false\n",
            "    capturable: false\n",
            "  lr_scheduler:\n",
            "    _target_: torch.optim.lr_scheduler.LambdaLR\n",
            "    _partial_: true\n",
            "    lr_lambda:\n",
            "      _target_: chemtorch.scheduler.graphgps_cosine_with_warmup_lr.get_cosine_scheduler_with_warmup\n",
            "      num_warmup_steps: 10\n",
            "      num_training_steps: 200\n",
            "      num_cycles: 0.5\n",
            "    num_warmup_steps: 10\n",
            "  _target_: chemtorch.routine.regression.train\n",
            "  _recursive_: false\n",
            "  epochs: 200\n",
            "  clip_grad_norm: true\n",
            "  clip_grad_norm_value: 1\n",
            "  patience: 30\n",
            "  min_delta: 0.01\n",
            "  save_model_parameters: false\n",
            "  model_path: null\n",
            "  use_wandb: false\n",
            "log: false\n",
            "project_name: graph\n",
            "group_name: null\n",
            "run_name: null\n",
            "seed: 0\n",
            "parameter_limit: null\n",
            "runtime_args_from_train_dataset_props:\n",
            "- num_node_features\n",
            "- num_edge_features\n",
            "- mean\n",
            "- std\n",
            "use_loaded_model: false\n",
            "pretrained_path: null\n",
            "use_cuda: true\n",
            "device: gpu\n",
            "num_node_features: 88\n",
            "num_edge_features: 22\n",
            "mean: 61.87636896449704\n",
            "std: 30.194722870764288\n",
            "\n",
            "Total parameters: 305,921\n",
            "GNN(\n",
            "  (encoder): DirectedEdgeEncoder(\n",
            "    (edge_init): Linear(in_features=110, out_features=128, bias=True)\n",
            "  )\n",
            "  (layer_stack): DMPNNStack(\n",
            "    (dmpnn_blocks): LayerStack(\n",
            "      (layers): ModuleList(\n",
            "        (0-2): 3 x DMPNNBlock(\n",
            "          (graph_conv): DMPNNConv()\n",
            "          (activation): ReLU()\n",
            "          (norm): Identity()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (ffn_norm_in): Identity()\n",
            "          (ffn_linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "          (ffn_linear2): Linear(in_features=256, out_features=128, bias=True)\n",
            "          (ffn_act_fn): ReLU()\n",
            "          (ffn_norm_out): Identity()\n",
            "          (ffn_dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (ffn_dropout2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (edge_to_node_embedding): EdgeToNodeEmbedding(\n",
            "      (linear): Linear(in_features=216, out_features=128, bias=True)\n",
            "      (activation): ReLU()\n",
            "      (aggregation): SumAggregation()\n",
            "    )\n",
            "  )\n",
            "  (pool): GlobalPool()\n",
            "  (head): MLP(\n",
            "    (activation): ReLU()\n",
            "    (layers): Sequential(\n",
            "      (0): Dropout(p=0.02, inplace=False)\n",
            "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (2): ReLU()\n",
            "      (3): Dropout(p=0.02, inplace=False)\n",
            "      (4): Linear(in_features=128, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Epoch 0, Train RMSE: 31.71382832043904, Val RMSE: 31.99644127745153, Time: 0.94\n",
            "Epoch 1, Train RMSE: 31.150997472868948, Val RMSE: 27.604926275795503, Time: 0.42\n",
            "Epoch 2, Train RMSE: 27.832625267783758, Val RMSE: 22.697803824432867, Time: 0.43\n",
            "Epoch 3, Train RMSE: 25.124551797541457, Val RMSE: 19.175621275568187, Time: 0.42\n",
            "Epoch 4, Train RMSE: 23.35462169447811, Val RMSE: 24.380530395169707, Time: 0.43\n",
            "Epoch 5, Train RMSE: 25.61508239863894, Val RMSE: 21.066801776914026, Time: 0.41\n",
            "Epoch 6, Train RMSE: 21.440576093534805, Val RMSE: 16.794772415237833, Time: 0.41\n",
            "Epoch 7, Train RMSE: 20.765054044774885, Val RMSE: 18.61614572596287, Time: 0.48\n",
            "Epoch 8, Train RMSE: 19.262963623963085, Val RMSE: 17.35898614324298, Time: 0.44\n",
            "Epoch 9, Train RMSE: 19.63219616397267, Val RMSE: 17.103172512106912, Time: 0.42\n",
            "Epoch 10, Train RMSE: 21.267547901674043, Val RMSE: 16.43159284780553, Time: 0.46\n",
            "Epoch 11, Train RMSE: 18.58609184467229, Val RMSE: 17.368224487921346, Time: 0.40\n",
            "Epoch 12, Train RMSE: 17.734293261100177, Val RMSE: 16.735603405197338, Time: 0.39\n",
            "Epoch 13, Train RMSE: 16.914207743314034, Val RMSE: 18.32242708092906, Time: 0.39\n",
            "Epoch 14, Train RMSE: 16.343571678341245, Val RMSE: 19.297283471069225, Time: 0.43\n",
            "Epoch 15, Train RMSE: 17.641902529305494, Val RMSE: 17.591998171623246, Time: 0.41\n",
            "Epoch 16, Train RMSE: 15.587551803931058, Val RMSE: 18.248466874540092, Time: 0.57\n",
            "Epoch 17, Train RMSE: 15.405280835229998, Val RMSE: 21.12155949472435, Time: 0.59\n",
            "Epoch 18, Train RMSE: 18.063905654647336, Val RMSE: 21.166462749302333, Time: 0.64\n",
            "Epoch 19, Train RMSE: 15.110903389444232, Val RMSE: 22.3256399263516, Time: 0.60\n",
            "Epoch 20, Train RMSE: 16.233075997717016, Val RMSE: 17.35527562043563, Time: 0.39\n",
            "Epoch 21, Train RMSE: 13.86302919607439, Val RMSE: 17.591883931725828, Time: 0.41\n",
            "Epoch 22, Train RMSE: 13.318531657899115, Val RMSE: 17.29101424102538, Time: 0.42\n",
            "Epoch 23, Train RMSE: 13.606853755229785, Val RMSE: 19.96868295898181, Time: 0.41\n",
            "Epoch 24, Train RMSE: 12.789013582298015, Val RMSE: 16.54922278179052, Time: 0.41\n",
            "Epoch 25, Train RMSE: 11.687451041215972, Val RMSE: 17.377753592331178, Time: 0.39\n",
            "Epoch 26, Train RMSE: 12.358366296938154, Val RMSE: 18.745165016652912, Time: 0.43\n",
            "Epoch 27, Train RMSE: 11.291599250019763, Val RMSE: 17.539715294165063, Time: 0.39\n",
            "Epoch 28, Train RMSE: 11.388081500786456, Val RMSE: 17.86241191959736, Time: 0.41\n",
            "Epoch 29, Train RMSE: 10.554761952688922, Val RMSE: 17.716316013044963, Time: 0.41\n",
            "Epoch 30, Train RMSE: 10.614441775550091, Val RMSE: 16.44584024483055, Time: 0.40\n",
            "Epoch 31, Train RMSE: 10.728089826055614, Val RMSE: 18.44124566805724, Time: 0.44\n",
            "Epoch 32, Train RMSE: 11.796660283641987, Val RMSE: 17.624507816671553, Time: 0.41\n",
            "Epoch 33, Train RMSE: 10.751891923361239, Val RMSE: 19.8235799029195, Time: 0.41\n",
            "Epoch 34, Train RMSE: 10.307319588388049, Val RMSE: 18.16679487733932, Time: 0.42\n",
            "Epoch 35, Train RMSE: 10.462071899260346, Val RMSE: 17.44690761305084, Time: 0.40\n",
            "Epoch 36, Train RMSE: 8.657089908767066, Val RMSE: 17.36002868966875, Time: 0.43\n",
            "Epoch 37, Train RMSE: 8.755006937996965, Val RMSE: 18.67339825487548, Time: 0.39\n",
            "Epoch 38, Train RMSE: 8.874739522200866, Val RMSE: 16.074099677152052, Time: 0.43\n",
            "Epoch 39, Train RMSE: 8.787087885823357, Val RMSE: 16.340455617992596, Time: 0.41\n",
            "Epoch 40, Train RMSE: 7.805562104946057, Val RMSE: 16.332067109779796, Time: 0.39\n",
            "Epoch 41, Train RMSE: 8.978711700124094, Val RMSE: 18.148188940836164, Time: 0.40\n",
            "Epoch 42, Train RMSE: 8.84519100243906, Val RMSE: 18.451817945991, Time: 0.48\n",
            "Epoch 43, Train RMSE: 8.214234478713708, Val RMSE: 16.872210451312167, Time: 0.64\n",
            "Epoch 44, Train RMSE: 7.3440647788255475, Val RMSE: 17.82242146930079, Time: 0.58\n",
            "Epoch 45, Train RMSE: 8.091576950984953, Val RMSE: 16.588737725292358, Time: 0.64\n",
            "Epoch 46, Train RMSE: 7.400733406220708, Val RMSE: 17.171999144543406, Time: 0.44\n",
            "Epoch 47, Train RMSE: 6.66462202507093, Val RMSE: 17.64501804297806, Time: 0.41\n",
            "Epoch 48, Train RMSE: 6.535770585757903, Val RMSE: 17.37043169106275, Time: 0.41\n",
            "Epoch 49, Train RMSE: 6.496003903279752, Val RMSE: 16.566258329291735, Time: 0.42\n",
            "Epoch 50, Train RMSE: 6.0488826760334815, Val RMSE: 16.31720590676624, Time: 0.42\n",
            "Epoch 51, Train RMSE: 6.70704632214745, Val RMSE: 17.48516195368731, Time: 0.41\n",
            "Epoch 52, Train RMSE: 6.500648759597329, Val RMSE: 17.0898202059323, Time: 0.42\n",
            "Epoch 53, Train RMSE: 5.751508100356531, Val RMSE: 18.44426929040724, Time: 0.41\n",
            "Epoch 54, Train RMSE: 5.9762510131247835, Val RMSE: 17.553138732917752, Time: 0.42\n",
            "Epoch 55, Train RMSE: 5.961038611118216, Val RMSE: 17.43229275813334, Time: 0.39\n",
            "Epoch 56, Train RMSE: 5.4594186572560375, Val RMSE: 17.538601844785784, Time: 0.40\n",
            "Epoch 57, Train RMSE: 5.987887554328426, Val RMSE: 15.919491647094919, Time: 0.39\n",
            "Epoch 58, Train RMSE: 5.646102485950848, Val RMSE: 16.85702245056209, Time: 0.41\n",
            "Epoch 59, Train RMSE: 5.483999907879651, Val RMSE: 16.861492951622544, Time: 0.39\n",
            "Epoch 60, Train RMSE: 5.501631408103851, Val RMSE: 16.983224866592955, Time: 0.40\n",
            "Epoch 61, Train RMSE: 5.710393083874637, Val RMSE: 16.254767023297074, Time: 0.40\n",
            "Epoch 62, Train RMSE: 5.6129033884650275, Val RMSE: 18.071388042727488, Time: 0.39\n",
            "Epoch 63, Train RMSE: 5.6118179144708344, Val RMSE: 16.546651931190567, Time: 0.40\n",
            "Epoch 64, Train RMSE: 5.500641005182533, Val RMSE: 16.412132603181114, Time: 0.41\n",
            "Epoch 65, Train RMSE: 6.123416306638338, Val RMSE: 17.111147332009814, Time: 0.40\n",
            "Epoch 66, Train RMSE: 4.449585696018331, Val RMSE: 17.177942109885727, Time: 0.42\n",
            "Epoch 67, Train RMSE: 4.8319114479009695, Val RMSE: 17.661257846853623, Time: 0.40\n",
            "Epoch 68, Train RMSE: 5.045975643369817, Val RMSE: 17.611501758948528, Time: 0.39\n",
            "Epoch 69, Train RMSE: 4.736162408505563, Val RMSE: 16.692083039256705, Time: 0.51\n",
            "Epoch 70, Train RMSE: 5.006159421789529, Val RMSE: 16.70585081322512, Time: 0.60\n",
            "Epoch 71, Train RMSE: 4.314087727496072, Val RMSE: 15.678838646636637, Time: 0.59\n",
            "Epoch 72, Train RMSE: 4.458330291081516, Val RMSE: 16.565312490124587, Time: 0.60\n",
            "Epoch 73, Train RMSE: 4.9702908028276225, Val RMSE: 16.702029222899043, Time: 0.40\n",
            "Epoch 74, Train RMSE: 4.399951951891643, Val RMSE: 15.994549823166201, Time: 0.42\n",
            "Epoch 75, Train RMSE: 4.42073394826667, Val RMSE: 16.646693251880805, Time: 0.40\n",
            "Epoch 76, Train RMSE: 4.318884284088957, Val RMSE: 16.169568680103563, Time: 0.43\n",
            "Epoch 77, Train RMSE: 3.928034227971819, Val RMSE: 16.030430220034802, Time: 0.41\n",
            "Epoch 78, Train RMSE: 4.088609336082452, Val RMSE: 16.2487790432437, Time: 0.39\n",
            "Epoch 79, Train RMSE: 4.4591772879042315, Val RMSE: 16.165522671767377, Time: 0.41\n",
            "Epoch 80, Train RMSE: 4.2870679988037494, Val RMSE: 16.09455504287674, Time: 0.39\n",
            "Epoch 81, Train RMSE: 3.9681094396096537, Val RMSE: 16.405975906122933, Time: 0.41\n",
            "Epoch 82, Train RMSE: 4.482493574113383, Val RMSE: 17.323348146731718, Time: 0.40\n",
            "Epoch 83, Train RMSE: 3.891248415595252, Val RMSE: 16.44819740784265, Time: 0.40\n",
            "Epoch 84, Train RMSE: 4.141516708227947, Val RMSE: 16.38592191564513, Time: 0.40\n",
            "Epoch 85, Train RMSE: 4.048294113134621, Val RMSE: 16.227662390254576, Time: 0.39\n",
            "Epoch 86, Train RMSE: 4.131650476479108, Val RMSE: 16.398343334061224, Time: 0.41\n",
            "Epoch 87, Train RMSE: 3.90173029540686, Val RMSE: 16.929091863195186, Time: 0.39\n",
            "Epoch 88, Train RMSE: 3.715175545653526, Val RMSE: 15.910310153643323, Time: 0.41\n",
            "Epoch 89, Train RMSE: 3.918588423858227, Val RMSE: 17.053087372239872, Time: 0.41\n",
            "Epoch 90, Train RMSE: 3.4759671848095812, Val RMSE: 16.575665323433746, Time: 0.39\n",
            "Epoch 91, Train RMSE: 3.6011822348736438, Val RMSE: 16.72045881452853, Time: 0.40\n",
            "Epoch 92, Train RMSE: 3.733187817961974, Val RMSE: 16.571554290964944, Time: 0.38\n",
            "Epoch 93, Train RMSE: 3.8116880709224104, Val RMSE: 15.932551445706, Time: 0.40\n",
            "Epoch 94, Train RMSE: 3.549930013047325, Val RMSE: 16.524498930501085, Time: 0.39\n",
            "Epoch 95, Train RMSE: 3.531286188801015, Val RMSE: 15.383410300436802, Time: 0.41\n",
            "Epoch 96, Train RMSE: 3.4980740016309304, Val RMSE: 16.23005922082942, Time: 0.54\n",
            "Epoch 97, Train RMSE: 3.6956759832546964, Val RMSE: 16.26816206872787, Time: 0.59\n",
            "Epoch 98, Train RMSE: 3.6503464730308743, Val RMSE: 16.17240376652788, Time: 0.59\n",
            "Epoch 99, Train RMSE: 3.275852377820089, Val RMSE: 16.4621929399105, Time: 0.61\n",
            "Epoch 100, Train RMSE: 3.4548336997081583, Val RMSE: 16.24194705007499, Time: 0.39\n",
            "Epoch 101, Train RMSE: 3.136029121341564, Val RMSE: 16.28505750206874, Time: 0.41\n",
            "Epoch 102, Train RMSE: 3.285528100703553, Val RMSE: 16.53441173623646, Time: 0.43\n",
            "Epoch 103, Train RMSE: 3.4734450175023377, Val RMSE: 16.88205449530853, Time: 0.42\n",
            "Epoch 104, Train RMSE: 3.394203372129029, Val RMSE: 16.440180103972818, Time: 0.41\n",
            "Epoch 105, Train RMSE: 3.19059931850279, Val RMSE: 16.09260595273429, Time: 0.42\n",
            "Epoch 106, Train RMSE: 3.6243480227604925, Val RMSE: 16.182404055379333, Time: 0.41\n",
            "Epoch 107, Train RMSE: 3.700839983470233, Val RMSE: 16.696678948621138, Time: 0.39\n",
            "Epoch 108, Train RMSE: 3.286359024764099, Val RMSE: 15.651764163814766, Time: 0.39\n",
            "Epoch 109, Train RMSE: 3.3254934303311714, Val RMSE: 15.628730737940314, Time: 0.41\n",
            "Epoch 110, Train RMSE: 3.3056173256481896, Val RMSE: 16.075710462107146, Time: 0.40\n",
            "Epoch 111, Train RMSE: 3.4062791175647678, Val RMSE: 16.126527455433024, Time: 0.41\n",
            "Epoch 112, Train RMSE: 3.3175652122268717, Val RMSE: 16.25564877914239, Time: 0.40\n",
            "Epoch 113, Train RMSE: 3.2464881409499426, Val RMSE: 16.109579157294494, Time: 0.40\n",
            "Epoch 114, Train RMSE: 2.894266175605047, Val RMSE: 15.955840735838374, Time: 0.40\n",
            "Epoch 115, Train RMSE: 2.8079105655143297, Val RMSE: 16.16121310249981, Time: 0.40\n",
            "Epoch 116, Train RMSE: 3.1634118435452447, Val RMSE: 16.421355349484262, Time: 0.40\n",
            "Epoch 117, Train RMSE: 2.9984940881841005, Val RMSE: 15.931001262129241, Time: 0.40\n",
            "Epoch 118, Train RMSE: 2.9236524754666458, Val RMSE: 15.986767523474414, Time: 0.41\n",
            "Epoch 119, Train RMSE: 2.948948404413772, Val RMSE: 16.202077820250203, Time: 0.42\n",
            "Epoch 120, Train RMSE: 2.9878428817048586, Val RMSE: 15.80856077273544, Time: 0.40\n",
            "Epoch 121, Train RMSE: 2.812481943708315, Val RMSE: 16.216181824035267, Time: 0.40\n",
            "Epoch 122, Train RMSE: 2.9603454481055316, Val RMSE: 15.980440599839739, Time: 0.42\n",
            "Epoch 123, Train RMSE: 2.720864011193361, Val RMSE: 15.966082369655558, Time: 0.55\n",
            "Epoch 124, Train RMSE: 2.7310102414274717, Val RMSE: 15.888813697731742, Time: 0.56\n",
            "Epoch 125, Train RMSE: 2.8529413939245174, Val RMSE: 16.23279607372652, Time: 0.60\n",
            "Early stopping at epoch 125\n",
            "Test RMSE: 13.474808245007875\n",
            "Test MAE: 9.366542443878433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Language models\n",
        "\n",
        "ChemTorch can deal with many different data modalities, and tasks. For example, to train a language model on the task of reaction classification using SMILES strings as representation (on 5% of the USPTO-1K dataset), run:"
      ],
      "metadata": {
        "id": "tgjF2eTprByv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/main.py +experiment=token dataset.subsample=0.05 routine.epochs=2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPCVcLLBaMaC",
        "outputId": "c72a86ea-a88e-451b-b514-7c03c2312780"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "INFO: Data ingestor instantiated successfully\n",
            "INFO: Data ingestor finished successfully\n",
            "INFO: Data module factory instantiated successfully\n",
            "INFO: Precomputing 20030 items...\n",
            "INFO: Precomputation finished in 9.51s.\n",
            "INFO: Precomputing 2226 items...\n",
            "INFO: Precomputation finished in 1.03s.\n",
            "INFO: Precomputing 2226 items...\n",
            "INFO: Precomputation finished in 1.21s.\n",
            "INFO: Data modules instantiated successfully\n",
            "INFO: Dataloaders instantiated successfully\n",
            "INFO: Updating global config with properties of train dataset:\n",
            "INFO: Final config:\n",
            "data_ingestor:\n",
            "  data_source:\n",
            "    _target_: chemtorch.data_ingestor.data_source.PreSplitCSVSource\n",
            "    data_folder: data/uspto-1k/classes/pre_split\n",
            "  column_mapper:\n",
            "    _target_: chemtorch.data_ingestor.column_mapper.ColumnFilterAndRename\n",
            "    column_mapping:\n",
            "      smiles: reaction\n",
            "      label: labels\n",
            "  _target_: chemtorch.data_ingestor.SimpleDataIngestor\n",
            "dataset:\n",
            "  representation:\n",
            "    tokenizer:\n",
            "      _target_: chemtorch.tokenizer.simple_tokenizer.SimpleTokenizer\n",
            "    _target_: chemtorch.representation.token.simple_token_representation.SimpleTokenRepresentation\n",
            "    vocab_path: resources/uspto-1k/vocab.txt\n",
            "    max_sentence_length: 400\n",
            "    pad_token: '[PAD]'\n",
            "    unk_token: '[UNK]'\n",
            "  _target_: chemtorch.dataset.token_dataset.TokenDataset\n",
            "  _partial_: true\n",
            "  precompute_all: true\n",
            "  cache: true\n",
            "  max_size_cache: null\n",
            "  subsample: 0.05\n",
            "dataloader:\n",
            "  _target_: torch.utils.data.DataLoader\n",
            "  batch_size: 50\n",
            "  num_workers: 0\n",
            "  pin_memory: true\n",
            "model:\n",
            "  _target_: chemtorch.model.han.HAN\n",
            "  embedding_in_channels: 599\n",
            "  embedding_hidden_channels: 200\n",
            "  gru_hidden_channels: 200\n",
            "  class_num: 1000\n",
            "  dropout: 0.2\n",
            "routine:\n",
            "  loss:\n",
            "    _target_: torch.nn.CrossEntropyLoss\n",
            "  optimizer:\n",
            "    _target_: torch.optim.AdamW\n",
            "    _partial_: true\n",
            "    weight_decay: 0.01\n",
            "    amsgrad: false\n",
            "    lr: 0.001\n",
            "    betas:\n",
            "    - 0.9\n",
            "    - 0.999\n",
            "    eps: 1.0e-08\n",
            "    foreach: null\n",
            "    maximize: false\n",
            "    capturable: false\n",
            "  lr_scheduler:\n",
            "    _target_: torch.optim.lr_scheduler.LambdaLR\n",
            "    _partial_: true\n",
            "    lr_lambda:\n",
            "      _target_: chemtorch.scheduler.graphgps_cosine_with_warmup_lr.get_cosine_scheduler_with_warmup\n",
            "      num_warmup_steps: 10\n",
            "      num_training_steps: 2\n",
            "      num_cycles: 0.5\n",
            "    num_warmup_steps: 10\n",
            "  _target_: chemtorch.routine.classification.train\n",
            "  _recursive_: false\n",
            "  epochs: 2\n",
            "  clip_grad_norm: true\n",
            "  clip_grad_norm_value: 1\n",
            "  patience: 30\n",
            "  min_delta: 0.01\n",
            "  save_model_parameters: false\n",
            "  model_path: null\n",
            "  use_wandb: false\n",
            "log: false\n",
            "project_name: token\n",
            "group_name: null\n",
            "run_name: null\n",
            "seed: 0\n",
            "parameter_limit: null\n",
            "runtime_args_from_train_dataset_props:\n",
            "- vocab_size\n",
            "use_loaded_model: false\n",
            "pretrained_path: null\n",
            "use_cuda: true\n",
            "device: gpu\n",
            "vocab_size: 599\n",
            "\n",
            "Total parameters: 3,492,000\n",
            "HAN(\n",
            "  (embedding): Embedding(599, 200)\n",
            "  (word_gru): GRU(200, 200, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (word_dense): Linear(in_features=400, out_features=400, bias=True)\n",
            "  (sentence_gru): GRU(400, 200, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (sentence_dense): Linear(in_features=400, out_features=400, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=True)\n",
            "  (fc): Linear(in_features=400, out_features=1000, bias=True)\n",
            ")\n",
            "Epoch 0, Train Loss: 6.9063, Val Loss: 6.9057, Val Acc: 0.0004, Time: 39.64\n",
            "Epoch 1, Train Loss: 6.3054, Val Loss: 6.2371, Val Acc: 0.0247, Time: 41.05\n",
            "Test BCE Loss: 6.1768\n",
            "Test Accuracy: 0.0346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How about an MLP on a reaction fingerprint for reaction classification?"
      ],
      "metadata": {
        "id": "JOCafzYZrZiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/main.py +experiment=fingerprint dataset.subsample=0.001"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc65XZPkcGg_",
        "outputId": "44107da0-a8b7-4fec-ff51-6f4eab001fc1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "INFO: Data ingestor instantiated successfully\n",
            "INFO: Data ingestor finished successfully\n",
            "INFO: Data module factory instantiated successfully\n",
            "INFO: Precomputing 401 items...\n",
            "INFO: Precomputation finished in 4.79s.\n",
            "INFO: Precomputing 45 items...\n",
            "INFO: Precomputation finished in 0.59s.\n",
            "INFO: Precomputing 45 items...\n",
            "INFO: Precomputation finished in 0.77s.\n",
            "INFO: Data modules instantiated successfully\n",
            "INFO: Dataloaders instantiated successfully\n",
            "INFO: Updating global config with properties of train dataset:\n",
            "INFO: Final config:\n",
            "data_ingestor:\n",
            "  data_source:\n",
            "    _target_: chemtorch.data_ingestor.data_source.PreSplitCSVSource\n",
            "    data_folder: data/uspto-1k/classes/pre_split\n",
            "  column_mapper:\n",
            "    _target_: chemtorch.data_ingestor.column_mapper.ColumnFilterAndRename\n",
            "    column_mapping:\n",
            "      smiles: reaction\n",
            "      label: labels\n",
            "  _target_: chemtorch.data_ingestor.SimpleDataIngestor\n",
            "dataset:\n",
            "  representation:\n",
            "    _target_: chemtorch.representation.fingerprint.drfp.DRFP\n",
            "    _recursive_: false\n",
            "    n_folded_length: 2048\n",
            "    min_radius: 0\n",
            "    radius: 3\n",
            "    rings: true\n",
            "    root_central_atom: true\n",
            "    include_hydrogens: false\n",
            "  _target_: chemtorch.dataset.FingerprintDataset\n",
            "  _partial_: true\n",
            "  precompute_all: true\n",
            "  cache: true\n",
            "  max_size_cache: null\n",
            "  subsample: 0.001\n",
            "dataloader:\n",
            "  _target_: torch.utils.data.DataLoader\n",
            "  batch_size: 50\n",
            "  num_workers: 0\n",
            "  pin_memory: true\n",
            "model:\n",
            "  _target_: chemtorch.model.mlp.MLP\n",
            "  in_channels: 2048\n",
            "  hidden_size: 256\n",
            "  out_channels: 1000\n",
            "  num_hidden_layers: 2\n",
            "  dropout: 0.02\n",
            "  act: relu\n",
            "routine:\n",
            "  loss:\n",
            "    _target_: torch.nn.CrossEntropyLoss\n",
            "  optimizer:\n",
            "    _target_: torch.optim.AdamW\n",
            "    _partial_: true\n",
            "    weight_decay: 0.01\n",
            "    amsgrad: false\n",
            "    lr: 0.001\n",
            "    betas:\n",
            "    - 0.9\n",
            "    - 0.999\n",
            "    eps: 1.0e-08\n",
            "    foreach: null\n",
            "    maximize: false\n",
            "    capturable: false\n",
            "  lr_scheduler:\n",
            "    _target_: torch.optim.lr_scheduler.LambdaLR\n",
            "    _partial_: true\n",
            "    lr_lambda:\n",
            "      _target_: chemtorch.scheduler.graphgps_cosine_with_warmup_lr.get_cosine_scheduler_with_warmup\n",
            "      num_warmup_steps: 10\n",
            "      num_training_steps: 200\n",
            "      num_cycles: 0.5\n",
            "    num_warmup_steps: 10\n",
            "  _target_: chemtorch.routine.classification.train\n",
            "  _recursive_: false\n",
            "  epochs: 200\n",
            "  clip_grad_norm: true\n",
            "  clip_grad_norm_value: 1\n",
            "  patience: 30\n",
            "  min_delta: 0.01\n",
            "  save_model_parameters: false\n",
            "  model_path: null\n",
            "  use_wandb: false\n",
            "log: false\n",
            "project_name: fingerprint\n",
            "group_name: null\n",
            "run_name: null\n",
            "seed: 0\n",
            "parameter_limit: null\n",
            "runtime_args_from_train_dataset_props:\n",
            "- fp_length\n",
            "use_loaded_model: false\n",
            "pretrained_path: null\n",
            "use_cuda: true\n",
            "device: gpu\n",
            "fp_length: 2048\n",
            "\n",
            "Total parameters: 847,336\n",
            "MLP(\n",
            "  (activation): ReLU()\n",
            "  (layers): Sequential(\n",
            "    (0): Dropout(p=0.02, inplace=False)\n",
            "    (1): Linear(in_features=2048, out_features=256, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.02, inplace=False)\n",
            "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Dropout(p=0.02, inplace=False)\n",
            "    (7): Linear(in_features=256, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch 0, Train Loss: 6.9055, Val Loss: 6.9135, Val Acc: 0.0000, Time: 0.23\n",
            "Epoch 1, Train Loss: 6.9005, Val Loss: 6.9105, Val Acc: 0.0000, Time: 0.04\n",
            "Epoch 2, Train Loss: 6.8801, Val Loss: 6.9035, Val Acc: 0.0000, Time: 0.04\n",
            "Epoch 3, Train Loss: 6.8389, Val Loss: 6.8878, Val Acc: 0.0222, Time: 0.04\n",
            "Epoch 4, Train Loss: 6.7398, Val Loss: 6.8341, Val Acc: 0.0444, Time: 0.04\n",
            "Epoch 5, Train Loss: 6.3856, Val Loss: 6.6710, Val Acc: 0.0222, Time: 0.04\n",
            "Epoch 6, Train Loss: 5.4615, Val Loss: 7.7841, Val Acc: 0.0222, Time: 0.03\n",
            "Epoch 7, Train Loss: 5.0065, Val Loss: 8.6874, Val Acc: 0.0667, Time: 0.03\n",
            "Epoch 8, Train Loss: 4.6564, Val Loss: 8.5236, Val Acc: 0.0444, Time: 0.05\n",
            "Epoch 9, Train Loss: 4.3448, Val Loss: 8.4094, Val Acc: 0.0444, Time: 0.03\n",
            "Epoch 10, Train Loss: 3.9250, Val Loss: 8.5294, Val Acc: 0.0889, Time: 0.04\n",
            "Epoch 11, Train Loss: 3.4276, Val Loss: 8.7403, Val Acc: 0.0889, Time: 0.04\n",
            "Epoch 12, Train Loss: 2.8916, Val Loss: 9.0064, Val Acc: 0.1556, Time: 0.03\n",
            "Epoch 13, Train Loss: 2.3358, Val Loss: 9.5238, Val Acc: 0.1556, Time: 0.04\n",
            "Epoch 14, Train Loss: 1.7910, Val Loss: 10.0816, Val Acc: 0.1778, Time: 0.03\n",
            "Epoch 15, Train Loss: 1.3138, Val Loss: 10.7834, Val Acc: 0.1778, Time: 0.03\n",
            "Epoch 16, Train Loss: 0.8874, Val Loss: 11.5836, Val Acc: 0.2000, Time: 0.04\n",
            "Epoch 17, Train Loss: 0.5718, Val Loss: 12.3058, Val Acc: 0.2222, Time: 0.03\n",
            "Epoch 18, Train Loss: 0.3469, Val Loss: 12.9262, Val Acc: 0.2000, Time: 0.03\n",
            "Epoch 19, Train Loss: 0.2126, Val Loss: 13.3332, Val Acc: 0.2444, Time: 0.03\n",
            "Epoch 20, Train Loss: 0.1292, Val Loss: 13.7757, Val Acc: 0.2222, Time: 0.03\n",
            "Epoch 21, Train Loss: 0.0869, Val Loss: 14.1286, Val Acc: 0.2222, Time: 0.03\n",
            "Epoch 22, Train Loss: 0.0602, Val Loss: 14.2457, Val Acc: 0.2222, Time: 0.04\n",
            "Epoch 23, Train Loss: 0.0442, Val Loss: 14.3413, Val Acc: 0.2222, Time: 0.03\n",
            "Epoch 24, Train Loss: 0.0310, Val Loss: 14.4475, Val Acc: 0.2222, Time: 0.03\n",
            "Epoch 25, Train Loss: 0.0240, Val Loss: 14.6511, Val Acc: 0.2222, Time: 0.03\n",
            "Epoch 26, Train Loss: 0.0245, Val Loss: 14.8205, Val Acc: 0.2000, Time: 0.03\n",
            "Epoch 27, Train Loss: 0.0169, Val Loss: 14.9406, Val Acc: 0.2222, Time: 0.04\n",
            "Epoch 28, Train Loss: 0.0168, Val Loss: 15.0025, Val Acc: 0.2000, Time: 0.04\n",
            "Epoch 29, Train Loss: 0.0177, Val Loss: 14.9710, Val Acc: 0.2222, Time: 0.03\n",
            "Epoch 30, Train Loss: 0.0120, Val Loss: 15.0257, Val Acc: 0.2222, Time: 0.03\n",
            "Epoch 31, Train Loss: 0.0101, Val Loss: 15.1509, Val Acc: 0.2222, Time: 0.03\n",
            "Epoch 32, Train Loss: 0.0121, Val Loss: 15.2164, Val Acc: 0.2222, Time: 0.04\n",
            "Epoch 33, Train Loss: 0.0078, Val Loss: 15.2010, Val Acc: 0.2222, Time: 0.04\n",
            "Epoch 34, Train Loss: 0.0071, Val Loss: 15.2887, Val Acc: 0.2222, Time: 0.04\n",
            "Epoch 35, Train Loss: 0.0073, Val Loss: 15.3628, Val Acc: 0.2222, Time: 0.03\n",
            "Early stopping at epoch 35\n",
            "Test BCE Loss: 14.9184\n",
            "Test Accuracy: 0.3111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make a barrier height prediction model on our own custom data."
      ],
      "metadata": {
        "id": "-DRvpUbqstjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/hesther/rxn_workshop/refs/heads/main/data/e2sn2/train_full.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HtVTCszcOZG",
        "outputId": "f0cdf525-6420-4815-a000-a55a4e71dd48"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-24 11:30:14--  https://raw.githubusercontent.com/hesther/rxn_workshop/refs/heads/main/data/e2sn2/train_full.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 522030 (510K) [text/plain]\n",
            "Saving to: ‘train_full.csv’\n",
            "\n",
            "train_full.csv      100%[===================>] 509.79K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2025-06-24 11:30:14 (141 MB/s) - ‘train_full.csv’ saved [522030/522030]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can either drop the data csv into the data folder and create a config file in `conf/data_ingestor`, or use an existing data ingestor and overwrite the path and column names:"
      ],
      "metadata": {
        "id": "66-sr4WUt95k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/main.py +experiment=graph data_ingestor=rdb7_fwd data_ingestor.data_source.data_path=train_full.csv data_ingestor.column_mapper.column_mapping.label=ea data_ingestor.column_mapper.column_mapping.smiles=AAM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbzCWSrSs_F3",
        "outputId": "fa04b8cd-40f2-4aa4-ce35-3b43962ad98b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "INFO: Data ingestor instantiated successfully\n",
            "INFO: Data ingestor finished successfully\n",
            "INFO: Data module factory instantiated successfully\n",
            "INFO: Precomputing 2016 items...\n",
            "INFO: Precomputation finished in 6.82s.\n",
            "INFO: Precomputing 112 items...\n",
            "INFO: Precomputation finished in 0.37s.\n",
            "INFO: Precomputing 112 items...\n",
            "INFO: Precomputation finished in 0.37s.\n",
            "INFO: Data modules instantiated successfully\n",
            "INFO: Dataloaders instantiated successfully\n",
            "INFO: Updating global config with properties of train dataset:\n",
            "INFO: Final config:\n",
            "data_ingestor:\n",
            "  data_source:\n",
            "    _target_: chemtorch.data_ingestor.data_source.SingleCSVSource\n",
            "    data_path: train_full.csv\n",
            "  column_mapper:\n",
            "    _target_: chemtorch.data_ingestor.column_mapper.ColumnFilterAndRename\n",
            "    column_mapping:\n",
            "      smiles: AAM\n",
            "      label: ea\n",
            "  data_splitter:\n",
            "    _target_: chemtorch.data_ingestor.data_splitter.RatioSplitter\n",
            "    train_ratio: 0.9\n",
            "    val_ratio: 0.05\n",
            "    test_ratio: 0.05\n",
            "  _target_: chemtorch.data_ingestor.SimpleDataIngestor\n",
            "dataset:\n",
            "  representation:\n",
            "    atom_featurizer:\n",
            "      _target_: chemtorch.featurizer.FeaturizerCompose\n",
            "      featurizers:\n",
            "      - _target_: chemtorch.featurizer.OrganicAtomicNumberOneHotFeaturizer\n",
            "      - _target_: chemtorch.featurizer.AtomDegreeFeaturizer\n",
            "      - _target_: chemtorch.featurizer.AtomFormalChargeFeaturizer\n",
            "      - _target_: chemtorch.featurizer.AtomHCountFeaturizer\n",
            "      - _target_: chemtorch.featurizer.AtomHybridizationFeaturizer\n",
            "      - _target_: chemtorch.featurizer.AtomIsAromaticFeaturizer\n",
            "      - _target_: chemtorch.featurizer.CentiAtomMassFeaturizer\n",
            "      - _target_: chemtorch.featurizer.AtomIsInRingFeaturizer\n",
            "    bond_featurizer:\n",
            "      _target_: chemtorch.featurizer.FeaturizerCompose\n",
            "      featurizers:\n",
            "      - _target_: chemtorch.featurizer.BondTypeFeaturizer\n",
            "      - _target_: chemtorch.featurizer.BondIsConjugatedFeaturizer\n",
            "      - _target_: chemtorch.featurizer.BondInRingFeaturizer\n",
            "    _target_: chemtorch.representation.graph.cgr.CGR\n",
            "    in_channel_multiplier: 2\n",
            "  _target_: chemtorch.dataset.GraphDataset\n",
            "  _partial_: true\n",
            "  precompute_all: true\n",
            "  cache: true\n",
            "  max_size_cache: null\n",
            "  subsample: null\n",
            "dataloader:\n",
            "  _target_: torch_geometric.loader.DataLoader\n",
            "  batch_size: 50\n",
            "  num_workers: 0\n",
            "  pin_memory: true\n",
            "  generator:\n",
            "    _target_: chemtorch.utils.get_generator\n",
            "    seed: 0\n",
            "model:\n",
            "  encoder:\n",
            "    _target_: chemtorch.encoder.directed_edge_enc.DirectedEdgeEncoder\n",
            "    in_channels: 110\n",
            "    out_channels: 128\n",
            "  layer_stack:\n",
            "    dmpnn_blocks:\n",
            "      layer:\n",
            "        graph_conv:\n",
            "          _target_: chemtorch.layer.gnn_layer.DMPNNConv\n",
            "          in_channels: 128\n",
            "          out_channels: 128\n",
            "          separate_nn: false\n",
            "        _target_: chemtorch.layer.gnn_layer.DMPNNBlock\n",
            "        residual: true\n",
            "        ffn: true\n",
            "        dropout: 0.1\n",
            "        act: relu\n",
            "        norm: null\n",
            "        hidden_channels: 128\n",
            "      _target_: chemtorch.layer.layer_stack.LayerStack\n",
            "      _recursive_: false\n",
            "      depth: 3\n",
            "    _target_: chemtorch.layer.gnn_layer.dmpnn_stack.DMPNNStack\n",
            "    edge_to_node_embedding:\n",
            "      _target_: chemtorch.layer.gnn_layer.dmpnn_stack.EdgeToNodeEmbedding\n",
            "      embedding_size: 128\n",
            "      num_node_features: 88\n",
            "  pool:\n",
            "    _target_: chemtorch.pool.pool.GlobalPool\n",
            "    aggr: add\n",
            "  head:\n",
            "    _target_: chemtorch.model.mlp.MLP\n",
            "    in_channels: 128\n",
            "    hidden_size: 128\n",
            "    out_channels: 1\n",
            "    num_hidden_layers: 1\n",
            "    dropout: 0.02\n",
            "    act: relu\n",
            "  _target_: chemtorch.model.gnn.GNN\n",
            "  hidden_channels: 128\n",
            "routine:\n",
            "  loss:\n",
            "    _target_: torch.nn.modules.loss.MSELoss\n",
            "    reduction: sum\n",
            "  optimizer:\n",
            "    _target_: torch.optim.AdamW\n",
            "    _partial_: true\n",
            "    weight_decay: 0.01\n",
            "    amsgrad: false\n",
            "    lr: 0.001\n",
            "    betas:\n",
            "    - 0.9\n",
            "    - 0.999\n",
            "    eps: 1.0e-08\n",
            "    foreach: null\n",
            "    maximize: false\n",
            "    capturable: false\n",
            "  lr_scheduler:\n",
            "    _target_: torch.optim.lr_scheduler.LambdaLR\n",
            "    _partial_: true\n",
            "    lr_lambda:\n",
            "      _target_: chemtorch.scheduler.graphgps_cosine_with_warmup_lr.get_cosine_scheduler_with_warmup\n",
            "      num_warmup_steps: 10\n",
            "      num_training_steps: 200\n",
            "      num_cycles: 0.5\n",
            "    num_warmup_steps: 10\n",
            "  _target_: chemtorch.routine.regression.train\n",
            "  _recursive_: false\n",
            "  epochs: 200\n",
            "  clip_grad_norm: true\n",
            "  clip_grad_norm_value: 1\n",
            "  patience: 30\n",
            "  min_delta: 0.01\n",
            "  save_model_parameters: false\n",
            "  model_path: null\n",
            "  use_wandb: false\n",
            "log: false\n",
            "project_name: graph\n",
            "group_name: null\n",
            "run_name: null\n",
            "seed: 0\n",
            "parameter_limit: null\n",
            "runtime_args_from_train_dataset_props:\n",
            "- num_node_features\n",
            "- num_edge_features\n",
            "- mean\n",
            "- std\n",
            "use_loaded_model: false\n",
            "pretrained_path: null\n",
            "use_cuda: true\n",
            "device: gpu\n",
            "num_node_features: 88\n",
            "num_edge_features: 22\n",
            "mean: 21.412126810683052\n",
            "std: 13.373403041440561\n",
            "\n",
            "Total parameters: 305,921\n",
            "GNN(\n",
            "  (encoder): DirectedEdgeEncoder(\n",
            "    (edge_init): Linear(in_features=110, out_features=128, bias=True)\n",
            "  )\n",
            "  (layer_stack): DMPNNStack(\n",
            "    (dmpnn_blocks): LayerStack(\n",
            "      (layers): ModuleList(\n",
            "        (0-2): 3 x DMPNNBlock(\n",
            "          (graph_conv): DMPNNConv()\n",
            "          (activation): ReLU()\n",
            "          (norm): Identity()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (ffn_norm_in): Identity()\n",
            "          (ffn_linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "          (ffn_linear2): Linear(in_features=256, out_features=128, bias=True)\n",
            "          (ffn_act_fn): ReLU()\n",
            "          (ffn_norm_out): Identity()\n",
            "          (ffn_dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (ffn_dropout2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (edge_to_node_embedding): EdgeToNodeEmbedding(\n",
            "      (linear): Linear(in_features=216, out_features=128, bias=True)\n",
            "      (activation): ReLU()\n",
            "      (aggregation): SumAggregation()\n",
            "    )\n",
            "  )\n",
            "  (pool): GlobalPool()\n",
            "  (head): MLP(\n",
            "    (activation): ReLU()\n",
            "    (layers): Sequential(\n",
            "      (0): Dropout(p=0.02, inplace=False)\n",
            "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (2): ReLU()\n",
            "      (3): Dropout(p=0.02, inplace=False)\n",
            "      (4): Linear(in_features=128, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Epoch 0, Train RMSE: 13.62057787326083, Val RMSE: 14.349695646980997, Time: 1.78\n",
            "Epoch 1, Train RMSE: 12.676397567836517, Val RMSE: 12.651560113328127, Time: 0.82\n",
            "Epoch 2, Train RMSE: 11.492531382941298, Val RMSE: 11.091732310491766, Time: 0.79\n",
            "Epoch 3, Train RMSE: 9.119500161912658, Val RMSE: 9.117576382035601, Time: 0.81\n",
            "Epoch 4, Train RMSE: 7.683808471596219, Val RMSE: 6.616882113153128, Time: 0.80\n",
            "Epoch 5, Train RMSE: 6.968778191068959, Val RMSE: 8.427946596352008, Time: 0.79\n",
            "Epoch 6, Train RMSE: 6.0059768789318255, Val RMSE: 6.245841651746996, Time: 0.79\n",
            "Epoch 7, Train RMSE: 5.954315787339577, Val RMSE: 8.280635690529735, Time: 0.77\n",
            "Epoch 8, Train RMSE: 7.019972238078202, Val RMSE: 7.331891390031186, Time: 0.77\n",
            "Epoch 9, Train RMSE: 6.43204511615246, Val RMSE: 6.6879570164938125, Time: 0.79\n",
            "Epoch 10, Train RMSE: 5.3152600467544415, Val RMSE: 7.940275090289878, Time: 0.80\n",
            "Epoch 11, Train RMSE: 5.1305811196317785, Val RMSE: 6.640833217436699, Time: 0.78\n",
            "Epoch 12, Train RMSE: 5.031593074119308, Val RMSE: 7.156133439750269, Time: 0.80\n",
            "Epoch 13, Train RMSE: 5.059313112461204, Val RMSE: 6.726110199671857, Time: 1.17\n",
            "Epoch 14, Train RMSE: 4.8081917242202765, Val RMSE: 5.962131651657967, Time: 1.09\n",
            "Epoch 15, Train RMSE: 4.590262821626497, Val RMSE: 5.346072825532285, Time: 0.78\n",
            "Epoch 16, Train RMSE: 4.4435508200515565, Val RMSE: 5.4394775255946195, Time: 0.79\n",
            "Epoch 17, Train RMSE: 4.31869989965061, Val RMSE: 5.197771773234942, Time: 0.80\n",
            "Epoch 18, Train RMSE: 4.342785844898793, Val RMSE: 7.032532634035495, Time: 0.81\n",
            "Epoch 19, Train RMSE: 4.638771380511686, Val RMSE: 5.737945795051083, Time: 0.77\n",
            "Epoch 20, Train RMSE: 4.66305720295466, Val RMSE: 6.048565127946517, Time: 0.79\n",
            "Epoch 21, Train RMSE: 4.217005842436845, Val RMSE: 5.568096641067113, Time: 0.82\n",
            "Epoch 22, Train RMSE: 4.140485653241917, Val RMSE: 5.354879612774708, Time: 0.78\n",
            "Epoch 23, Train RMSE: 3.8970098232021164, Val RMSE: 5.096527924843076, Time: 0.79\n",
            "Epoch 24, Train RMSE: 3.8564665796925617, Val RMSE: 5.590592968888607, Time: 0.81\n",
            "Epoch 25, Train RMSE: 4.0285550357751765, Val RMSE: 5.6111157121387265, Time: 0.80\n",
            "Epoch 26, Train RMSE: 3.804464157410924, Val RMSE: 5.242003053275619, Time: 0.78\n",
            "Epoch 27, Train RMSE: 3.792746477459989, Val RMSE: 5.0300183256880535, Time: 1.17\n",
            "Epoch 28, Train RMSE: 3.9164689332004308, Val RMSE: 5.433165280988917, Time: 1.08\n",
            "Epoch 29, Train RMSE: 3.7663226609132145, Val RMSE: 5.159506532400964, Time: 0.82\n",
            "Epoch 30, Train RMSE: 3.6940585502937076, Val RMSE: 5.106633553717152, Time: 0.78\n",
            "Epoch 31, Train RMSE: 3.6418153838657354, Val RMSE: 5.1298419694728805, Time: 0.80\n",
            "Epoch 32, Train RMSE: 3.8089629478505227, Val RMSE: 5.140762340955421, Time: 0.78\n",
            "Epoch 33, Train RMSE: 3.6104967099998753, Val RMSE: 5.1094473907275395, Time: 0.79\n",
            "Epoch 34, Train RMSE: 3.7465832721574412, Val RMSE: 5.17579771636498, Time: 0.76\n",
            "Epoch 35, Train RMSE: 3.528300310886429, Val RMSE: 4.986997704560702, Time: 0.77\n",
            "Epoch 36, Train RMSE: 3.8495666408634532, Val RMSE: 5.6150442634741164, Time: 0.78\n",
            "Epoch 37, Train RMSE: 3.534088614250359, Val RMSE: 5.062555053576401, Time: 0.79\n",
            "Epoch 38, Train RMSE: 3.5152574135086523, Val RMSE: 5.217139118913845, Time: 0.76\n",
            "Epoch 39, Train RMSE: 3.4837830729261174, Val RMSE: 5.168717114098803, Time: 0.75\n",
            "Epoch 40, Train RMSE: 3.520811197485927, Val RMSE: 4.88247150742171, Time: 0.80\n",
            "Epoch 41, Train RMSE: 3.4889445768826826, Val RMSE: 5.491670679817555, Time: 1.15\n",
            "Epoch 42, Train RMSE: 3.739564362515897, Val RMSE: 4.994668640580725, Time: 1.10\n",
            "Epoch 43, Train RMSE: 3.393713884530296, Val RMSE: 5.189831872112721, Time: 0.78\n",
            "Epoch 44, Train RMSE: 3.4902884762524913, Val RMSE: 5.097548225754975, Time: 0.78\n",
            "Epoch 45, Train RMSE: 3.3659718170980777, Val RMSE: 4.873284309767926, Time: 0.78\n",
            "Epoch 46, Train RMSE: 3.4277207420491, Val RMSE: 4.964780761528269, Time: 0.78\n",
            "Epoch 47, Train RMSE: 3.2730279998653895, Val RMSE: 4.933612422727289, Time: 0.82\n",
            "Epoch 48, Train RMSE: 3.301529189021644, Val RMSE: 4.964127135459441, Time: 0.81\n",
            "Epoch 49, Train RMSE: 3.3928195865244737, Val RMSE: 5.229323234377146, Time: 0.80\n",
            "Epoch 50, Train RMSE: 3.295538898213564, Val RMSE: 4.8979718996098836, Time: 0.77\n",
            "Epoch 51, Train RMSE: 3.3172633632730273, Val RMSE: 4.8862016263416965, Time: 0.76\n",
            "Epoch 52, Train RMSE: 3.3522134784464614, Val RMSE: 4.951934102674944, Time: 0.77\n",
            "Epoch 53, Train RMSE: 3.236656674161101, Val RMSE: 5.048249039975158, Time: 0.77\n",
            "Epoch 54, Train RMSE: 3.202481833294004, Val RMSE: 4.95027930222076, Time: 0.76\n",
            "Epoch 55, Train RMSE: 3.276002884692365, Val RMSE: 4.834375478518868, Time: 1.13\n",
            "Epoch 56, Train RMSE: 3.1672540086520162, Val RMSE: 4.998081303894897, Time: 1.18\n",
            "Epoch 57, Train RMSE: 3.2682357646171365, Val RMSE: 4.9086170862312795, Time: 0.77\n",
            "Epoch 58, Train RMSE: 3.1539348004749517, Val RMSE: 5.147919751545883, Time: 0.77\n",
            "Epoch 59, Train RMSE: 3.0987654596568617, Val RMSE: 4.989534137205478, Time: 0.76\n",
            "Epoch 60, Train RMSE: 3.0668385194918026, Val RMSE: 4.93789272038716, Time: 0.80\n",
            "Epoch 61, Train RMSE: 3.0547347979774155, Val RMSE: 4.80879543960268, Time: 0.83\n",
            "Epoch 62, Train RMSE: 2.9977885678625538, Val RMSE: 4.760379683957128, Time: 0.78\n",
            "Epoch 63, Train RMSE: 3.0632766984333215, Val RMSE: 5.109224547579753, Time: 0.79\n",
            "Epoch 64, Train RMSE: 3.050538037480476, Val RMSE: 5.18828544166331, Time: 0.78\n",
            "Epoch 65, Train RMSE: 3.0485946106421924, Val RMSE: 5.000433712627758, Time: 0.78\n",
            "Epoch 66, Train RMSE: 2.9718268261202976, Val RMSE: 5.221363067831172, Time: 0.80\n",
            "Epoch 67, Train RMSE: 3.0483041394898662, Val RMSE: 5.152721713322074, Time: 0.82\n",
            "Epoch 68, Train RMSE: 3.064293939310748, Val RMSE: 5.401985558518204, Time: 0.76\n",
            "Epoch 69, Train RMSE: 2.9937344926978002, Val RMSE: 5.011890516345623, Time: 1.16\n",
            "Epoch 70, Train RMSE: 2.9628058102663672, Val RMSE: 5.13008433883675, Time: 1.11\n",
            "Epoch 71, Train RMSE: 3.0946196287536663, Val RMSE: 4.900122040254943, Time: 0.77\n",
            "Epoch 72, Train RMSE: 2.9364974563408346, Val RMSE: 5.0700460440875155, Time: 0.84\n",
            "Epoch 73, Train RMSE: 2.95981192017047, Val RMSE: 4.9317406205514756, Time: 0.78\n",
            "Epoch 74, Train RMSE: 2.9149669235734637, Val RMSE: 5.190025965815676, Time: 0.79\n",
            "Epoch 75, Train RMSE: 2.951858652664463, Val RMSE: 4.952663822136043, Time: 0.77\n",
            "Epoch 76, Train RMSE: 2.8947666499779747, Val RMSE: 5.070886161378267, Time: 0.79\n",
            "Epoch 77, Train RMSE: 2.876735826665086, Val RMSE: 4.901760429369516, Time: 0.77\n",
            "Epoch 78, Train RMSE: 2.848889412240972, Val RMSE: 5.008247575734172, Time: 0.77\n",
            "Epoch 79, Train RMSE: 2.7941629057723425, Val RMSE: 4.855974786868929, Time: 0.77\n",
            "Epoch 80, Train RMSE: 2.9085692309120477, Val RMSE: 4.949542797670411, Time: 0.79\n",
            "Epoch 81, Train RMSE: 2.78621444823224, Val RMSE: 5.08174402541595, Time: 0.81\n",
            "Epoch 82, Train RMSE: 2.872236665320058, Val RMSE: 5.14119661284233, Time: 0.85\n",
            "Epoch 83, Train RMSE: 2.821109056919841, Val RMSE: 4.88015155761118, Time: 1.12\n",
            "Epoch 84, Train RMSE: 2.832523529412904, Val RMSE: 5.219857035169061, Time: 1.09\n",
            "Epoch 85, Train RMSE: 2.820008998613001, Val RMSE: 4.875290505723567, Time: 0.79\n",
            "Epoch 86, Train RMSE: 2.7559433781658154, Val RMSE: 4.989387334321027, Time: 0.82\n",
            "Epoch 87, Train RMSE: 2.798699151719151, Val RMSE: 4.97751344904026, Time: 0.81\n",
            "Epoch 88, Train RMSE: 2.6896338751841005, Val RMSE: 4.972622807592618, Time: 0.80\n",
            "Epoch 89, Train RMSE: 2.7339036807804535, Val RMSE: 4.830209472172527, Time: 0.77\n",
            "Epoch 90, Train RMSE: 2.717213251168454, Val RMSE: 4.946384599574766, Time: 0.80\n",
            "Epoch 91, Train RMSE: 2.729556038916619, Val RMSE: 5.133174844357545, Time: 0.81\n",
            "Epoch 92, Train RMSE: 2.736428323636113, Val RMSE: 5.083790363277561, Time: 0.79\n",
            "Early stopping at epoch 92\n",
            "Test RMSE: 4.144622125635435\n",
            "Test MAE: 3.1446876872133607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3D information\n",
        "We can also train on 3D data."
      ],
      "metadata": {
        "id": "oAsbhZOWxRMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/main.py +experiment=xyz dataset.subsample=0.05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RlPSJn2ta_W",
        "outputId": "dc9b3966-4bf1-48fb-da47-37b28239d2c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "INFO: Data ingestor instantiated successfully\n",
            "INFO: Data ingestor finished successfully\n",
            "INFO: Data module factory instantiated successfully\n",
            "INFO: Precomputing 537 items...\n",
            "INFO: Precomputation finished in 0.37s.\n",
            "INFO: Precomputing 30 items...\n",
            "INFO: Precomputation finished in 0.02s.\n",
            "INFO: Precomputing 30 items...\n",
            "INFO: Precomputation finished in 0.02s.\n",
            "INFO: Data modules instantiated successfully\n",
            "INFO: Dataloaders instantiated successfully\n",
            "INFO: Updating global config with properties of train dataset:\n",
            "INFO: Final config:\n",
            "data_ingestor:\n",
            "  data_source:\n",
            "    _target_: chemtorch.data_ingestor.data_source.SingleCSVSource\n",
            "    data_path: data/rdb7/barriers/forward/data.csv\n",
            "  column_mapper:\n",
            "    _target_: chemtorch.data_ingestor.column_mapper.ColumnFilterAndRename\n",
            "    column_mapping:\n",
            "      smiles: smiles\n",
            "      reaction_dir: rxn\n",
            "      label: dE0\n",
            "  data_splitter:\n",
            "    _target_: chemtorch.data_ingestor.data_splitter.RatioSplitter\n",
            "    train_ratio: 0.9\n",
            "    val_ratio: 0.05\n",
            "    test_ratio: 0.05\n",
            "  _target_: chemtorch.data_ingestor.SimpleDataIngestor\n",
            "dataset:\n",
            "  representation:\n",
            "    _target_: chemtorch.representation.graph.xyz_graph.XYZReactionRepresentation\n",
            "    root_dir: data/rdb7/geometries/forward\n",
            "  _target_: chemtorch.dataset.GraphDataset\n",
            "  _partial_: true\n",
            "  precompute_all: true\n",
            "  cache: true\n",
            "  max_size_cache: null\n",
            "  subsample: 0.05\n",
            "dataloader:\n",
            "  _target_: torch_geometric.loader.DataLoader\n",
            "  batch_size: 50\n",
            "  num_workers: 0\n",
            "  pin_memory: true\n",
            "model:\n",
            "  head:\n",
            "    _target_: chemtorch.model.mlp.MLP\n",
            "    in_channels: 128\n",
            "    hidden_size: 128\n",
            "    out_channels: 1\n",
            "    num_hidden_layers: 1\n",
            "    dropout: 0.02\n",
            "    act: relu\n",
            "  _target_: chemtorch.model.dimenetplusplus.DimeNetPlusPlus\n",
            "  hidden_channels: 128\n",
            "  out_channels: 128\n",
            "  num_blocks: 4\n",
            "  int_emb_size: 64\n",
            "  basis_emb_size: 8\n",
            "  out_emb_channels: 256\n",
            "  num_spherical: 7\n",
            "  num_radial: 6\n",
            "  cutoff: 5.0\n",
            "  max_num_neighbors: 32\n",
            "  envelope_exponent: 5\n",
            "  num_before_skip: 1\n",
            "  num_after_skip: 2\n",
            "  num_output_layers: 3\n",
            "  act: swish\n",
            "  output_initializer: zeros\n",
            "routine:\n",
            "  loss:\n",
            "    _target_: torch.nn.modules.loss.MSELoss\n",
            "    reduction: sum\n",
            "  optimizer:\n",
            "    _target_: torch.optim.AdamW\n",
            "    _partial_: true\n",
            "    weight_decay: 0.01\n",
            "    amsgrad: false\n",
            "    lr: 0.001\n",
            "    betas:\n",
            "    - 0.9\n",
            "    - 0.999\n",
            "    eps: 1.0e-08\n",
            "    foreach: null\n",
            "    maximize: false\n",
            "    capturable: false\n",
            "  lr_scheduler:\n",
            "    _target_: torch.optim.lr_scheduler.LambdaLR\n",
            "    _partial_: true\n",
            "    lr_lambda:\n",
            "      _target_: chemtorch.scheduler.graphgps_cosine_with_warmup_lr.get_cosine_scheduler_with_warmup\n",
            "      num_warmup_steps: 10\n",
            "      num_training_steps: 200\n",
            "      num_cycles: 0.5\n",
            "    num_warmup_steps: 10\n",
            "  _target_: chemtorch.routine.regression.train\n",
            "  _recursive_: false\n",
            "  epochs: 200\n",
            "  clip_grad_norm: true\n",
            "  clip_grad_norm_value: 1\n",
            "  patience: 30\n",
            "  min_delta: 0.01\n",
            "  save_model_parameters: false\n",
            "  model_path: null\n",
            "  use_wandb: false\n",
            "log: false\n",
            "project_name: xyz\n",
            "group_name: null\n",
            "run_name: null\n",
            "seed: 0\n",
            "parameter_limit: null\n",
            "runtime_args_from_train_dataset_props:\n",
            "- mean\n",
            "- std\n",
            "use_loaded_model: false\n",
            "pretrained_path: null\n",
            "use_cuda: true\n",
            "device: gpu\n",
            "mean: 79.55948763500932\n",
            "std: 22.017933290548957\n",
            "\n",
            "Total parameters: 2,065,031\n",
            "DimeNetPlusPlus(\n",
            "  (rbf): BesselBasisLayer(\n",
            "    (envelope): Envelope()\n",
            "  )\n",
            "  (sbf): SphericalBasisLayer(\n",
            "    (envelope): Envelope()\n",
            "  )\n",
            "  (emb): EmbeddingBlock(\n",
            "    (emb): Embedding(95, 128)\n",
            "    (lin_rbf): Linear(in_features=6, out_features=128, bias=True)\n",
            "    (lin): Linear(in_features=384, out_features=128, bias=True)\n",
            "  )\n",
            "  (output_blocks): ModuleList(\n",
            "    (0-4): 5 x OutputPPBlock(\n",
            "      (lin_rbf): Linear(in_features=6, out_features=128, bias=False)\n",
            "      (lin_up): Linear(in_features=128, out_features=256, bias=False)\n",
            "      (lins): ModuleList(\n",
            "        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "      )\n",
            "      (lin): Linear(in_features=256, out_features=128, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (interaction_blocks): ModuleList(\n",
            "    (0-3): 4 x InteractionPPBlock(\n",
            "      (lin_rbf1): Linear(in_features=6, out_features=8, bias=False)\n",
            "      (lin_rbf2): Linear(in_features=8, out_features=128, bias=False)\n",
            "      (lin_sbf1): Linear(in_features=42, out_features=8, bias=False)\n",
            "      (lin_sbf2): Linear(in_features=8, out_features=64, bias=False)\n",
            "      (lin_kj): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (lin_ji): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (lin_down): Linear(in_features=128, out_features=64, bias=False)\n",
            "      (lin_up): Linear(in_features=64, out_features=128, bias=False)\n",
            "      (layers_before_skip): ModuleList(\n",
            "        (0): ResidualLayer(\n",
            "          (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (lin): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (layers_after_skip): ModuleList(\n",
            "        (0-1): 2 x ResidualLayer(\n",
            "          (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): MLP(\n",
            "    (activation): ReLU()\n",
            "    (layers): Sequential(\n",
            "      (0): Dropout(p=0.02, inplace=False)\n",
            "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (2): ReLU()\n",
            "      (3): Dropout(p=0.02, inplace=False)\n",
            "      (4): Linear(in_features=128, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Epoch 0, Train RMSE: 22.11633130959344, Val RMSE: 25.13456675188169, Time: 21.29\n",
            "Epoch 1, Train RMSE: 21.90056332773807, Val RMSE: 24.64578929526134, Time: 20.27\n",
            "Epoch 2, Train RMSE: 21.45370538180701, Val RMSE: 24.83951070463142, Time: 23.14\n",
            "Epoch 3, Train RMSE: 20.46846448393426, Val RMSE: 26.155393319075927, Time: 18.99\n",
            "Epoch 4, Train RMSE: 20.2748810300147, Val RMSE: 22.655761197093756, Time: 18.64\n",
            "Epoch 5, Train RMSE: 19.296659917829693, Val RMSE: 21.310623665784707, Time: 22.04\n",
            "Epoch 6, Train RMSE: 19.091848725153376, Val RMSE: 21.063455864217094, Time: 19.81\n",
            "Epoch 7, Train RMSE: 18.685322204049378, Val RMSE: 22.080827654635844, Time: 20.29\n",
            "Epoch 8, Train RMSE: 20.005817330236255, Val RMSE: 20.589383009412625, Time: 19.77\n",
            "Epoch 9, Train RMSE: 19.649229606299933, Val RMSE: 24.590357143163725, Time: 19.84\n",
            "Epoch 10, Train RMSE: 23.203591903972406, Val RMSE: 32.44290345966221, Time: 20.51\n",
            "Epoch 11, Train RMSE: 22.943689826007308, Val RMSE: 21.21247374226028, Time: 18.83\n",
            "Epoch 12, Train RMSE: 18.42946478113623, Val RMSE: 19.442118447529886, Time: 19.69\n",
            "Epoch 13, Train RMSE: 15.808948947393342, Val RMSE: 22.128327459514225, Time: 19.34\n",
            "Epoch 14, Train RMSE: 15.431213334365614, Val RMSE: 20.37524898711113, Time: 20.56\n",
            "Epoch 15, Train RMSE: 12.895002807758878, Val RMSE: 20.052329087462557, Time: 20.45\n",
            "Epoch 16, Train RMSE: 13.766583793581585, Val RMSE: 25.446012832514015, Time: 20.20\n",
            "Epoch 17, Train RMSE: 13.954008033893379, Val RMSE: 20.19245902181926, Time: 21.02\n",
            "Epoch 18, Train RMSE: 11.385400063007298, Val RMSE: 17.592126087027005, Time: 18.88\n",
            "Epoch 19, Train RMSE: 11.769351688183859, Val RMSE: 20.152347202860813, Time: 18.66\n",
            "Epoch 20, Train RMSE: 10.57424761032264, Val RMSE: 17.36713738311914, Time: 19.49\n",
            "Epoch 21, Train RMSE: 11.136653668478724, Val RMSE: 19.064886745902054, Time: 18.98\n",
            "Epoch 22, Train RMSE: 9.58425045324298, Val RMSE: 18.90586009326108, Time: 19.39\n",
            "Epoch 23, Train RMSE: 8.845579571407493, Val RMSE: 18.90447617165457, Time: 17.69\n",
            "Epoch 24, Train RMSE: 8.203449584352171, Val RMSE: 16.46661019232344, Time: 19.77\n",
            "Epoch 25, Train RMSE: 8.026395644283568, Val RMSE: 18.645489791851716, Time: 19.68\n",
            "Epoch 26, Train RMSE: 7.509055202499159, Val RMSE: 17.49681264797313, Time: 20.68\n",
            "Epoch 27, Train RMSE: 6.263409686689855, Val RMSE: 19.278892951586908, Time: 21.13\n",
            "Epoch 28, Train RMSE: 6.619178661553445, Val RMSE: 19.388414480214248, Time: 19.66\n",
            "Epoch 29, Train RMSE: 6.567565997096695, Val RMSE: 17.02549775462507, Time: 19.61\n",
            "Epoch 30, Train RMSE: 7.616436511457089, Val RMSE: 16.951427823581852, Time: 19.90\n",
            "Epoch 31, Train RMSE: 6.569504260632514, Val RMSE: 19.97215085503118, Time: 19.27\n",
            "Epoch 32, Train RMSE: 5.7177239836941185, Val RMSE: 18.222494018850636, Time: 23.66\n",
            "Epoch 33, Train RMSE: 6.157878834527573, Val RMSE: 18.602740363885285, Time: 18.84\n",
            "Epoch 34, Train RMSE: 5.921596711798896, Val RMSE: 17.779196493342443, Time: 19.77\n",
            "Epoch 35, Train RMSE: 7.243800966019341, Val RMSE: 19.593452358272607, Time: 19.52\n",
            "Epoch 36, Train RMSE: 6.115960749638055, Val RMSE: 19.453917283488472, Time: 19.45\n",
            "Epoch 37, Train RMSE: 4.2942694514981605, Val RMSE: 18.591536513210723, Time: 19.29\n",
            "Epoch 38, Train RMSE: 4.623629186433282, Val RMSE: 18.757791525281547, Time: 18.87\n",
            "Epoch 39, Train RMSE: 4.78238440127134, Val RMSE: 18.300472003791466, Time: 19.55\n",
            "Epoch 40, Train RMSE: 4.066064063487415, Val RMSE: 17.05051892331794, Time: 19.55\n",
            "Epoch 41, Train RMSE: 5.055567574865192, Val RMSE: 18.101927625730834, Time: 22.17\n",
            "Epoch 42, Train RMSE: 5.158916731687227, Val RMSE: 16.416724901539336, Time: 20.32\n",
            "Epoch 43, Train RMSE: 4.904929596904112, Val RMSE: 18.45825858509185, Time: 19.44\n",
            "Epoch 44, Train RMSE: 4.2210189934153854, Val RMSE: 18.75112461282689, Time: 20.51\n",
            "Epoch 45, Train RMSE: 4.557673161115734, Val RMSE: 16.19641404262632, Time: 20.80\n",
            "Epoch 46, Train RMSE: 4.074181297766499, Val RMSE: 18.347310468525308, Time: 19.66\n",
            "Epoch 47, Train RMSE: 4.08909003919366, Val RMSE: 17.173923349426328, Time: 20.35\n",
            "Epoch 48, Train RMSE: 3.532636927105932, Val RMSE: 17.18354066016358, Time: 20.10\n",
            "Epoch 49, Train RMSE: 4.789207070488148, Val RMSE: 16.60445545301516, Time: 20.36\n",
            "Epoch 50, Train RMSE: 4.044330518699284, Val RMSE: 16.07754996145246, Time: 20.49\n",
            "Epoch 51, Train RMSE: 3.8403578265682463, Val RMSE: 18.0679362601189, Time: 19.58\n",
            "Epoch 52, Train RMSE: 3.330304072704894, Val RMSE: 17.875240900901257, Time: 20.13\n",
            "Epoch 53, Train RMSE: 3.882488766842501, Val RMSE: 17.180387481859835, Time: 20.89\n",
            "Epoch 54, Train RMSE: 3.380917906185641, Val RMSE: 18.501629089807853, Time: 19.87\n",
            "Epoch 55, Train RMSE: 3.6267432920925464, Val RMSE: 18.253317281510547, Time: 20.77\n",
            "Epoch 56, Train RMSE: 4.174266115662742, Val RMSE: 17.390255207127016, Time: 19.28\n",
            "Epoch 57, Train RMSE: 4.141685724246558, Val RMSE: 16.35246849326147, Time: 20.03\n",
            "Epoch 58, Train RMSE: 3.160173753234902, Val RMSE: 17.643500690604142, Time: 20.14\n",
            "Epoch 59, Train RMSE: 2.8595037379232897, Val RMSE: 18.086241895797322, Time: 19.57\n",
            "Epoch 60, Train RMSE: 3.3010756502401453, Val RMSE: 17.931460230892057, Time: 20.36\n",
            "Epoch 61, Train RMSE: 3.14290464043334, Val RMSE: 18.746833768927925, Time: 20.09\n",
            "Epoch 62, Train RMSE: 3.0483197821525705, Val RMSE: 17.35303172566249, Time: 21.67\n",
            "Epoch 63, Train RMSE: 2.7613481113726674, Val RMSE: 17.839356641545347, Time: 20.80\n",
            "Epoch 64, Train RMSE: 3.169514346060035, Val RMSE: 17.58358304966683, Time: 20.83\n",
            "Epoch 65, Train RMSE: 3.1687623334114177, Val RMSE: 17.64389845253264, Time: 19.73\n",
            "Epoch 66, Train RMSE: 4.293556513306769, Val RMSE: 19.281064065094508, Time: 20.60\n",
            "Epoch 67, Train RMSE: 4.012968973579307, Val RMSE: 17.847285943687424, Time: 19.27\n",
            "Epoch 68, Train RMSE: 3.8739105662045388, Val RMSE: 17.429810354232416, Time: 20.49\n",
            "Epoch 69, Train RMSE: 3.110333012356379, Val RMSE: 17.450566583256986, Time: 18.82\n",
            "Epoch 70, Train RMSE: 2.3941889109000276, Val RMSE: 18.366217614982464, Time: 19.06\n",
            "Epoch 71, Train RMSE: 3.250257041743367, Val RMSE: 17.66498092086398, Time: 19.70\n",
            "Epoch 72, Train RMSE: 3.012254640076532, Val RMSE: 18.78641586673463, Time: 19.19\n",
            "Epoch 73, Train RMSE: 2.6951067417499943, Val RMSE: 18.705963863020855, Time: 19.66\n",
            "Epoch 74, Train RMSE: 3.0869030950750282, Val RMSE: 18.342430941294456, Time: 19.74\n",
            "Epoch 75, Train RMSE: 3.6806332290098807, Val RMSE: 18.6855540872885, Time: 20.43\n",
            "Epoch 76, Train RMSE: 3.0100941908790606, Val RMSE: 17.11628210158866, Time: 19.89\n",
            "Epoch 77, Train RMSE: 2.3920790295766223, Val RMSE: 18.135521076620016, Time: 19.60\n",
            "Epoch 78, Train RMSE: 2.7350560675249205, Val RMSE: 17.35188954950754, Time: 20.59\n",
            "Epoch 79, Train RMSE: 3.4325161958383217, Val RMSE: 18.032393021950845, Time: 19.20\n",
            "Epoch 80, Train RMSE: 2.909402265738493, Val RMSE: 17.768666327215193, Time: 20.17\n",
            "Early stopping at epoch 80\n",
            "Test RMSE: 15.009921493803876\n",
            "Test MAE: 9.285058864624022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p-D7DDjNwYJv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}